<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>01 用 MXNet 实现一个神经网络 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="第 7 章 Hello World! 从第7章开始就进入到了实战阶段，将通过一系列基础例子了解基于深度学习计算机 视觉的有趣应用和实施方法。本章作为实战阶段的第一章，用一" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/12-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/07-hello-world/01-%E7%94%A8-mxnet-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="01 用 MXNet 实现一个神经网络" />
<meta property="og:description" content="第 7 章 Hello World! 从第7章开始就进入到了实战阶段，将通过一系列基础例子了解基于深度学习计算机 视觉的有趣应用和实施方法。本章作为实战阶段的第一章，用一" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/12-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/07-hello-world/01-%E7%94%A8-mxnet-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><meta property="article:published_time" content="2018-08-29T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-29T00:00:00&#43;00:00"/>
<meta itemprop="name" content="01 用 MXNet 实现一个神经网络">
<meta itemprop="description" content="第 7 章 Hello World! 从第7章开始就进入到了实战阶段，将通过一系列基础例子了解基于深度学习计算机 视觉的有趣应用和实施方法。本章作为实战阶段的第一章，用一">


<meta itemprop="datePublished" content="2018-08-29T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-29T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="4251">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="01 用 MXNet 实现一个神经网络"/>
<meta name="twitter:description" content="第 7 章 Hello World! 从第7章开始就进入到了实战阶段，将通过一系列基础例子了解基于深度学习计算机 视觉的有趣应用和实施方法。本章作为实战阶段的第一章，用一"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">01 用 MXNet 实现一个神经网络</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-29 </span>
        
        <span class="more-meta"> 4251 words </span>
        <span class="more-meta"> 9 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#第-7-章-hello-world">第 7 章 Hello World!</a>
<ul>
<li><a href="#用mxnet实现一个神经网络">用MXNet实现一个神经网络</a>
<ul>
<li><a href="#基础工具-nvidia驱动和cuda安装">基础工具、NVIDIA驱动和CUDA安装</a></li>
</ul></li>
<li><a href="#安装-mxnet">安装 MXNet</a>
<ul>
<li><a href="#mxnet-基本使用">MXNet 基本使用</a></li>
</ul></li>
<li><a href="#用mxnet实现一个两层神经网络">用MXNet实现一个两层神经网络</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="第-7-章-hello-world">第 7 章 Hello World!</h1>

<p>从第7章开始就进入到了实战阶段，将通过一系列基础例子了解基于深度学习计算机 视觉的有趣应用和实施方法。本章作为实战阶段的第一章，用一个Hello World级别的神 经网络小例子带大家一起入门最基本的训练（Training）-预测（Inference）流程，以及 MXNet 和 Caffe 这两种框架的基本使用。</p>

<p>对于 MXNet 和 Caffe 这两种框架，本书会基于实例对其用法进行最基本的介绍。不过作为一本入门书，本书不会涉及分布式使用、代码细节和架 构等方面，而只会专注于基本算法的实现方法。深度学习框架多种多样，但框架终究是工具，重点是通过了解每个例子和背后对应的思想来入门深度学习/计算机视觉这个有用又有趣的领域。<span style="color:red;">嗯。</span></p>

<h2 id="用mxnet实现一个神经网络">用MXNet实现一个神经网络</h2>

<p>MXNet 的起源已经在第 1 章大概介绍过，本节主要介绍 MXNet 的安装和基本概念，并通过实现一个最简单的神经网络来了解 MXNet 的使用。</p>

<h3 id="基础工具-nvidia驱动和cuda安装">基础工具、NVIDIA驱动和CUDA安装</h3>

<p>首先是安装大部分开发工具需要的基础依赖工具包，比如 git、用于矩阵计算的 atlas, 和图可视化的 graphviz 等，以Ubuntu 16.04 LTS为例，执行下面命令安装包：<span style="color:red;">这个 atlas 是什么？</span></p>

<pre><code>sudo apt update
sudo apt install build-essential git libatlas-base-dev
sudo pip install graphviz
</code></pre>

<p><span style="color:red;">现在的 linux 用来做机器学习的机器普遍都是装的 ubuntu 吗？</span></p>

<p>第 5 章和第 6 章讲过的 pip、NumPy 和 OpenCV 也是需要的，安装方法已经讲过在此就不再介绍了。</p>

<p>对于当前的所有深度学习框架，如果要训练神经网络一定离不开（NVIDIA的）GPU, 以及配套的 GPU 编程工具包 CUDA。所以接下来是安装 NVIDIA 的驱动和 CUDA 工具包。安装 NVIDIA 驱动前需要先卸载系统自己的驱动：</p>

<pre><code>sudo apt --purge remove xserver-xorg-video-nouveau
</code></pre>

<p>然后添加NVIDIA驱动的源：</p>

<pre><code>sudo add-apt-repository ppa:graphics-drivers/ppa
</code></pre>

<p>然后就可以安装驱动和CUDA工具包了。</p>

<pre><code>sudo apt install nvidia-361 nvidia-settings nvidia-prime
sudo apt install nvidia-cuda-toolkit
</code></pre>

<p>安装完成后，在控制台输入：</p>

<pre><code>nvidia-smi
</code></pre>

<p>如果安装成功则会显示显卡的信息，比如笔者的 GTX980M 笔记本显示信息如图7-1 所示。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180830/bKk4m1ab7H.png?imageslim" alt="mark" /></p>

<p>如果是其他的Linux系统，步骤也很相似，首先卸载自带的显卡驱动，然后用系统自带软件包或者到 NVIDIA 官网下载驱动及 CUDA 按照说明进行安装，下载地址为 <a href="http://www.nvidia.com/Download/index.aspx">http://www.nvidia.com/Download/index.aspx</a> 或  <a href="https://developer.nvidia.com/cuda-downloads。">https://developer.nvidia.com/cuda-downloads。</a><span style="color:red;">还没有亲自安装过，一定要亲自实践并总结。</span></p>

<p>cuDNN 是 CUDA 中专门为加速深度神经网络设计的库，是个可选的安装选项。下载 ±也址为 <a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a> 。</p>

<p>找到对应的版本并填写需要的信息之后就可以下载了。下载之后是一个压缩包，这里 以 cuDNN 5.1 为例，执行以下命令将 cuDNN 中的库解压并添加到 CUDA 对应文件夹下。</p>

<pre><code>tar -xvzf cudnn-8.0-linux-x64-v5.1-ga.tgz
sudo cp -P cuda/include/cudnn.h /usr/local/cuda/include
sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64
</code></pre>

<p>任何深度学习框架中，基于 CPU 的矩阵计算包也是基础的库之一，除了本节一开始安 装的 atlas, Intel 的 MKL (Math Kernel Library) 因为其优异的性能，往往是一个更佳的选项，MKL 的下载地址为 <a href="https://software.intel.com/en-us/intel-mkl/">https://software.intel.com/en-us/intel-mkl/</a> 。<span style="color:red;">嗯。</span></p>

<p>MKL 对于个人是免费的，需要一定的注册步骤获取一个许可证。其安装也不难，下载好安装包并解压后，执行 install.sh 或者 install_GUI.sh ，按照指示一步步安装即可。</p>

<h2 id="安装-mxnet">安装 MXNet</h2>

<p>MXNet 的 github 页面是 <a href="https://github.com/dmlc/mxnet">https://github.com/dmlc/mxnet</a> 。</p>

<p>在这里可以找到源代码的 git 地址，然后在要保存的控制台地址中输入下面命令:</p>

<pre><code>git clone --recursive https://github.com/dmlc/mxnet
</code></pre>

<p><span style="color:red;">为什么要写 recursive ？</span></p>

<p>之后就会在执行命令的文件夹下得到一个 mxnet 的文件夹。第一步是配置安装的基础选项，打开 mxnet/make 文件夹下的 config.mk 文件，主要需要配置的是以下3个选项。</p>

<ul>
<li>USE_CUDA = 0；</li>
<li>USE_CUDNN = 0；</li>
<li>USEBLAS = atlas。</li>
</ul>

<p>上面列出的都是默认选项，对于训练网络的需求，需要至少把 USE_CUDA 改成 1，如果需要 cuDNN 和 mkl 的话则需要把 USE_CUDNN 改成 1，USE_BLAS 改为 mkl。</p>

<p>配置好后就可以开始安装了。在Ubuntu下有个非常方便的方式，就是进入 mxnet/setup-utils 文件夹下，直接执行对应脚本：</p>

<pre><code>cd mxnet/setup-utils
sh install-mxnet-ubuntu-python.sh
</code></pre>

<p>等待执行结束就大功告成了。当然如果不是 Ubuntu，那么一般的方式是回到 mxnet 目录下，执行：</p>

<pre><code>cd mxnet
make - j
</code></pre>

<p>自动利用所有可用的 CPU 核对代码进行编译，如果在 -j 后面直接加上数字可以指定用的核数。然后配置 Python 接口：</p>

<pre><code>cd python
sudo python setup.py install
</code></pre>

<p><span style="color:red;">配置 Python 接口是什么意思？</span></p>

<p>万事俱备，接下来可以开始使用这个强大的框架了。</p>

<h3 id="mxnet-基本使用">MXNet 基本使用</h3>

<p>在第 3 章已经提到过，当描述一个神经网络或者一些计算公式及函数的时候，实质上是在描述一种可以用图表示的计算关系。在MXNet中，这种计算关系可以有两种方式表达和计算，即命令式(Imperative)和符号式(Symbolic)。比如 (a+b)*c，命令式计算的代码如下：</p>

<pre><code class="language-python">import mxnet as mx

# 申请内存并赋值，默认利用CPU
a = mx.nd.array([1])
b = mx.nd.array([2])
c = mx.nd.array([3])

# 执行计算
d = (a + b) * c

# 将结果以NumPy的array的形式表示 [9.]
print(d.asnumpy())
# 将结果以标量的形式表示 9.0
print(d.asscalar())
</code></pre>

<p>和 NumPy 的 array 很像，其实 NDArray 里很多的操作和方法确实和 NumPy 是一样的。</p>

<p>回到正题，相应的符号式计算的代码如下：</p>

<pre><code class="language-python">import mxnet as mx

# 定义3个符号变量，注意符号变量都需要一个显式指定的名字
a = mx.sys.Variable('a')
b = mx.sys.Variable('b')
c = mx.sys.Variable('c')

# 定义计算关系
d = (a + b) * c

# 指定每个输入符号对应的输入
input_args = {
    'a': mx.nd.array([1]),
    'b': mx.nd.array([2]),
    'c': mx.nd.array([3]),
}

# a、b、c和d定义的只是计算关系，执行计算(包括申请相应内存等操作)需要Executor
# 用bind ()函数指定输入，d为输出，cpu ()指定计算在cpu上进行
executor = d.bind(ctx=mx.cpu(), args=input_args)
# 执行计算
executor.forward()
# 打印结果，[9.]
print(executor.outputs[0].asnumpy())
</code></pre>

<p><span style="color:red;">这种方式看了有点眼熟，与 Tensorflow 感觉差不多类似的</span></p>

<p>可以看到，命令式计算非常灵活直接，每个变量的内存分配是即时完成的，计算也是即时完成。而符号式计算则是函数式编程的思路，计算也是延迟(lazy)的，符号变量只能定义计算关系。<span style="color:red;">这就是函数式编程的思路吗？想知道这个思路到底是什么？</span></p>

<p>这种计算关系在执行前需要通过 bind() 方法产生一个执行器(Executor), 用来把数据的 NDArray 和 Symbol 绑定起来，实际的计算发生在 Executor 调用时。</p>

<p>符号式计算很明显要麻烦一些，不过优点是延迟计算和对计算图的优化能得到更优的性能。另外，在MXNet中通过符号式计算求导是非常方便的，继续接前面例子：<span style="color:red;">为什么对计算图的优化能得到更优的性能？</span></p>

<pre><code class="language-python">import mxnet as mx

# 定义3个符号变量，注意符号变量都需要一个显式指定的名字
a = mx.sys.Variable('a')
b = mx.sys.Variable('b')
c = mx.sys.Variable('c')

# 定义计算关系
d = (a + b) * c

# 指定每个输入符号对应的输入
input_args = {
    'a': mx.nd.array([1]),
    'b': mx.nd.array([2]),
    'c': mx.nd.array([3]),
}

# a、b、c和d定义的只是计算关系，执行计算(包括申请相应内存等操作)需要Executor
# 用bind ()函数指定输入，d为输出，cpu ()指定计算在cpu上进行
executor = d.bind(ctx=mx.cpu(), args=input_args)
# 执行计算
executor.forward()
# 打印结果，[9.]
print(executor.outputs[0].asnumpy())

#定义一个变量用来保存关于a的梯度，随便初始化一下
grad_a=mx.nd.empty(1)

#在bind ()函数中指定要求梯度的变量
executor=d.bind(
    ctx=mx.cpu(),
    args=input_args,
    args_grad={'a':grad_a}
)
#因为梯度是传播的，所以最后输出节点的梯度需要指定，这里用1
executor.backward(out_grags=mx.nd.ones(1))
#计算出梯度为 3.0，也就是 c 的值，将自动刷新在 grad_a 中
print(grad_a.asscalar())
</code></pre>

<p><span style="color:red;">什么意思？上面的求导的过程没大看懂，为什么最后的输出的节点的梯度需要指定？而且 <code>args_grad={'a':grad_a}</code> 是用来做什么的？ </span></p>

<p>在 MXNet 中，第一段代码中，用于命令式计算的 NDArray 是一个非常基础的模块，符号式计算的 Symbolic 模块结合 NDArray 一起使用可以定义一些基础的计算关系并进行计算。在这两个模块基础上可以搭建一些简单的计算关系，比如神经网络。但是如果每次都像上面代码一样从底层搭建，并且自己指定计算梯度等操作，甚至更进一步比如在神经 网络中进行后向传播和梯度更新等，将是一件非常麻烦的事情。所以在 NDArray 和 Symbolic 基础上，MXNet提供了一些接口进行封装来简化这些操作，包含通用性更好的 Module 模块和更为简单的 Model 模块。<span style="color:red;">嗯。</span></p>

<p>既然要训练模型，就不能避免与数据和机器打交道，所以MXNet也提供了数据读取和处理的 IO Data Loading 模块和用来支持多 GPU 卡及分布式计算的 KVStore 模块。本书 作为一本入门书籍，将主要涉及 6 大模块中除了 KVStore 以外的模块，神经网络模型的搭 建也主要基于 Module 和 Model 模块，而不需要从 Symbolic 开始进行复杂的底层编写。更多关于这些模块的细节可以参考官方文档 <a href="http://mxnet.io/zh/api/python/">http://mxnet.io/zh/api/python/</a> 。<span style="color:red;">嗯，这些模块都要总结，而且 KVStore 也要掌握，</span></p>

<h2 id="用mxnet实现一个两层神经网络">用MXNet实现一个两层神经网络</h2>

<p>本节将基于 Model 模块实现本书中第一个神经网络。数据和网络的模型结构就是第 3 章中 3.2.2中的例子，产生数据的代码如下：</p>

<pre><code class="language-python">import pickle
import numpy as np
import matplotlib.pyplot as plt


# 划分类别的边界
def cos_curve(x):
    return 0.25 * np.sin(2 * x * np.pi + 0.5 * np.pi) + 0.5


# samples 保存二维点的坐标，labels 标明类别
np.random.seed(123)
samples = []
labels = []

# 单位样本空间内平均样本数为 50
sample_density = 50
for i in range(sample_density):
    x1, x2 = np.random.random(2)
    # 计算当前 x1 对应的分类边界
    bound = cos_curve(x1)
    # 为了方便可视化，舍弃太靠近边界的样本
    if bound - 0.1 &lt; x2 &lt;= bound + 0.1:
        continue
    else:
        samples.append((x1, x2))
        # 上半部分标签为1，下半部分标签为0
        if x2 &gt; bound:
            labels.append(1)
        else:
            labels.append(0)

# 讲生成的样本和标签保存
with open('data.pkl', 'wb') as f:
    pickle.dump((samples, labels), f)

# 可视化
for i, sample in enumerate(samples):
    plt.plot(sample[0], sample[1],
             'o' if labels[i] else '^',
             mec='r' if labels[i] else 'b',
             mfc='none',
             markersize=10)

x1 = np.linspace(0, 1)
plt.plot(x1, cos_curve(x1), 'k--')
plt.show()
</code></pre>

<p>数据的二维可视化参照第3章的图3-7所示。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180831/mj98f4Kb59.png?imageslim" alt="mark" /></p>

<p>然后通过 MXNet 的 model 模块搭建一个两层神经网络，网络结构参照第3章的图3-8所示，</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180831/2ba909hiCc.png?imageslim" alt="mark" /></p>

<p>代码如下：</p>

<pre><code class="language-python">import pickle
import logging
import numpy as np
import mxnet as mx
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define the network
# 定义 data
data = mx.sym.Variable('data')
# data 经过一个两输出的全连接层
fc1 = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=2)
# 再经过一个 sigmoid 激活层
sigmoid1 = mx.sym.Activation(data=fc1, name='sigmoid1', act_type='sigmoid')
# 然后再经过一个全连接层
fc2 = mx.sym.FullyConnected(data=sigmoid1, name='fc2', num_hidden=2)
# 最后经过 Softmax 输出，SoftmaxOutput 还自带 NLL 作为loss 进行计算
mlp = mx.sym.SoftmaxOutput(data=fc2, name='softmax')


# 网络结构可视化，基于 graphviz
shape = {'data': (2,)}
mlp_dot = mx.viz.plot_network(symbol=mlp, shape=shape)
mlp_dot.render('simple_mlp.gv', view=True)


# 接着，我们就开始读取数据来训练这个模型
# 用 pickle 读取数据
with open('../data.pkl', 'rb') as f:
    samples, labels = pickle.load(f)

# 设置 logging 级别，显示训练时候的信息
logging.getLogger().setLevel(logging.DEBUG)

# 对于这个简单例子，就用全量梯度下降法，整个数据集作为一个 batch
batch_size = len(labels)
samples = np.array(samples)
labels = np.array(labels)

# 生成训练数据迭代器
train_iter = mx.io.NDArrayIter(samples, labels, batch_size)

# 利用 mxnet.model.FeedForward.create 训练一个网络
# 迭代 1000 代，学习率 0.1，冲量系数 0.99
model = mx.model.FeedForward.create(
    symbol=mlp,
    X=train_iter,
    num_epoch=1000,
    learning_rate=0.1,
    momentum=0.99)

'''
# mxnet 也提供先设置好参数，然后通过 fit() 方法进行训练的方式。
model = mx.model.FeedForward(
    symbol=mlp,
    num_epoch=1000,
    learning_rate=0.1,
    momentum=0.99)
model.fit(X=train_iter)
'''

# 使用训练好的模型对一个点进行预测
print(model.predict(mx.nd.array([[0.5, 0.5]])))

# 接下来对所有样本和取值范围的平面进行分类，并画出对应的概率可视化的图

# 定义取值范围平面的采样格点，以 0.05 为间隔
X = np.arange(0, 1.05, 0.05)
Y = np.arange(0, 1.05, 0.05)
X, Y = np.meshgrid(X, Y)

# 按照模型可以接受的格式生成每个格点的坐标
grids = mx.nd.array([[X[i][j], Y[i][j]] for i in range(X.shape[0]) for j in range(X.shape[1])])
# 获取模型预测的结果，以标签 1 为结果
grid_probs = model.predict(grids)[:, 1].reshape(X.shape)

# 定义图标
fig = plt.figure('Sample Surface')
ax = fig.gca(projection='3d')

# 画出整个结果的表面
ax.plot_surface(X, Y, grid_probs, alpha=0.15, color='k', rstride=2, cstride=2, lw=0.5)

# 按照标签选出对应的样本
samples0 = samples[labels==0]
samples0_probs = model.predict(samples0)[:, 1]
samples1 = samples[labels==1]
samples1_probs = model.predict(samples1)[:, 1]

# 按照标签画出散点图，标签为 0 的是红色圆，标签为 1 的是蓝色三角
ax.scatter(samples0[:, 0], samples0[:, 1], samples0_probs, c='b', marker='^', s=50)
ax.scatter(samples1[:, 0], samples1[:, 1], samples1_probs, c='r', marker='o', s=50)

plt.show()
</code></pre>

<p>说明：</p>

<ol>
<li><p>在执行程序的时候，我们会得到如图7-2所示的结构图。
<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180831/g95g1gk0BC.png?imageslim" alt="mark" /></p></li>

<li><p>我们用的是 mxnet.io.NDArrayIter 来生成一个用于遍历训练数据和对应标签的迭代器，然后用model进行训练。<span style="color:red;">是不是一定要用这个？</span></p></li>

<li><p>训练过程中可以看到实时输出的loss信息，每迭代完一代（epoch）就进行一次重 新设置：</p>

<pre><code>INFO:root:Start training with [cpu (0)]
INFO:root:Epoch[0] Resetting Data Iterator
INFO:root:Epoch[0] Time cost=0.007
INFO:root:Epoch[1] Resetting Data Iterator
INFO:root:Epoch[1] Time cost=0.002
</code></pre></li>

<li><p>我们会用训练好的模型对所有样本和取值范围的平面进行分类，并画出对应的三维的概率可视化的图如下：
<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180831/AcJGk9GCl5.png?imageslim" alt="mark" /></p></li>
</ol>

<p>对于上面的程序有两个地方想知道：<span style="color:red;">SoftmaxOutput 还自带 NLL 作为loss 是什么意思？冲量系数是什么？了解下。</span></p>

<p>注：这只是最简单的例子，目的是帮助了解 MXNet 的基本使用，并没有设置验证/测试集，训练迭代1000次也只是随便定的。7.2 节将用 Caffe 实现这个例子，而更加实际的例子会从第8章开始。<span style="color:red;">嗯。</span></p>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/12-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/03-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/01-%E6%84%9F%E7%9F%A5%E6%9C%BA/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">01 感知机</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/12-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/11-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/01-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B/">
            <span class="next-text nav-default">01 目标检测算法简介</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
