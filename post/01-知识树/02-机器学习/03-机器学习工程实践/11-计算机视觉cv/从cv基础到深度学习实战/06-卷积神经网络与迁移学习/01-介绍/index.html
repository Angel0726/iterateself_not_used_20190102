<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>01 介绍 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="需要补充的 还是要合并到卷积神经网络的。这种基础的东西放在 cv 里面重复了。而且经典的net可能会用在各种场景下，同意放在基础的机器学习中进行总结" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/%E4%BB%8Ecv%E5%9F%BA%E7%A1%80%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/06-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/01-%E4%BB%8B%E7%BB%8D/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="01 介绍" />
<meta property="og:description" content="需要补充的 还是要合并到卷积神经网络的。这种基础的东西放在 cv 里面重复了。而且经典的net可能会用在各种场景下，同意放在基础的机器学习中进行总结" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/%E4%BB%8Ecv%E5%9F%BA%E7%A1%80%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/06-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/01-%E4%BB%8B%E7%BB%8D/" /><meta property="article:published_time" content="2018-08-18T16:38:00&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-18T16:38:00&#43;00:00"/>
<meta itemprop="name" content="01 介绍">
<meta itemprop="description" content="需要补充的 还是要合并到卷积神经网络的。这种基础的东西放在 cv 里面重复了。而且经典的net可能会用在各种场景下，同意放在基础的机器学习中进行总结">


<meta itemprop="datePublished" content="2018-08-18T16:38:00&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-18T16:38:00&#43;00:00" />
<meta itemprop="wordCount" content="3446">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="01 介绍"/>
<meta name="twitter:description" content="需要补充的 还是要合并到卷积神经网络的。这种基础的东西放在 cv 里面重复了。而且经典的net可能会用在各种场景下，同意放在基础的机器学习中进行总结"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">01 介绍</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-18 </span>
        
        <span class="more-meta"> 3446 words </span>
        <span class="more-meta"> 7 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#需要补充的">需要补充的</a>
<ul>
<li><a href="#数据输入层-input-layer">数据输入层/ Input layer</a></li>
<li><a href="#卷积计算层-conv-layer">卷积计算层/ CONV layer</a></li>
<li><a href="#激励层">激励层</a></li>
<li><a href="#池化层">池化层</a></li>
<li><a href="#全连接层-fc">全连接层 FC</a></li>
<li><a href="#一般cnn结构依次为">一般CNN结构依次为</a></li>
<li><a href="#总结一下">总结一下：</a></li>
</ul></li>
<li><a href="#对卷积层的理解">对卷积层的理解</a></li>
<li><a href="#相关资料">相关资料</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h2 id="需要补充的">需要补充的</h2>

<ul>
<li>还是要合并到卷积神经网络的。这种基础的东西放在 cv 里面重复了。而且经典的net可能会用在各种场景下，同意放在基础的机器学习中进行总结更好些。</li>
</ul>

<p>论文资源：</p>

<ul>
<li>RCNN 链接：<a href="https://pan.baidu.com/s/1LrJ-uQRfjb41I1btvGbecA">https://pan.baidu.com/s/1LrJ-uQRfjb41I1btvGbecA</a> 密码：ks3g</li>
<li>Fast-RCNN 链接：<a href="https://pan.baidu.com/s/1XWm6emUWAoBPK8oE_kY2lw">https://pan.baidu.com/s/1XWm6emUWAoBPK8oE_kY2lw</a> 密码：vxt4</li>
<li>Faster-RCNN 链接：<a href="https://pan.baidu.com/s/1e2i6SSX34pEwG6B52gGsKg">https://pan.baidu.com/s/1e2i6SSX34pEwG6B52gGsKg</a> 密码：gey3</li>
<li>R-FCN  链接：<a href="https://pan.baidu.com/s/1fXvKITAoFPfE1prBolai8Q">https://pan.baidu.com/s/1fXvKITAoFPfE1prBolai8Q</a> 密码：45y3</li>
</ul>

<p>主要讲卷积神经网络和一些迁移学习的内容</p>

<ul>
<li>卷积神经网络快速回顾
1.层级结构 2.数据处理 3.训练算法 4.优缺点</li>
<li>典型CNN
1.AlexNet 2.GoogLeNet 3.VGG Net 4.ResNet</li>
<li>物体定位
1.回归的思路</li>
<li>物体检测
1.早期做法 2.RCNN/Fast-RCNN/Faster_RCNN 3.R-FCN</li>
<li>文艺绘画与Neural Style
1.风格描述 2.主体对调与损失最小化</li>
</ul>

<p>从物体定位到物体检测到Neural Style</p>

<p>我们先回顾一下卷积神经网络</p>

<p>卷积神经网络层级结构</p>

<p>不同层次结构有不同形式(运算)与功能</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/lJ0GaGmbCd.png?imageslim" alt="mark" /></p>

<p>这是一个非常典型的卷积神经网络，现在的有些变更，但是差不多。</p>

<p>主要是以下层次</p>

<ul>
<li>数据输入层/ Input layer</li>
<li>卷积计算层/ CONV layer</li>
<li>ReLU激励层 / ReLU layer  activation layer</li>
<li>池化层 / Pooling layer 有时候会说叫下采样层。</li>
<li>全连接层 / FC layer</li>
<li>Batch Normalization层(可能有)</li>
</ul>

<p>FC 可以在一定程度上完成小部分的信息还原。因为前面做了大量的下采样，大量的参数共享，捕捉到了大的图片信息，比较detal 的信息可能在这个途中有所损失，因此接一个 全连接层可以对信息有一定程度的还原。<span style="color:red;">真的是这样吗？为什么可以有一定程度的还原？为什么detail 的信息会损失？</span></p>

<p>这个 Batch Normalization 是 15年的时候 google 的一篇paper，现在看各种各样的开源的package，包括 caffee tensorflow，等现在都有这个 Batch Normalization 。</p>

<p>我们分别过一下：</p>

<h3 id="数据输入层-input-layer">数据输入层/ Input layer</h3>

<p>有3种常见的图像数据处理方式</p>

<ul>
<li>去均值

<ul>
<li>把输入数据各个维度都中心化到0</li>
</ul></li>
<li>归一化

<ul>
<li>幅度归一化到同样的范围</li>
</ul></li>
<li>PCA/白化

<ul>
<li>用PCA降维</li>
<li>白化是对数据每个特征轴上的幅度归一化</li>
</ul></li>
</ul>

<p>基本上在我们的 image classification 或 object detection 里面，我们对数据做得处理只会有一个去均值，我会把训练集上的图片的均值求出来，然后做训练的时候，把训练集上减去这个均值，预测的时候，也都要减去这个训练集上的均值。</p>

<p>均值的求法有两种，</p>

<ul>
<li>看早期的 AlexNet，它会求出一幅图 在 scale 为同一幅度比如 225*225*3 之后，它把整个训练集的 100万张图片的225*225*3 的矩阵求和，然后除以100万，拿到的均值矩阵是 225*225*3 的维度的。</li>
<li>VGG 是14年在ImageNet上提出来的，在 transferLearning 上非常好用，它只求了三个颜色通道的均值。所以，相当于 100万*255*255 个像素的平均 。所以这是不同的求均值的方法，任何一种都OK。<span style="color:red;">有什么优劣的区别吗？</span></li>
</ul>

<p>所以一般情况下只会做去均值。下面的 归一化和PCA、白化其实只是在其他的场景中可能会用到，在图像中基本不会使用，因为 RGB本身就在同样的范围内，因为不用做 scaling 归一化。</p>

<h3 id="卷积计算层-conv-layer">卷积计算层/ CONV layer</h3>

<p>卷积层的目的是为了抽取特征。不是为了降噪。</p>

<p><span style="color:red;">对了，对于这个地方我还有个问题，之前不是说这种对应位置的相乘是相关运算吗？只有位置反过来的相乘才是卷积吗？为什么这个地方是对应位置相乘但是是卷积呢？对于卷积和相关性的计算的概念还是要确认下的。</span></p>

<ul>
<li>局部关联。每个神经元看做一个filter。</li>
<li>窗口(receptive field)滑动，filter对局部数据计算</li>
<li>涉及概念：

<ul>
<li>深度/depth</li>
<li>步长/stride demo</li>
<li>填充值/zero-padding</li>
</ul></li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/5CdAme5CBJ.png?imageslim" alt="mark" /></p>

<p>cs231n.github.io/assets/conv-demo/index.html</p>

<p>进入到 CONV layer 层的数据已经是经过 resize 到一个尺寸之后再减去均值会后的数据。</p>

<p>卷积层</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/d314di11EF.png?imageslim" alt="mark" /></p>

<p>最左边的三个矩阵标识的是三个图像 7*7 大小 ，之所以是三个矩阵，是因为有三个颜色通道。卷积层这一层有很多个神经元，每个神经元可以看做一个滤波器，对原来的数据进行卷积处理。</p>

<p>depth 就是下一层有多少个神经元在共同完成这个事情，上面这个图像上是两个神经元，这个 depth 是当前的这个卷积层的神经元的个数，也是你下一层会得到的输出的矩阵的厚度。比如上图，因为你有两个神经元，因此，你得到的输出的矩阵是两个。</p>

<p>stride 扫描的时候是重叠还是不重叠，取决于你的滑动窗口的大小和你的步长。</p>

<p>zero-padding 这个东西很重要，因为你从最左侧滑到最右侧，并不一定会滑到边缘，比如你的inputsize 是 7*7 的，你的 窗口是 3*3 的，strde是3，那么你滑到第二次的时候，右边只有一列了，不能滑了，这一列一次也没参加过计算。怎么办呢？可以看到上面的额图中，实际上图像周围是有一圈0 的，因此，当你办法滑动完整个图像的时候，可以补一圈0或者若干圈0 。使得他刚好能从最左侧滑动到最右侧。之所以补的是0 而不是其他的数据，因为 0*w 还是0 ，不会影响到真正的像素的计算的结果。</p>

<p>这个卷积的运算是什么样的呢？</p>

<p>它是对应的位置相乘然后加起来，然后，把三个通道的结果加起来。然后再加一个偏执项，就得到了每个位置的结果。<span style="color:red;">我一直想知道这个输入的时候，每个神经元与对应的三个通道的输入是怎么连接的？事实上我感觉这个用图来画是不好画的，因为这个连接的时候，是这个窗口的权重在滑动的时候是不变的，不能像全连接一样可以画出一个完整的连接，只能画出对这个虚拟的窗口的连接。所以，用数学来表示反而更好一些。</span></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/0hBaIhI68B.png?imageslim" alt="mark" /></p>

<ul>
<li>参数共享机制</li>
<li>假设每个神经元连接数据窗的权重是固定的</li>
<li>固定每个神经元连接权重，可以看做模板 每个神经元只关注一个特性</li>
<li>需要估算的权重个数减少: AlexNet 1亿 =&gt; 3.5w</li>
<li>一组固定的权重和不同窗口内数据做内积: 卷积</li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/5CdAme5CBJ.png?imageslim" alt="mark" /></p>

<p>怎么去体现每个神经元从自己的角度去观察这幅图？主要就是靠参数共享这个机制 。</p>

<h3 id="激励层">激励层</h3>

<p>ReLU</p>

<p>把卷积层输出结果做非线性映射</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/C9BgEBAHib.png?imageslim" alt="mark" /></p>

<p>为什么需要这一层呢？比如有阵风吹过来，但是这个对于人体是没有伤害的，因此这个信息可能不需要给大脑说，但是有的人给了你一拳，这个是需要有强烈的反应的。所以，这个信号需要传递给大脑。</p>

<p>那么什么样的信号需要传递给大脑的？什么样的信号不需要？这里就用了一个激励函数来对传过来的信号进行过滤。<span style="color:red;">实际上，我还是有些不清楚，为什么这个对传进来的信号进行这种形式的过滤是符合道理的？</span></p>

<p>在卷积层后面接上这些激励函数就是对输出做一个限定，到底该不该往后传。</p>

<p>有一些非常常用的接在卷积层后面的激励函数：</p>

<ul>
<li>Sigmoid</li>
<li>Tanh(双曲正切)</li>
<li>ReLU</li>
<li>Leaky ReLU</li>
<li>ELU</li>
<li>Maxout</li>
</ul>

<p>这些都有人在用。目前位置用的最多的还是 ReLU 修正线性单元。</p>

<p>激励层(Sigmoid/Tanh/Tanh/Maxout/ELU)</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/LFglh6e5E2.png?imageslim" alt="mark" /></p>

<p>所有的这些图像都是非线性的。</p>

<p>这个 ReLU 是没有 w 的，但是 Maxout 是有 w 的。</p>

<p>Maxout 是 Leaky ReLU 的推广的一个形式。</p>

<p>在激励层后面接一个池化层。</p>

<h3 id="池化层">池化层</h3>

<p>池化层 / Pooling layer</p>

<ul>
<li>夹在连续的卷积层中间</li>
<li>压缩数据和参数的量，减小过拟合</li>
</ul>

<p>他做得事情就是一个事情：减少数据量。
因为经过卷积之后，我们得到的数据量非常大，数据量多不一定是好事，以为可能带来过拟合的问题。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/9D80aihJ99.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/0HEl3bD4LB.png?imageslim" alt="mark" /></p>

<p>池化层有两种 Pooling 的方式来降维：</p>

<ul>
<li>Max pooling 用一个滑动窗口，在 max pooling 的时候每次从窗口对应的位置中取出最大的。</li>
<li>average pooling 用一个滑动窗口，在 max pooling 的时候每次从窗口对应的位置中取平均值。</li>
</ul>

<p>使用的场景：</p>

<ul>
<li>传统的图像识别 image classifcation 和物体检测 object detection 里面用的都是max pooling</li>
<li>average pooling 在图像识别 image classification 用的不多，但是在其他地方，比如 neural style 里面用的是average pooling</li>
</ul>

<p>注意：老师在这个地方提了下 dropout 不是一层，而是一种正则化的方式。</p>

<p>老师还提到了一种 random pooling ，用的很少，比 average pooling 用的还少。</p>

<h3 id="全连接层-fc">全连接层 FC</h3>

<p>全连接层 / FC layer</p>

<ul>
<li>两层之间所有神经元都有权重连接</li>
<li>通常全连接层在卷积神经网络尾部</li>
</ul>

<p>这个一般是接在尾部的，因为如果接在中间，那么运算量太大了。</p>

<p>但是近几年的神经网络越来越不倾向与使用全连接层了，他们会用很小的卷积比如 1*1 的滑动窗口来代替全连接层。这个 在 object detection 里面频繁使用。<span style="color:red;">不知道现在还是不是这样，还是要确认下的。</span></p>

<h3 id="一般cnn结构依次为">一般CNN结构依次为</h3>

<ul>
<li>INPUT</li>
<li>[[CONV -&gt; RELU]*N -&gt; POOL?]*M</li>
<li>[FC -&gt; RELU]*K</li>
<li>FC</li>
</ul>

<p>注意：[CONV -&gt; RELU]*N -&gt; POOL? 是若干个 卷积层+激励层 然后才接一个 池化层。<span style="color:red;">这个我之前还真的没注意。我一直以为是卷积层+激励层+池化层。</span></p>

<p>而且全连接层后面可能还会接 RELU ，当然最后还是会以全连接层结束掉。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/bbe8e38H55.png?imageslim" alt="mark" /></p>

<h3 id="总结一下">总结一下：</h3>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/bEhm478l4L.png?imageslim" alt="mark" /></p>

<p>这个在 FC 之前是要把 pooling 过来的东西拉平，</p>

<h2 id="对卷积层的理解">对卷积层的理解</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/2HE44EBH5A.png?imageslim" alt="mark" /></p>

<p>第一个卷积层：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180811/jBLCE6k2Fg.png?imageslim" alt="mark" /></p>

<p>可见，可解释性不是那么的强，他做得事情是对的，但是人不明白他为什么要这么做。它的物理含义并不是那么的明确。</p>

<p>到后面的卷积层，里面的内容拿出来看的话人根本没有办法理解。</p>

<ul>
<li>优点

<ul>
<li>共享卷积核，对高维数据处理无压力</li>
<li>无需手动选取特征，训练好权重，即得特征</li>
<li>分类效果好</li>
</ul></li>
<li>缺点

<ul>
<li>需要调参，需要大样本量，训练最好要GPU</li>
<li>物理含义不明确</li>
</ul></li>
</ul>

<h2 id="相关资料">相关资料</h2>

<ul>
<li>七月在线 opencv计算机视觉</li>
</ul>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/%E4%BB%8Ecv%E5%9F%BA%E7%A1%80%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/06-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/02-%E5%87%A0%E4%B8%AA%E5%85%B8%E5%9E%8B%E7%9A%84%E7%BD%91%E7%BB%9C/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">02 几个典型的网络</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/%E4%BB%8Ecv%E5%9F%BA%E7%A1%80%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/05-%E5%9D%90%E6%A0%87%E5%8F%98%E6%8D%A2%E4%B8%8E%E8%A7%86%E8%A7%89%E6%B5%8B%E9%87%8F/03-ar-%E9%A1%B9%E7%9B%AE/">
            <span class="next-text nav-default">03 AR 项目</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
