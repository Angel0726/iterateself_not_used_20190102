<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>03 几种最新方法 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="有同学问了下 关于 visual tracking 有什么方法： 老师说：目前：大概分为两大块： single object tracking multi object tracking 老师只做 single object tracking，每年都有 VOT 的比赛， 做到现在感觉一个是" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/11-%E9%9D%99%E6%80%81%E5%9B%BE%E5%83%8F%E9%A2%86%E5%9F%9F/13-%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-image-captioning/03-%E5%87%A0%E7%A7%8D%E6%9C%80%E6%96%B0%E6%96%B9%E6%B3%95/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="03 几种最新方法" />
<meta property="og:description" content="有同学问了下 关于 visual tracking 有什么方法： 老师说：目前：大概分为两大块： single object tracking multi object tracking 老师只做 single object tracking，每年都有 VOT 的比赛， 做到现在感觉一个是" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/11-%E9%9D%99%E6%80%81%E5%9B%BE%E5%83%8F%E9%A2%86%E5%9F%9F/13-%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-image-captioning/03-%E5%87%A0%E7%A7%8D%E6%9C%80%E6%96%B0%E6%96%B9%E6%B3%95/" /><meta property="article:published_time" content="2018-08-18T16:33:39&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-18T16:33:39&#43;00:00"/>
<meta itemprop="name" content="03 几种最新方法">
<meta itemprop="description" content="有同学问了下 关于 visual tracking 有什么方法： 老师说：目前：大概分为两大块： single object tracking multi object tracking 老师只做 single object tracking，每年都有 VOT 的比赛， 做到现在感觉一个是">


<meta itemprop="datePublished" content="2018-08-18T16:33:39&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-18T16:33:39&#43;00:00" />
<meta itemprop="wordCount" content="4369">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="03 几种最新方法"/>
<meta name="twitter:description" content="有同学问了下 关于 visual tracking 有什么方法： 老师说：目前：大概分为两大块： single object tracking multi object tracking 老师只做 single object tracking，每年都有 VOT 的比赛， 做到现在感觉一个是"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">03 几种最新方法</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-18 </span>
        
        <span class="more-meta"> 4369 words </span>
        <span class="more-meta"> 9 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#show-attend-and-tell">Show, Attend and Tell</a></li>
<li><a href="#neuraltalk">NeuralTalk</a></li>
<li><a href="#densecap">DenseCap</a></li>
<li><a href="#相关资料">相关资料</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<p>有同学问了下 关于 visual  tracking 有什么方法：</p>

<p>老师说：目前：大概分为两大块：</p>

<ul>
<li>single object tracking</li>
<li>multi object tracking</li>
</ul>

<p>老师只做 single object tracking，每年都有 VOT 的比赛，
做到现在感觉一个是 库不够大，现在最大的 VOT 的库也才60多个视频，后面的没听清楚。<span style="color:red;">visual tracking 还是要总结下的。视频追踪</span></p>

<p>OK，下面我们讲一下三篇工作：</p>

<ul>
<li>Show, Attend and Tell</li>
<li>NeuralTalk</li>
<li>DenseCap</li>
</ul>

<h2 id="show-attend-and-tell">Show, Attend and Tell</h2>

<p>我们先回顾一下 RNN for Captioning：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/BaHGKkeIGk.png?imageslim" alt="mark" /></p>

<p>上面是一个普通的 image Captioning 的流程，上面图的记号变了，y 相当于之前我们的 x，d 相当于我们之前的 y。</p>

<p>image 的feature 蕴含了所有的信息，通常来说这个 feature 是 1w 多维的，非常复杂的一个 feature。<span style="color:red;">其实我在想，这个feature 是怎么输入到 RNN 的 hidden state 里面的？</span></p>

<p>之前我们讲过，google 开始做这个的时候 CNN 用的是 googlenet ，结果效果不好，一辆货车坐在铁轨上，然后用了 Resnet 之后效果就很好。</p>

<p>从上面的图可以看出，我们的 RNN 是只看了一次图，然后就继续了。</p>

<p>那么如果 RNN 每一个 timestep 看的是图的不同部位呢？这个的确是一个好问题。</p>

<p>因此 Show, Attend and Tell 这篇文章就开始看看这种做法：</p>

<p>Soft Attention for Captioning</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/66Cf6kmI0d.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/mc2Fe30g1b.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/GF7c3b3E84.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/6HlidBf524.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/lalK5EcCGB.png?imageslim" alt="mark" /></p>

<p>他的思路是这样的：这个特征，作为 h0 之后，输出了 a1，然后 a1 和 feature结合得到了 z1，然后 z1 和输入的 y1 输入进去，输出 a2，这里 a2 和 d1 是一样的，只是画了两块，这个 a2像 a1 一样再与 feature 进行作用。<span style="color:red;">a1 和 feature 是怎么作用得到 z1 的？具体是怎么计算的？老师讲到这个地方没听懂。</span></p>

<p>他这个出发点就是 feature 可以不断地来修饰，而不是只输入一张图。</p>

<p>这个是发在 ICML 2015 的，就是这么个简单的改动，就发在 ICML 了，这个主要是因为它提的这个比较早。</p>

<p>Soft vs Hard Attention</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/HldkAaAE2c.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/mhI4j8gjf2.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/adE7mAJ9ib.png?imageslim" alt="mark" /></p>

<p><span style="color:red;">没怎么看明白，上面的 grid 是怎么划分的？而且 RNN 怎么输出的是这些 grid 的概率的？不是输出的单词的概率 Distribution over vocab 吗？而且，这个 Hard attention 是什么意思？什么叫 according to p,z= that vector ？ 而且怎么用 RL 来训练的？</span></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/lDGlj2eCcg.png?imageslim" alt="mark" /></p>

<p>上图中白色是高值。</p>

<p>可见：</p>

<ul>
<li>对于 Soft attention 来说，这个 a bird flying over a  的时候，鸟上是亮的，然后 body of water 的时候，水是亮的。</li>
<li>对于 hard attention 感觉就不是特别准。都在物体的边缘上。</li>
</ul>

<p>hard attention 应该没什么用吧？<span style="color:red;">确认下。</span></p>

<p><span style="color:red;">说实话，这个关注点为什么亮还是没明白？</span></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/eBmI0F8imB.png?imageslim" alt="mark" /></p>

<p><span style="color:red;">嗯，可见这种 train 出来的结果还是真的不错的，想知道上面这几个图是比特意挑出来的较好的，还是都这么好？上面这个训练的数据量是多大的？看论文确认下。</span></p>

<p>老师说：只是看起来高大上而已，其实就是一个 RNN 在 train。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/eKhijkeG7d.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/Gm6ai43jc5.png?imageslim" alt="mark" /></p>

<p>image captioning 还有一个逆问题，就是给你一句话，能不能找到这个区域，这个老师没讲，链接老师会给的。<span style="color:red;">这个逆问题现在怎么样了？这个我感觉还是很重要的，比如说，打游戏的时候，别人问你 line 出没出跳刀，你首先要找到 line，然后去看有没有跳到。我要知道 line 指代的是这个画面上的这个人物。</span></p>

<p>我们来总结下这篇文章：</p>

<p>最重要的就是这个：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/D9lfDB23bL.png?imageslim" alt="mark" /></p>

<p>通过各种 location ，让每次输入的feature 都不同。<span style="color:red;">还是要仔细看的，没怎么明白细节是怎么实现的。</span></p>

<h2 id="neuraltalk">NeuralTalk</h2>

<p>稍微注意下，这个 NeuralTalk 的名称与论文的名称不是一致的。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/24dGgFBFdc.png?imageslim" alt="mark" /></p>

<p>之前是给出一个 图，然后 CNN 的特征作为 input。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/1a3j2fd3HD.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/D5m53k9H4J.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/ChCCgH8edD.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/eei5DJ0im7.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/mjbkI8f5ED.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/Gk5l3jkHgd.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/357el29I9D.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/j7ihgjJF5I.png?imageslim" alt="mark" /></p>

<p>这个 END 老师说了下，比如有 1000 个词，那么这个 vector 就是 1001，因为还有这个 END 的标识。在 RL 里面也有这个 END。其实只要涉及  这种迭代的训练的，就一定要有什么时候是个头的问题。</p>

<p>他在COCO 上测试，COCO 上大概有 10w 张图，然后，每张图有5个 sentence。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/Ll57G9akiK.png?imageslim" alt="mark" /></p>

<p>COCO 的评价标准是你跟任何一个句子重叠就行。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/gljbaEK1F2.png?imageslim" alt="mark" /></p>

<p>老师说：大家作为起点的话，推荐使用 NeuralTalk。</p>

<p><a href="https://github.com/karpathy/neuraltalk2">neuraltalk2</a> 这个是一个人用 torch 写的一个neural talk。</p>

<p>他的 Readme里面有写了怎么装 torch 和一些训练的 工具包等。</p>

<p>如果你要做 image captioning ，要找代码的话，这个是一个很好的起点。学一些 torch 的基本的东西。</p>

<p>首先，他给了一些图，<a href="https://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html">https://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html</a> 这个是他在所有的 test 上的结果。</p>

<p>我们看一下他主要的函数：</p>

<p>他是用 lua 写的，torch 是用 lua 写的：</p>

<pre><code class="language-lua">require 'torch'
require 'nn'
require 'nngraph'
-- exotic things
require 'loadcaffe'
-- local imports
local utils = require 'misc.utils'
require 'misc.DataLoader'
require 'misc.LanguageModel'
local net_utils = require 'misc.net_utils'
require 'misc.optim_updates'


-- 首先是一些参数的设置
-------------------------------------------------------------------------------
-- Input arguments and options
-------------------------------------------------------------------------------
cmd = torch.CmdLine()
cmd:text()
cmd:text('Train an Image Captioning model')
cmd:text()
cmd:text('Options')

-- Data input settings
cmd:option('-input_h5','coco/data.h5','path to the h5file containing the preprocessed dataset')
cmd:option('-input_json','coco/data.json','path to the json file containing additional info and vocab')
cmd:option('-cnn_proto','model/VGG_ILSVRC_16_layers_deploy.prototxt','path to CNN prototxt file in Caffe format. Note this MUST be a VGGNet-16 right now.')
cmd:option('-cnn_model','model/VGG_ILSVRC_16_layers.caffemodel','path to CNN model file containing the weights, Caffe format. Note this MUST be a VGGNet-16 right now.')
cmd:option('-start_from', '', 'path to a model checkpoint to initialize model weights from. Empty = don\'t')

-- Model settings
cmd:option('-rnn_size',512,'size of the rnn in number of hidden nodes in each layer')
cmd:option('-input_encoding_size',512,'the encoding size of each token in the vocabulary, and the image.')

-- Optimization: General
cmd:option('-max_iters', -1, 'max number of iterations to run for (-1 = run forever)')
cmd:option('-batch_size',16,'what is the batch size in number of images per batch? (there will be x seq_per_img sentences)')
cmd:option('-grad_clip',0.1,'clip gradients at this value (note should be lower than usual 5 because we normalize grads by both batch and seq_length)')
cmd:option('-drop_prob_lm', 0.5, 'strength of dropout in the Language Model RNN')
cmd:option('-finetune_cnn_after', -1, 'After what iteration do we start finetuning the CNN? (-1 = disable; never finetune, 0 = finetune from start)')
cmd:option('-seq_per_img',5,'number of captions to sample for each image during training. Done for efficiency since CNN forward pass is expensive. E.g. coco has 5 sents/image')


-- Optimization: for the Language Model
-- 这个语言模型，他选的是 adam ，而没有选 sgd，一般来说是用 sgd的，但是这个应该是他自己尝试了之后使用的 adam的。
-- 包括一些learning rate 都与普通的不一样，应该都是调出来的。
cmd:option('-optim','adam','what update to use? rmsprop|sgd|sgdmom|adagrad|adam')
cmd:option('-learning_rate',4e-4,'learning rate')
cmd:option('-learning_rate_decay_start', -1, 'at what iteration to start decaying learning rate? (-1 = dont)')
cmd:option('-learning_rate_decay_every', 50000, 'every how many iterations thereafter to drop LR by half?')
cmd:option('-optim_alpha',0.8,'alpha for adagrad/rmsprop/momentum/adam')
cmd:option('-optim_beta',0.999,'beta used for adam')
cmd:option('-optim_epsilon',1e-8,'epsilon that goes into denominator for smoothing')
-- Optimization: for the CNN
cmd:option('-cnn_optim','adam','optimization to use for CNN')
cmd:option('-cnn_optim_alpha',0.8,'alpha for momentum of CNN')
cmd:option('-cnn_optim_beta',0.999,'alpha for momentum of CNN')
cmd:option('-cnn_learning_rate',1e-5,'learning rate for the CNN')
cmd:option('-cnn_weight_decay', 0, 'L2 weight decay just for the CNN')

-- Evaluation/Checkpointing
cmd:option('-val_images_use', 3200, 'how many images to use when periodically evaluating the validation loss? (-1 = all)')
cmd:option('-save_checkpoint_every', 2500, 'how often to save a model checkpoint?')
cmd:option('-checkpoint_path', '', 'folder to save checkpoints into (empty = this folder)')
cmd:option('-language_eval', 0, 'Evaluate language as well (1 = yes, 0 = no)? BLEU/CIDEr/METEOR/ROUGE_L? requires coco-caption code from Github.')
cmd:option('-losses_log_every', 25, 'How often do we snapshot losses, for inclusion in the progress dump? (0 = disable)')

-- misc
cmd:option('-backend', 'cudnn', 'nn|cudnn')
cmd:option('-id', '', 'an id identifying this run/job. used in cross-val and appended when writing progress files')
cmd:option('-seed', 123, 'random number generator seed to use')
cmd:option('-gpuid', 0, 'which gpu to use. -1 = use CPU')

cmd:text()

-------------------------------------------------------------------------------
-- Basic Torch initializations
-------------------------------------------------------------------------------
local opt = cmd:parse(arg)
torch.manualSeed(opt.seed)
torch.setdefaulttensortype('torch.FloatTensor') -- for CPU

if opt.gpuid &gt;= 0 then
  require 'cutorch'
  require 'cunn'
  if opt.backend == 'cudnn' then require 'cudnn' end
  cutorch.manualSeed(opt.seed)
  cutorch.setDevice(opt.gpuid + 1) -- note +1 because lua is 1-indexed
end

-------------------------------------------------------------------------------
-- Create the Data Loader instance
-------------------------------------------------------------------------------
local loader = DataLoader{h5_file = opt.input_h5, json_file = opt.input_json}

-------------------------------------------------------------------------------
-- Initialize the networks
-------------------------------------------------------------------------------
local protos = {}

if string.len(opt.start_from) &gt; 0 then
  -- load protos from file
  print('initializing weights from ' .. opt.start_from)
  local loaded_checkpoint = torch.load(opt.start_from)
  protos = loaded_checkpoint.protos
  net_utils.unsanitize_gradients(protos.cnn)
  local lm_modules = protos.lm:getModulesList()
  for k,v in pairs(lm_modules) do net_utils.unsanitize_gradients(v) end
  protos.crit = nn.LanguageModelCriterion() -- not in checkpoints, create manually
  protos.expander = nn.FeatExpander(opt.seq_per_img) -- not in checkpoints, create manually
else
  -- create protos from scratch
  -- intialize language model
  local lmOpt = {}
  lmOpt.vocab_size = loader:getVocabSize()
  lmOpt.input_encoding_size = opt.input_encoding_size
  lmOpt.rnn_size = opt.rnn_size
  lmOpt.num_layers = 1
  lmOpt.dropout = opt.drop_prob_lm
  lmOpt.seq_length = loader:getSeqLength()
  lmOpt.batch_size = opt.batch_size * opt.seq_per_img
  protos.lm = nn.LanguageModel(lmOpt)
  -- initialize the ConvNet
  local cnn_backend = opt.backend
  if opt.gpuid == -1 then cnn_backend = 'nn' end -- override to nn if gpu is disabled
  local cnn_raw = loadcaffe.load(opt.cnn_proto, opt.cnn_model, cnn_backend)
  protos.cnn = net_utils.build_cnn(cnn_raw, {encoding_size = opt.input_encoding_size, backend = cnn_backend})
  -- initialize a special FeatExpander module that &quot;corrects&quot; for the batch number discrepancy
  -- where we have multiple captions per one image in a batch. This is done for efficiency
  -- because doing a CNN forward pass is expensive. We expand out the CNN features for each sentence
  protos.expander = nn.FeatExpander(opt.seq_per_img)
  -- criterion for the language model
  protos.crit = nn.LanguageModelCriterion()
end

-- ship everything to GPU, maybe
if opt.gpuid &gt;= 0 then
  for k,v in pairs(protos) do v:cuda() end
end

-- flatten and prepare all model parameters to a single vector.
-- Keep CNN params separate in case we want to try to get fancy with different optims on LM/CNN
local params, grad_params = protos.lm:getParameters()
local cnn_params, cnn_grad_params = protos.cnn:getParameters()
print('total number of parameters in LM: ', params:nElement())
print('total number of parameters in CNN: ', cnn_params:nElement())
assert(params:nElement() == grad_params:nElement())
assert(cnn_params:nElement() == cnn_grad_params:nElement())

-- construct thin module clones that share parameters with the actual
-- modules. These thin module will have no intermediates and will be used
-- for checkpointing to write significantly smaller checkpoint files
local thin_lm = protos.lm:clone()
thin_lm.core:share(protos.lm.core, 'weight', 'bias') -- todo: we are assuming that LM has specific members! figure out clean way to get rid of, not modular.
thin_lm.lookup_table:share(protos.lm.lookup_table, 'weight', 'bias')
local thin_cnn = protos.cnn:clone('weight', 'bias')
-- sanitize all modules of gradient storage so that we dont save big checkpoints
net_utils.sanitize_gradients(thin_cnn)
local lm_modules = thin_lm:getModulesList()
for k,v in pairs(lm_modules) do net_utils.sanitize_gradients(v) end

-- create clones and ensure parameter sharing. we have to do this
-- all the way here at the end because calls such as :cuda() and
-- :getParameters() reshuffle memory around.
protos.lm:createClones()

collectgarbage() -- &quot;yeah, sure why not&quot;
-------------------------------------------------------------------------------
-- Validation evaluation
-------------------------------------------------------------------------------
local function eval_split(split, evalopt)
  local verbose = utils.getopt(evalopt, 'verbose', true)
  local val_images_use = utils.getopt(evalopt, 'val_images_use', true)

  protos.cnn:evaluate()
  protos.lm:evaluate()
  loader:resetIterator(split) -- rewind iteator back to first datapoint in the split
  local n = 0
  local loss_sum = 0
  local loss_evals = 0
  local predictions = {}
  local vocab = loader:getVocab()
  while true do

    -- fetch a batch of data
    local data = loader:getBatch{batch_size = opt.batch_size, split = split, seq_per_img = opt.seq_per_img}
    data.images = net_utils.prepro(data.images, false, opt.gpuid &gt;= 0) -- preprocess in place, and don't augment
    n = n + data.images:size(1)

    -- forward the model to get loss
    local feats = protos.cnn:forward(data.images)
    local expanded_feats = protos.expander:forward(feats)
    local logprobs = protos.lm:forward{expanded_feats, data.labels}
    local loss = protos.crit:forward(logprobs, data.labels)
    loss_sum = loss_sum + loss
    loss_evals = loss_evals + 1

    -- forward the model to also get generated samples for each image
    local seq = protos.lm:sample(feats)
    local sents = net_utils.decode_sequence(vocab, seq)
    for k=1,#sents do
      local entry = {image_id = data.infos[k].id, caption = sents[k]}
      table.insert(predictions, entry)
      if verbose then
        print(string.format('image %s: %s', entry.image_id, entry.caption))
      end
    end

    -- if we wrapped around the split or used up val imgs budget then bail
    local ix0 = data.bounds.it_pos_now
    local ix1 = math.min(data.bounds.it_max, val_images_use)
    if verbose then
      print(string.format('evaluating validation performance... %d/%d (%f)', ix0-1, ix1, loss))
    end

    if loss_evals % 10 == 0 then collectgarbage() end
    if data.bounds.wrapped then break end -- the split ran out of data, lets break out
    if n &gt;= val_images_use then break end -- we've used enough images
  end

  local lang_stats
  if opt.language_eval == 1 then
    lang_stats = net_utils.language_eval(predictions, opt.id)
  end

  return loss_sum/loss_evals, predictions, lang_stats
end

-------------------------------------------------------------------------------
-- Loss function
-------------------------------------------------------------------------------
local iter = 0
local function lossFun()
  protos.cnn:training()
  protos.lm:training()
  grad_params:zero()
  if opt.finetune_cnn_after &gt;= 0 and iter &gt;= opt.finetune_cnn_after then
    cnn_grad_params:zero()
  end

  -----------------------------------------------------------------------------
  -- Forward pass
  -----------------------------------------------------------------------------
  -- get batch of data
  local data = loader:getBatch{batch_size = opt.batch_size, split = 'train', seq_per_img = opt.seq_per_img}
  data.images = net_utils.prepro(data.images, true, opt.gpuid &gt;= 0) -- preprocess in place, do data augmentation
  -- data.images: Nx3x224x224
  -- data.seq: LxM where L is sequence length upper bound, and M = N*seq_per_img

  -- forward the ConvNet on images (most work happens here)
  local feats = protos.cnn:forward(data.images)
  -- we have to expand out image features, once for each sentence
  local expanded_feats = protos.expander:forward(feats)
  -- forward the language model
  local logprobs = protos.lm:forward{expanded_feats, data.labels}
  -- forward the language model criterion
  local loss = protos.crit:forward(logprobs, data.labels)

  -----------------------------------------------------------------------------
  -- Backward pass
  -----------------------------------------------------------------------------
  -- backprop criterion
  local dlogprobs = protos.crit:backward(logprobs, data.labels)
  -- backprop language model
  local dexpanded_feats, ddummy = unpack(protos.lm:backward({expanded_feats, data.labels}, dlogprobs))
  -- backprop the CNN, but only if we are finetuning
  if opt.finetune_cnn_after &gt;= 0 and iter &gt;= opt.finetune_cnn_after then
    local dfeats = protos.expander:backward(feats, dexpanded_feats)
    local dx = protos.cnn:backward(data.images, dfeats)
  end

  -- clip gradients
  -- print(string.format('claming %f%% of gradients', 100*torch.mean(torch.gt(torch.abs(grad_params), opt.grad_clip))))
  grad_params:clamp(-opt.grad_clip, opt.grad_clip)

  -- apply L2 regularization
  if opt.cnn_weight_decay &gt; 0 then
    cnn_grad_params:add(opt.cnn_weight_decay, cnn_params)
    -- note: we don't bother adding the l2 loss to the total loss, meh.
    cnn_grad_params:clamp(-opt.grad_clip, opt.grad_clip)
  end
  -----------------------------------------------------------------------------

  -- and lets get out!
  local losses = { total_loss = loss }
  return losses
end

-------------------------------------------------------------------------------
-- Main loop
-------------------------------------------------------------------------------
local loss0
local optim_state = {}
local cnn_optim_state = {}
local loss_history = {}
local val_lang_stats_history = {}
local val_loss_history = {}
local best_score
while true do

  -- eval loss/gradient
  local losses = lossFun()
  if iter % opt.losses_log_every == 0 then loss_history[iter] = losses.total_loss end
  print(string.format('iter %d: %f', iter, losses.total_loss))

  -- save checkpoint once in a while (or on final iteration)
  if (iter % opt.save_checkpoint_every == 0 or iter == opt.max_iters) then

    -- evaluate the validation performance
    local val_loss, val_predictions, lang_stats = eval_split('val', {val_images_use = opt.val_images_use})
    print('validation loss: ', val_loss)
    print(lang_stats)
    val_loss_history[iter] = val_loss
    if lang_stats then
      val_lang_stats_history[iter] = lang_stats
    end

    local checkpoint_path = path.join(opt.checkpoint_path, 'model_id' .. opt.id)

    -- write a (thin) json report
    local checkpoint = {}
    checkpoint.opt = opt
    checkpoint.iter = iter
    checkpoint.loss_history = loss_history
    checkpoint.val_loss_history = val_loss_history
    checkpoint.val_predictions = val_predictions -- save these too for CIDEr/METEOR/etc eval
    checkpoint.val_lang_stats_history = val_lang_stats_history

    utils.write_json(checkpoint_path .. '.json', checkpoint)
    print('wrote json checkpoint to ' .. checkpoint_path .. '.json')

    -- write the full model checkpoint as well if we did better than ever
    local current_score
    if lang_stats then
      -- use CIDEr score for deciding how well we did
      current_score = lang_stats['CIDEr']
    else
      -- use the (negative) validation loss as a score
      current_score = -val_loss
    end
    if best_score == nil or current_score &gt; best_score then
      best_score = current_score
      if iter &gt; 0 then -- dont save on very first iteration
        -- include the protos (which have weights) and save to file
        local save_protos = {}
        save_protos.lm = thin_lm -- these are shared clones, and point to correct param storage
        save_protos.cnn = thin_cnn
        checkpoint.protos = save_protos
        -- also include the vocabulary mapping so that we can use the checkpoint
        -- alone to run on arbitrary images without the data loader
        checkpoint.vocab = loader:getVocab()
        torch.save(checkpoint_path .. '.t7', checkpoint)
        print('wrote checkpoint to ' .. checkpoint_path .. '.t7')
      end
    end
  end

  -- decay the learning rate for both LM and CNN
  local learning_rate = opt.learning_rate
  local cnn_learning_rate = opt.cnn_learning_rate
  if iter &gt; opt.learning_rate_decay_start and opt.learning_rate_decay_start &gt;= 0 then
    local frac = (iter - opt.learning_rate_decay_start) / opt.learning_rate_decay_every
    local decay_factor = math.pow(0.5, frac)
    learning_rate = learning_rate * decay_factor -- set the decayed rate
    cnn_learning_rate = cnn_learning_rate * decay_factor
  end

  -- perform a parameter update
  if opt.optim == 'rmsprop' then
    rmsprop(params, grad_params, learning_rate, opt.optim_alpha, opt.optim_epsilon, optim_state)
  elseif opt.optim == 'adagrad' then
    adagrad(params, grad_params, learning_rate, opt.optim_epsilon, optim_state)
  elseif opt.optim == 'sgd' then
    sgd(params, grad_params, opt.learning_rate)
  elseif opt.optim == 'sgdm' then
    sgdm(params, grad_params, learning_rate, opt.optim_alpha, optim_state)
  elseif opt.optim == 'sgdmom' then
    sgdmom(params, grad_params, learning_rate, opt.optim_alpha, optim_state)
  elseif opt.optim == 'adam' then
    adam(params, grad_params, learning_rate, opt.optim_alpha, opt.optim_beta, opt.optim_epsilon, optim_state)
  else
    error('bad option opt.optim')
  end

  -- do a cnn update (if finetuning, and if rnn above us is not warming up right now)
  if opt.finetune_cnn_after &gt;= 0 and iter &gt;= opt.finetune_cnn_after then
    if opt.cnn_optim == 'sgd' then
      sgd(cnn_params, cnn_grad_params, cnn_learning_rate)
    elseif opt.cnn_optim == 'sgdm' then
      sgdm(cnn_params, cnn_grad_params, cnn_learning_rate, opt.cnn_optim_alpha, cnn_optim_state)
    elseif opt.cnn_optim == 'adam' then
      adam(cnn_params, cnn_grad_params, cnn_learning_rate, opt.cnn_optim_alpha, opt.cnn_optim_beta, opt.optim_epsilon, cnn_optim_state)
    else
      error('bad option for opt.cnn_optim')
    end
  end

  -- stopping criterions
  iter = iter + 1
  if iter % 10 == 0 then collectgarbage() end -- good idea to do this once in a while, i think
  if loss0 == nil then loss0 = losses.total_loss end
  if losses.total_loss &gt; loss0 * 20 then
    print('loss seems to be exploding, quitting.')
    break
  end
  if opt.max_iters &gt; 0 and iter &gt;= opt.max_iters then break end -- stopping criterion

end
</code></pre>

<p>老师说这个是一个很规范的 lua 的 project。</p>

<p>CNN 里面做传统的 task ，最好的还是用 sgd，你在训练的时候，首先还是要用 sgd，如果不要用，你可以选 rmsprop。</p>

<p><span style="color:red;">rmsprop,sgd,sgdmom,adagrad,adam 这些还是要好好总结下。</span></p>

<h2 id="densecap">DenseCap</h2>

<p>今天最重要的就是这个 DenseCap</p>

<p>我们看一下 DenseCap 的之前的工作：</p>

<p>Dense Captioning: Prior Work</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/7lbhBgC999.png?imageslim" alt="mark" /></p>

<p>这个有什么问题呢？</p>

<ul>
<li>No context. 预测没有包含每个区域外面的场景 The prediction does not include context outside of each region</li>
<li>Inefficient 我们必须独立的前向训练每个 region。 We must forward every region independently</li>
<li>Not end-to-end  我们的 external region 的proposal，经常是用别的数据集训练的。 The use of external region proposals, often trained on other datasets (e.g. object datasets)</li>
</ul>

<p>那么这个论文怎么做得呢？</p>

<p>他做了 End-to-end learning:</p>

<p>Formulate a single differentiable function from inputs to outputs.</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/72am1fA3fg.png?imageslim" alt="mark" /></p>

<p>怎么做的呢？</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/9BA6K41FA5.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/5f9aIL5bFi.png?imageslim" alt="mark" /></p>

<p>我把 它拿出来。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/je50f4keal.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/B6J03fCd5I.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/102g8H3C95.png?imageslim" alt="mark" /></p>

<p>就是就是做 RPN，这块没有什么创新。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/DBfdl8jeIj.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/B6ECd8D2eB.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/G77mAjEaI3.png?imageslim" alt="mark" /></p>

<p>把蓝的 crop 一下，crop 老师说应该是写错了，应该是压缩一下。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/gf1E6276mK.png?imageslim" alt="mark" /></p>

<p>他为什么要这么做呢？他这么做其实是要做 Box regression 和 Box classification，你要 minimize fize loss</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/GFAA1DKKKh.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/kfebif0ecC.png?imageslim" alt="mark" /></p>

<p>1、2 两个loss 是 localization 负责的，3、4 两个loss 是 recognition 负责的，那么他们有什么区别呢？老师说input 不一样吧。没听明白。</p>

<p>为什么叫 Dense Captioning 呢？</p>

<p>因为他对每个 region 都有一个描述，之前就是一幅图就描述完了。</p>

<p>他解决了一些问题：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/cLdj5m4cBa.png?imageslim" alt="mark" /></p>

<ol>
<li>周围的一圈都已经 crop 了</li>
<li>因为你用 ROI pooling ，这样你就不用重新计算了，你的ROI 的feature 就直接用前面的feature 就行了。</li>
<li>因为你从头到尾都用的是RPN，因此给的就是一张图，就不用 select search ，xbox 什么的。</li>
</ol>

<p>下面是一个场景的例子：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/L1E7fe6Kl7.png?imageslim" alt="mark" /></p>

<p>这个是stanford 的实验室场景，老师说，跟国内实验室完全都一样。世界上大学研究室都一样。</p>

<p>这个印度人就是 visual genenone 的主要设计者。都是 feifei li 的学生。</p>

<p>有人问，context 怎么加的，是 proposal 的两倍吗？这个不一定，要看具体的文章细节，老师说好像是1.2倍。<span style="color:red;">什么意思？没懂？</span></p>

<p>如果你从数值上看：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180816/clhieJJ6kg.png?imageslim" alt="mark" /></p>

<p><span style="color:red;">这个图也没看懂。</span></p>

<h2 id="相关资料">相关资料</h2>

<ul>
<li>七月在线 opencv计算机视觉</li>
</ul>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/11-%E9%9D%99%E6%80%81%E5%9B%BE%E5%83%8F%E9%A2%86%E5%9F%9F/13-%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-image-captioning/02-rnn-%E5%92%8C-lstm-/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">02 RNN 和 LSTM</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89cv/11-%E9%9D%99%E6%80%81%E5%9B%BE%E5%83%8F%E9%A2%86%E5%9F%9F/13-%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0-image-captioning/01-%E4%BB%8B%E7%BB%8D/">
            <span class="next-text nav-default">01 介绍</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
