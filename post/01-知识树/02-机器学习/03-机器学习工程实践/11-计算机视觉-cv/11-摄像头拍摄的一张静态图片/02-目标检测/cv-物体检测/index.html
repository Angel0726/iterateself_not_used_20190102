<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>cv 物体检测 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="cv 物体检测 缘由： 终于讲到我想要知道的东西了，一直想知道那种再行人走动的时候把人圈出来的是怎么做到的。以前我做过那种投影flash到地面上与走" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-cv/11-%E6%91%84%E5%83%8F%E5%A4%B4%E6%8B%8D%E6%91%84%E7%9A%84%E4%B8%80%E5%BC%A0%E9%9D%99%E6%80%81%E5%9B%BE%E7%89%87/02-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/cv-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="cv 物体检测" />
<meta property="og:description" content="cv 物体检测 缘由： 终于讲到我想要知道的东西了，一直想知道那种再行人走动的时候把人圈出来的是怎么做到的。以前我做过那种投影flash到地面上与走" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-cv/11-%E6%91%84%E5%83%8F%E5%A4%B4%E6%8B%8D%E6%91%84%E7%9A%84%E4%B8%80%E5%BC%A0%E9%9D%99%E6%80%81%E5%9B%BE%E7%89%87/02-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/cv-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B/" /><meta property="article:published_time" content="2018-08-16T19:00:26&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-16T19:00:26&#43;00:00"/>
<meta itemprop="name" content="cv 物体检测">
<meta itemprop="description" content="cv 物体检测 缘由： 终于讲到我想要知道的东西了，一直想知道那种再行人走动的时候把人圈出来的是怎么做到的。以前我做过那种投影flash到地面上与走">


<meta itemprop="datePublished" content="2018-08-16T19:00:26&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-16T19:00:26&#43;00:00" />
<meta itemprop="wordCount" content="5261">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="cv 物体检测"/>
<meta name="twitter:description" content="cv 物体检测 缘由： 终于讲到我想要知道的东西了，一直想知道那种再行人走动的时候把人圈出来的是怎么做到的。以前我做过那种投影flash到地面上与走"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">最近</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">最近</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">cv 物体检测</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-16 </span>
        
        <span class="more-meta"> 5261 words </span>
        <span class="more-meta"> 11 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#cv-物体检测">cv 物体检测</a></li>
<li><a href="#缘由">缘由：</a></li>
<li><a href="#物体检测问题概述">物体检测问题概述</a></li>
<li><a href="#物体识别的数据库与比赛">物体识别的数据库与比赛</a></li>
<li><a href="#比较关键的几个deep-models">比较关键的几个Deep Models：</a></li>
<li><a href="#经典方法-dpm">经典方法：  DPM</a></li>
<li><a href="#deep-learning">Deep Learning</a>
<ul>
<li><a href="#deep-learning-参数更新-loss设计">Deep Learning - 参数更新、loss设计：</a></li>
<li><a href="#总结-深度学习三要素-when-you-read-papers">总结：深度学习三要素（when you read papers）</a></li>
</ul></li>
<li><a href="#怎么用深度学习做物体检测">怎么用深度学习做物体检测</a>
<ul>
<li><a href="#怎么用深度学习做物体检测-1">怎么用深度学习做物体检测？</a></li>
<li><a href="#那么-区域提名-region-proposal-是怎么做出来的呢-怎么画出的目标框">那么，区域提名（Region Proposal）是怎么做出来的呢？怎么画出的目标框？</a></li>
<li><a href="#一些-region-proposals-方法">一些  region proposals 方法</a></li>
<li><a href="#那么用-dl-怎么做-region-proposal">那么用 DL 怎么做 Region Proposal？</a></li>
</ul></li>
<li><a href="#rcnn-的整体流程">RCNN 的整体流程</a>
<ul>
<li><a href="#step-1-train-or-download-a-classification-model-for-imagenet-resnet-101">Step 1: Train (or download) a classification model for ImageNet (ResNet-101)</a></li>
<li><a href="#step-2-fine-tune-model-for-detection">Step 2: Fine-tune model for detection</a></li>
<li><a href="#step-3-extract-features">Step 3: Extract features</a></li>
<li><a href="#step-4-train-one-binary-svm-per-class-to-classify-region-features">Step 4: Train one binary SVM per class to classify region features</a></li>
<li><a href="#step-5-bbox-regression">Step 5 (bbox regression):</a></li>
<li><a href="#rcnn-bottlenecks-瓶颈">RCNN bottlenecks 瓶颈</a></li>
</ul></li>
<li><a href="#fast-rcnn">Fast-RCNN</a>
<ul>
<li><a href="#测试的过程-test">测试的过程 test</a></li>
<li><a href="#训练-train">训练 train</a></li>
<li><a href="#fast-rcnn-roi-pooling-details-how-to-back-prop">Fast-RCNN ROI-pooling details：how to back-prop？</a></li>
<li><a href="#fast-rcnn-bbox-regression-loss">Fast-RCNN bbox regression loss</a></li>
</ul></li>
<li><a href="#faster-rcnn">Faster RCNN</a>
<ul>
<li><a href="#rpn具体细节">RPN具体细节：</a>
<ul>
<li><a href="#train-过程">Train 过程</a></li>
<li><a href="#test-过程">Test 过程</a></li>
</ul></li>
<li><a href="#rpn实验结果">RPN实验结果</a></li>
</ul></li>
<li><a href="#最新方法概述">最新方法概述：</a>
<ul>
<li><a href="#最新方法-rfcn">最新方法：RFCN：</a></li>
</ul></li>
<li><a href="#值得思考的问题">值得思考的问题：</a>
<ul>
<li>
<ul>
<li><a href="#joint-training">Joint Training</a></li>
<li><a href="#multi-depth-loss">Multi-depth Loss</a></li>
<li><a href="#stacked-hourglass-architecture">Stacked-Hourglass Architecture</a></li>
</ul></li>
</ul></li>
<li><a href="#comment">COMMENT：</a>
<ul>
<li><a href="#相关资料">相关资料：</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="cv-物体检测">cv 物体检测</h1>

<h1 id="缘由">缘由：</h1>

<p>终于讲到我想要知道的东西了，一直想知道那种再行人走动的时候把人圈出来的是怎么做到的。以前我做过那种投影flash到地面上与走过的人互动的，一直想用这种厉害的东西。。但是最后还是选择了一个最简单的方法。。因此非常想知道。</p>

<h1 id="物体检测问题概述">物体检测问题概述</h1>

<p>物体识别</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/lg976294Cl.png?imageslim" alt="mark" /></p>

<p>下面这四张图片分别对应的四种不同的要求：越来越难<strong>  </strong><strong>厉害  想知道这些都是怎么做到的。</strong></p>

<p>图片识别                       图片识别+定位                          物体检测                                图像分割</p>

<p>Classification                  Localization                         Object Detection            Instance Segmentation</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/7bAh7g3A9E.png?imageslim" alt="mark" /></p>

<p>左边两种应用的场景特点：</p>

<ul>
<li><p>Single object, single class</p></li>

<li><p>Large-scale, millions level</p></li>
</ul>

<p>右边两种应用的场景的特点：</p>

<ul>
<li><p>Multiple objects and classes</p></li>

<li><p>Thousands level</p></li>
</ul>

<p>其实还有更细致的 ：Matting <strong>这个现在是怎么做的？</strong></p>

<h1 id="物体识别的数据库与比赛">物体识别的数据库与比赛</h1>

<ul>
<li><p>The renowned ImageNet ILSVRC Challenge <a href="http://image-net.org/">http://image-net.org/</a> 最著名的 100多万张图片。 可能训练1周    物体检测的标准比赛</p></li>

<li><p>COCO Common Objects Dataset <a href="http://mscoco.org/">http://mscoco.org/</a> COCO只有20多万张，基本是真实世界里的，场景非常复杂 可能训练3~4天</p></li>

<li><p>SUN <a href="http://groups.csail.mit.edu/vision/SUN/">http://groups.csail.mit.edu/vision/SUN/</a></p></li>

<li><p>Pascal VOC:<a href="http://host.robots.ox.ax.uk/pascal/VOC/">http://host.robots.ox.ax.uk/pascal/VOC/</a> 这个只有20类，但是可以快速做实验，训练1天差不多。  <strong>VOC就是Visual Object Class</strong></p></li>

<li><p>CIFAR 这个数据量比较小，非常小的32*32 的，主要研究 network architecture，即研究网络的结构怎么变化的 。<strong>什么是network architecture？ </strong></p></li>
</ul>

<h1 id="比较关键的几个deep-models">比较关键的几个Deep Models：</h1>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/J3fHGd4EhD.png?imageslim" alt="mark" /></p>

<h1 id="经典方法-dpm">经典方法：  DPM</h1>

<p>Deformable Parts Model</p>

<p><strong>这个方法到底是怎么做的？现在还这么做吗？</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/IdmiKAf367.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/km3mIBkH3b.png?imageslim" alt="mark" />
<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/kB8fa28CId.png?imageslim" alt="mark" /></p>

<p>涉及到两篇论文：</p>

<ul>
<li><p>Felzenszwalbet al, “Object Detection with Discriminatively Trained Part Based Models”, PAMI 2010 <a href="http://www.rossgirshick.info/">link</a></p></li>

<li><p>Girschicket al, “Deformable Part Models are Convolutional Neural Networks”, CVPR 2015</p></li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/0gH5gFfmd0.png?imageslim" alt="mark" /></p>

<p>提出了一系列非常经典的做法，包括：
1. 如何应用stochastic gradient descent (SGD) 到training里。
2. NMS (non-maximum suppression) 对后期 testing 的处理非常重要。<strong>什么是NMS？极大值抑制，非常有用的，所有的与识别相关的都要用到NMS。</strong>
3. Data mining hard examples这些概念至今仍在使用。<strong>即 score为1的下次就不再参与训练，训练0.5左右的。这些就是hard的。实际中是怎么操作的？</strong></p>

<h1 id="deep-learning">Deep Learning</h1>

<p>为什么要用卷积？</p>

<ul>
<li><p>本质是：因为图像有invariance，即图像的不变性，跟文字是不同的，文字是有时序性的，而图像倒着正着都是鸟，所以我可以用相同的参数，就是卷积，在图像上的不同位置来滑动，因为图像有invariance，所以我才可以用卷积操作。<strong>这个要注意，之前不知道为什么可以用卷积</strong></p></li>

<li><p>其次才是可以降低计算量和参数量</p></li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/53741a1gJ3.png?imageslim" alt="mark" /></p>

<p>以前，怕纯CNN不带好，因此最后总是加上FC，但是现在很多都是纯CNN的model了，只不过最后的CNN用的是1*1的卷积。1*1的卷积有什么意义呢？1*1不会减少参数的，<strong>不知道。</strong></p>

<p>Loss over the whole dataset:</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/G770EhJlbH.png?imageslim" alt="mark" /></p>

<p>后面的是为了防止过拟合</p>

<p>In each solver iteration, we use a stochastic approximation of this objective, drawing a mini-batch of N &lt;&lt; |D| instances:</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/g8Ec2HgihI.png?imageslim" alt="mark" /></p>

<h2 id="deep-learning-参数更新-loss设计">Deep Learning - 参数更新、loss设计：</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/Imd9ea31A7.png?imageslim" alt="mark" /></p>

<p>Data term: error averagedover instances</p>

<p>Regularizationterm: penalizelarge weights to improve generalization</p>

<p>Stochastic Gradient Descent (known as Solver)</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/e0B1DddII5.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/CbcDL8CBlk.png?imageslim" alt="mark" /></p>

<p>对于一个AlexNet来说，
solver.prototxt里的设计如下：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/33e2AjH4d1.png?imageslim" alt="mark" /></p>

<p><a href="http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1HingeLossLayer.html">http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1HingeLossLayer.html</a></p>

<h2 id="总结-深度学习三要素-when-you-read-papers">总结：深度学习三要素（when you read papers）</h2>

<p>读文论就是读这三个方面，而任何一个方面，你做的好了，都可以发论文。</p>

<ol>
<li><p>模型 (structure, VGG, GoogleNet-BN, ResNet, etc.)</p></li>

<li><p>数据 (dataset statistics, ImageNet, COCO, SUN) 关注的数据量，还有比如人脸检测，从网上下载的有一些合成的图片，假的人脸照片，怎么把这些去除掉？对数据的要求和本身的分析是比较重要的。</p></li>

<li><p>算法</p>

<ol>
<li><p>训练过程 (loss, back-prop, sampling)</p></li>

<li><p>测试过程 (scale, NMS, post-processing)</p></li>
</ol></li>
</ol>

<h1 id="怎么用深度学习做物体检测">怎么用深度学习做物体检测</h1>

<p>是怎么联系的？是怎么想到的？实际上一篇论文submit的时候就已经过时了，这时候你就要想对于这个论文，what can you do in the future。因此每一篇论文的motivation是怎么想到的，这个是比较重要的。</p>

<h2 id="怎么用深度学习做物体检测-1">怎么用深度学习做物体检测？</h2>

<p>问题：给出一张图，识别图中的物体类别及位置。<strong>提示：用分类做检测？</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/dKciGl8gEa.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/fD9BH1GC44.png?imageslim" alt="mark" /></p>

<p>其实它是这么做的：</p>

<ol>
<li><p>先做区域提名（Region Proposal），也就是找出一些可能的感兴趣区域（Region Of Interest, ROI）。</p></li>

<li><p>然后使用分类模型，对ROI进行分类，比如说花这个框分类到花的概率是0.8 ，那么我们就知道这个框里面可能是花。这个分类可以用ResNet等等。</p></li>
</ol>

<p>所以，就把识别的问题detect，转换为了分类的问题 classification。<strong>利害呀</strong> 可见，本身其实没有这么神秘。实际上第一步中会找到很多框 2000多个？只是把后面分类得分高的几个画了出来。</p>

<h2 id="那么-区域提名-region-proposal-是怎么做出来的呢-怎么画出的目标框">那么，区域提名（Region Proposal）是怎么做出来的呢？怎么画出的目标框？</h2>

<p>一种方法是 Selective search method: bottom-up segmentation；  it merges regions at multiple scales and converts regions to boxes.   这个是<strong>非DL的方法</strong>。 <strong> super pixel 与selective search 有什么关系？</strong></p>

<p>有一个方法是：super pixel ，将图像上的像素进行聚类，逐渐的聚的越来越大，比如根据color的distance 或者hot的distance。<strong>到底实际是怎么聚的？有没有代码？</strong></p>

<p>###</p>

<p>computer vision 最大的哲学理念就是multiple scale 这个，即多尺度。它能解决很多问题，一个是看到一个物体在不同的视角；一个是把data <strong>tation做的非常好了，就是一张图变成很多张图。</strong>这个还想再了解下。**</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/6ggIJa51C0.png?imageslim" alt="mark" /></p>

<p>Uijlingset al, “Selective Search for Object Recognition”, IJCV 2013</p>

<h2 id="一些-region-proposals-方法">一些  region proposals 方法</h2>

<p>做物体检测的一些常用的方法。如果工程里用，最简单粗暴，说明天就要交这个任务，SelectiveSearch是最有效的，是非DL的方法。</p>

<p>而DL的方法，就比如fast RCNN 等很多。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/0JD5afFhc8.png?imageslim" alt="mark" /></p>

<p>这个表在下面这个文档中：</p>

<p>Hosang et al, “What makes for effective detection proposals?”, PAMI 2015</p>

<h2 id="那么用-dl-怎么做-region-proposal">那么用 DL 怎么做 Region Proposal？</h2>

<p>上面提到的Selective Search 实际上不是DL的方法</p>

<p>Region Proposasl Network</p>

<p>这是一个feature map，有一个中间点，有9个anchor，anchor box rpn <strong>什么是rpn？没明白这张图。</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/1bH370ik44.png?imageslim" alt="mark" /></p>

<p>其实就是 iteratively/ recursively 做一遍 sliding window, 把feature map上的每一个点，遍历搜索，设计出两种loss： <strong>没有很明白</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/LcK4jhC5Ii.png?imageslim" alt="mark" /></p>

<p>DL 的重要方法：RCNN，Fast-RCNN，Faster-RCNN：</p>

<h1 id="rcnn-的整体流程">RCNN 的整体流程</h1>

<p>pipeline 就是整体的流程</p>

<p>R是region的意思。为什么要warped一下？因为当时大家的网络最后一层都是一个FC，它的输入是fixed的，由于要求这最后的输入是一样的，所以最前面的输入也需要一样。后面发展到全卷积网络就不需要这个warped的操作。由于当时全都是加一个FC，因此加了warped。<strong>这也就是为什么1*1的卷积有他自己的用处，因为它虽然等于FC，但是允许你输入任意变化。</strong><strong>利害</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/4JeGj6L7KE.png?imageslim" alt="mark" /></p>

<p>当时它只把DL作为特征提取，然后做SVM，而且forward 2000遍，</p>

<h2 id="step-1-train-or-download-a-classification-model-for-imagenet-resnet-101">Step 1: Train (or download) a classification model for ImageNet (ResNet-101)</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/738G9lIcE9.png?imageslim" alt="mark" /></p>

<h2 id="step-2-fine-tune-model-for-detection">Step 2: Fine-tune model for detection</h2>

<ul>
<li><p>Instead of 1000 ImageNet classes, want 20 object classes + background （21是因为有个background）</p></li>

<li><p>Throw away final fully-connected layer, reinitialize from scratch</p></li>

<li><p>Keep training model using positive / negative regions from detection images</p></li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/0BLB89ifhi.png?imageslim" alt="mark" /></p>

<h2 id="step-3-extract-features">Step 3: Extract features</h2>

<ul>
<li><p>Extract region proposals for all images</p></li>

<li><p>For each region: warp to CNN input size, run forward through CNN, (save pool5 features to disk) <strong>什么是 pool5 features ？ 为什么要存起来？</strong></p></li>

<li><p>Have a big hard drive: features are ~200GB for PASCAL dataset! <strong>为什么会要这么多的存储空间？</strong></p></li>
</ul>

<p>虽然有些比较笨重的缺点，但是当时是第一个用DL的方法来做 object detection 的。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/99feB6AGDF.png?imageslim" alt="mark" /></p>

<h2 id="step-4-train-one-binary-svm-per-class-to-classify-region-features">Step 4: Train one binary SVM per class to classify region features</h2>

<p>相当于把softmax用SVM来做</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/6DjkgJ7hj7.png?imageslim" alt="mark" /></p>

<h2 id="step-5-bbox-regression">Step 5 (bbox regression):</h2>

<p>For each class, train a linear regression model to map from cached features to offsets to GT boxes to make up for “slightly wrong” proposals。</p>

<p>即只要有offset，他就把框动一动</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/1Hm0aE9beB.png?imageslim" alt="mark" /></p>

<p>RCNN results：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/jd6C9315b4.png?imageslim" alt="mark" /></p>

<p>两个结论：</p>

<ul>
<li><p>如果加了bbox reg 会有提高</p></li>

<li><p>网络越深，效果越好 。（当然，库也要大，至少10万张图）</p></li>
</ul>

<h2 id="rcnn-bottlenecks-瓶颈">RCNN bottlenecks 瓶颈</h2>

<ol>
<li><p>Slow at test-time: need to run full forward pass of CNN for each region proposal;</p></li>

<li><p>SVMs and regressors are post-hoc: CNN features not updated in response to SVMs and regressors; <strong>post-hoc 是什么？即不是end-to-end的，不是嵌在深度学习这个框架里面的，而是后面单独弄的一个。</strong></p></li>

<li><p>Complex multistage training pipeline.</p></li>
</ol>

<h1 id="fast-rcnn">Fast-RCNN</h1>

<h2 id="测试的过程-test">测试的过程 test</h2>

<p>所有的2000个框，都共享一个feature层，在 conv5 的 feature map 的时候再把它分开。这样大大减轻了计算量。就不用2000次了。<strong>这个到底实际是怎么做的？</strong>在 conv5之后的一些卷积或者FC也还是2000个forward，但是相比之前的RCNN，运算量已经减少很多了。</p>

<p>然后他提出了一个方法 RoI Pooling，它实际上就是feature 层的wraped操作。</p>

<p>而且他把SVM嵌到DL里面了，<strong>这个地方没明白？怎么嵌入的？而且从FCs出来的两个是什么？</strong></p>

<p><strong>图上的SPP是什么？ 而且在CONV的过程中，RoI是如何跟进的？</strong></p>

<p><strong>而且对于RoI的BP是怎么做的？</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/JLcAab532E.png?imageslim" alt="mark" /></p>

<p>Solution：Share computation of convolutional layers between proposals.</p>

<h2 id="训练-train">训练 train</h2>

<p>loss 是 smooth L1 和 softmax 加起来的。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/mDg3B4JlG9.png?imageslim" alt="mark" /></p>

<p>Train end-to-end：</p>

<p>解决了RCNN 重复计算每个区域的feature 问题。本质上是提供了ROI-pooling layer, 使任意大小的输入都能以固定大小输出。由于没有wraped操作，所以没有破坏长宽比。<strong>没有破坏吗？</strong></p>

<h2 id="fast-rcnn-roi-pooling-details-how-to-back-prop">Fast-RCNN ROI-pooling details：how to back-prop？</h2>

<p><strong>这一节没看懂，很重要</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/fjJLb1Eha1.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/kFiebCa9h2.png?imageslim" alt="mark" /></p>

<p>为什么又两项求和呢？原来是 28*28 的，</p>

<p>i是上一层的位置，j是这一层的位置 ，虽然是二维的，但是它拉成一个向量了，为了简写就这样了。</p>

<p>回顾一下深度学习课程中，如何计算层与层之间的梯度的？</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/E6gieII954.png?imageslim" alt="mark" /></p>

<h2 id="fast-rcnn-bbox-regression-loss">Fast-RCNN bbox regression loss</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/EK8f0j70hg.png?imageslim" alt="mark" /></p>

<p>t是target v是prediction，<strong>没明白</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/G7K3JA6iAB.png?imageslim" alt="mark" /></p>

<p><a href="https://github.com/ShaoqingRen/caffe/blob/062f2431162165c658a42d717baf8b74918aa18e/src/caffe/layers/smooth_L1_loss_layer.cu">https://github.com/ShaoqingRen/caffe/blob/062f2431162165c658a42d717baf8b74918aa18e/src/caffe/layers/smooth_L1_loss_layer.cu</a></p>

<p>那么具体的target (Ground Truth)坐标怎么算呢？</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/lDjl5chEff.png?imageslim" alt="mark" /></p>

<p>真实坐标减去已经有的坐标，然后除以宽度。我们预测的就是这个宽度</p>

<p>其实是预测差值。x - x_a (given box), x 是新的位置，t_x是偏移量。<strong>没明白？没有讲的很细。</strong></p>

<h1 id="faster-rcnn">Faster RCNN</h1>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/JH71CJEbGE.png?imageslim" alt="mark" /></p>

<p>又4个loss，rpn里面两个，一个是regression一个是classification 然后classificer里面又两个loss，也是一个regression和一个classification。</p>

<p>与Fast的区别，fast没有rpn这个过程，faster把selective search也嵌入到network里面了，forward一次，2000个框就有了，fast提框需要20s，faster需要1~2s这2000个框就有了。</p>

<p>使用CNN来直接生成region-proposal.不用外部模型！但两个网络如何share parameter?</p>

<p><a href="https://github.com/rbgirshick/py-faster-rcnn">https://github.com/rbgirshick/py-faster-rcnn</a></p>

<h2 id="rpn具体细节">RPN具体细节：</h2>

<p>rpn只有两类 前景和后景？ <strong>什么？</strong></p>

<h3 id="train-过程">Train 过程</h3>

<ol>
<li><p>对于一张图，scale到某一尺度上(from 500 -&gt; 800)</p></li>

<li><p>计算anchor 大小，生成output map:</p>

<ul>
<li>600 x 800 -&gt; 35 x 24 x 15 (anchors) ~ 2w anchors  其中 35是600除以16，15是自己定义的， 2W个框经过NMS就2000个了。<strong>什么是NMS？</strong></li>
</ul></li>

<li><p>计算bbox_regression_target, 准备input_blob</p></li>

<li><p>Forward, backward, 更新网络参数。</p></li>
</ol>

<p>找到这四步骤对应的函数，对于理解整个算法非常重要。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/7F03EkdeEG.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/1K13Im20Ch.png?imageslim" alt="mark" /></p>

<h3 id="test-过程">Test 过程</h3>

<ol>
<li><p>对于一张图，scale到某一尺度上(from 500 -&gt; 800)</p></li>

<li><p>Forward一遍，得出每个点的score, bbox_regressiontarget, 计算box大小，scale back to original image.</p></li>

<li><p>NMS等后续过程，凝练出top_k个置信度很高的box, 计算recall, 得出结论。</p></li>
</ol>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/GhkiBfjI83.png?imageslim" alt="mark" /></p>

<h2 id="rpn实验结果">RPN实验结果</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/E157Ch0iFJ.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/dmHJF8774D.png?imageslim" alt="mark" /></p>

<h1 id="最新方法概述">最新方法概述：</h1>

<ul>
<li>Fast-RCNN

<ul>
<li>Counterparts (Grid-based CNN,RCNN minus R,etc.)</li>
</ul></li>
<li>YOLO: Unified real-time object detection (cvpr&rsquo;16)</li>
<li>SSD:Single-shot multi-box detector(eccv&rsquo;16)</li>
<li>Inside-Outside Net (cvpr&rsquo;16)</li>
<li>Adaptive Object Detection using Adjacency-Zoom Prediction</li>
<li>Region-based FCN (NIPS&rsquo;16)</li>
</ul>

<h2 id="最新方法-rfcn">最新方法：RFCN：</h2>

<p>Region-based Fully Convolutional Networks</p>

<ul>
<li><p>Motivation:previous methods require a costly pre-region subnet to compute the losses/class score.</p></li>

<li><p>Now:a position-sensitive score map mechanism</p>

<ul>
<li><p>Fully convolutional with all computations shared on the entire image.</p></li>

<li><p>Solves the dilemma that detection is transition variance while classification is not .</p></li>
</ul></li>
</ul>

<p>Code available: <a href="https://github.com/daijifeng001/r-fcn">https://github.com/daijifengoo1/r-fcn</a></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/15Fc5ei52b.png?imageslim" alt="mark" /></p>

<p>每个颜色代表不同的位置选择区域。
The bank of kxk score maps correspond to a kxk spatial grid describing relative positions.</p>

<p>The loss of training:</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/I1hmGAhG26.png?imageslim" alt="mark" /></p>

<p>In particular, the classification loss:</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/mEg1aFJE5F.png?imageslim" alt="mark" /></p>

<p>问题1: RFCN跟DPM的关系，如何理解？</p>

<p>Position-sensitive score maps and ROI pooling</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/CHFKJK34eL.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/EE3kJaHI4I.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/HFkEK7JE1F.png?imageslim" alt="mark" /></p>

<p>Figure 1: a positive box of person class</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/KGmK5FllH5.png?imageslim" alt="mark" /></p>

<p>Figure 2: a negative box of person class</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/AiJB7aL5eB.png?imageslim" alt="mark" /></p>

<p>问题2: RCNN, Fast-RCNN, RFCN 在feature-map 层面是如何联系起来的？</p>

<ul>
<li>RCNN是从一开始就把2000个框全部分开了，就是2000个框都是不同的feature。</li>
<li>Fast-RCNN 包括 Faster-RCNN 是拦腰截断，从image的输入到中间这块，只有一张图，forward一次，然后通过ROI把2000个框分开了。</li>
<li>RFCN 它在最后一层分开。在FastScale上效果还比较好，但是在ImageNet上效果比Fast-RCNN还差一些。</li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/HGie3a24kg.png?imageslim" alt="mark" /></p>

<h1 id="值得思考的问题">值得思考的问题：</h1>

<h3 id="joint-training">Joint Training</h3>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/e5E6DDjH3E.png?imageslim" alt="mark" /></p>

<p>能不能一下把4个loss一起train？这样train好像效果不好。</p>

<h3 id="multi-depth-loss">Multi-depth Loss</h3>

<p>有么有不同深度的loss？</p>

<h3 id="stacked-hourglass-architecture">Stacked-Hourglass Architecture</h3>

<ul>
<li>使用模块进行网络设计</li>
<li>先降采样，再升采样的全卷积结构</li>
<li>跳级结构辅助升采样</li>
<li>中继监督训练</li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180727/6CFDEgfI8g.png?imageslim" alt="mark" /></p>

<p>能不能用高层信息来弥补低层的信息？先训练RCNN，后训练Fast-RCNN</p>

<p><a href="https://blog.csdn.net/shenxiaolu1984/article/details/51428392">https://blog.csdn.net/shenxiaolu1984/article/details/51428392</a></p>

<h1 id="comment">COMMENT：</h1>

<p>下载faster-rcnn 代码 (链接下页提供)，下载Pascal VOC数据库。</p>

<ul>
<li><p>跑通整个detection 的流程</p></li>

<li><p>熟悉每个步骤的代码，例如：</p>

<ul>
<li><p>function [input_blobs, random_scale_inds] = proposal_generate_minibatch(conf, image_roidb)</p></li>

<li><p>function [image_roidb, bbox_means, bbox_stds] = proposal_prepare_image_roidb(conf, imdbs,roidbs)</p></li>
</ul></li>
</ul>

<p>在此基础之上，将stage 2的multi-class detection, 改为fore/background分类问题，实现stage 1 RPN -&gt; stage 2 proposal refinement, aka, cascade.</p>

<p>参考解答：<a href="https://github.com/hli2020/faster_rcnn ">https://github.com/hli2020/faster_rcnn </a> 这个里面有一些注释。</p>

<p><strong>没有做，自己做过之后补充进来。</strong></p>

<p>#</p>

<p>一些参考资料</p>

<ol>
<li><p>Fast-RCNN</p>

<ul>
<li>(Caffe+ MATLAB): <a href="https://github.com/rbgirshick/fast-rcnn">https://github.com/rbgirshick/fast-rcnn</a></li>
</ul></li>

<li><p>Faster-RCNN</p>

<ul>
<li><p>(Caffe+ MATLAB): <a href="https://github.com/ShaoqingRen/faster_rcnn">https://github.com/ShaoqingRen/faster_rcnn</a></p></li>

<li><p>(Caffe+ Python): <a href="https://github.com/rbgirshick/py-faster-rcnn">https://github.com/rbgirshick/py-faster-rcnn</a></p></li>
</ul></li>

<li><p>YOLO   <a href="http://pjreddie.com/darknet/yolo/">http://pjreddie.com/darknet/yolo/</a></p></li>

<li><p>LocNet, AttractioNet (CVPR’16)   <a href="https://github.com/gidariss/LocNet">https://github.com/gidariss/LocNet</a></p></li>
</ol>

<p><strong>都要仔细看过然后补充进来。</strong></p>

<p><strong>这个Neural Style还是要自己试过之后，将代码和生成的图片都补充进来。</strong></p>

<p>#</p>

<p><strong>视频讲的很多地方没有很明白，再听一遍。</strong></p>

<p><strong>而且感觉有很多的东西可以学。</strong></p>

<p><strong>而且计算机视觉的课程也要总结下。</strong></p>

<p>RFCN能不能满足real-time的？现在最快的应该是1s处理4帧，比Faster要快，但是相比自动驾驶和无人机，还是不能够real-time的。</p>

<p>准确度要求的话：如果有准又快 RFCN就可以，再准一点点，就用Faster-RCNN 但是会慢一点点。</p>

<p>没有基础的就先看看Caffee的tutorial 里面有猫那个例子。</p>

<p>然后看CS231n 真门课就从0基础开始学的。而且讲的很好，有汉化版本吧。</p>

<p>目标物体放在非常复杂的背景下，怎么解决？用COCO train 不要用VOC train</p>

<h2 id="相关资料">相关资料：</h2>

<ol>
<li>七月在线 深度学习</li>
<li><a href="http://www.52ml.net/15680.html">DPM（Deformable Parts Model）</a> <strong>这个还没看</strong></li>
<li><a href="https://blog.csdn.net/szj_huhu/article/details/78157982">Selective Search</a>    <strong>还没看 Region Proposal 相关的</strong></li>
</ol>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/11-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-cv/01-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E7%A1%80/04-%E8%A7%86%E8%A7%89%E9%A1%B9%E7%9B%AE%E4%BC%98%E5%8C%96/01-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">数据增强</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/10-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/08-torch/01-%E4%BB%8B%E7%BB%8D/">
            <span class="next-text nav-default">01 介绍</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
