<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>01 概述 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="TODO 讲的还是很好的，很充分，很多以前不知道，不清楚的东西，比如到底要怎么使用 tf-idf 到底要怎么使用 word2vec 等都讲到了，很好。 DL 来做 NLP 的还没讲，老实说第六课" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/31-%E5%85%B6%E5%AE%83%E9%A1%B9%E7%9B%AE/%E4%B8%83%E6%9C%88kaggle/04-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B1%BB%E9%97%AE%E9%A2%98/01-%E6%A6%82%E8%BF%B0/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="01 概述" />
<meta property="og:description" content="TODO 讲的还是很好的，很充分，很多以前不知道，不清楚的东西，比如到底要怎么使用 tf-idf 到底要怎么使用 word2vec 等都讲到了，很好。 DL 来做 NLP 的还没讲，老实说第六课" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/31-%E5%85%B6%E5%AE%83%E9%A1%B9%E7%9B%AE/%E4%B8%83%E6%9C%88kaggle/04-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B1%BB%E9%97%AE%E9%A2%98/01-%E6%A6%82%E8%BF%B0/" /><meta property="article:published_time" content="2018-07-24T18:45:35&#43;00:00"/>
<meta property="article:modified_time" content="2018-07-24T18:45:35&#43;00:00"/>
<meta itemprop="name" content="01 概述">
<meta itemprop="description" content="TODO 讲的还是很好的，很充分，很多以前不知道，不清楚的东西，比如到底要怎么使用 tf-idf 到底要怎么使用 word2vec 等都讲到了，很好。 DL 来做 NLP 的还没讲，老实说第六课">


<meta itemprop="datePublished" content="2018-07-24T18:45:35&#43;00:00" />
<meta itemprop="dateModified" content="2018-07-24T18:45:35&#43;00:00" />
<meta itemprop="wordCount" content="5850">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="01 概述"/>
<meta name="twitter:description" content="TODO 讲的还是很好的，很充分，很多以前不知道，不清楚的东西，比如到底要怎么使用 tf-idf 到底要怎么使用 word2vec 等都讲到了，很好。 DL 来做 NLP 的还没讲，老实说第六课"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">iterate self</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">about</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">iterate self</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">about</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">01 概述</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-07-24 </span>
        
        <span class="more-meta"> 5850 words </span>
        <span class="more-meta"> 12 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#自然语言处理类问题">自然语言处理类问题</a>
<ul>
<li><a href="#主要内容">主要内容</a></li>
<li><a href="#nltk">NLTK</a></li>
</ul></li>
<li><a href="#完整的文本预处理过程">完整的文本预处理过程</a>
<ul>
<li><a href="#那么我们自然语言处理大概的流程是什么">那么我们自然语言处理大概的流程是什么？</a></li>
<li><a href="#tokenize-把长句-拆成有-意义-的-部件">Tokenize 把长句⼦拆成有“意义”的⼩部件</a></li>
<li><a href="#纷繁复杂的词形">纷繁复杂的词形</a></li>
<li><a href="#nltk-实现-stemming">NLTK 实现 Stemming</a></li>
<li><a href="#nltk实现lemma">NLTK实现Lemma</a></li>
<li><a href="#stopwords">Stopwords</a></li>
<li><a href="#总结一下">总结一下</a></li>
</ul></li>
<li><a href="#自然语言处理的几类问题的对应">自然语言处理的几类问题的对应</a>
<ul>
<li><a href="#那么什么是自然语言处理呢">那么什么是自然语言处理呢？</a></li>
<li><a href="#nltk在nlp上的经典应">NLTK在NLP上的经典应⽤</a>
<ul>
<li><a href="#情感分析">情感分析</a></li>
<li><a href="#应-本相似度">应⽤：⽂本相似度</a></li>
<li><a href="#文本分类">文本分类</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<p>TODO</p>

<ul>
<li>讲的还是很好的，很充分，很多以前不知道，不清楚的东西，比如到底要怎么使用 tf-idf 到底要怎么使用 word2vec 等都讲到了，很好。</li>
<li>DL 来做 NLP 的还没讲，老实说第六课会讲。</li>
<li>n-gram 没讲</li>
<li>KDD2013 比赛：判定文章作者 好像没讲</li>
<li>影评数据做情感分析 好像没讲</li>
</ul>

<h1 id="自然语言处理类问题">自然语言处理类问题</h1>

<h2 id="主要内容">主要内容</h2>

<ul>
<li>讲解NLP的基本思路与技法</li>
<li>Kaggle题⽬详解</li>
</ul>

<p>两个问题：一个是上一次的一个问题，</p>

<h2 id="nltk">NLTK</h2>

<p>今天课上的code会基于NLTK
做⼀点更详细的讲解
NLTK是Python上著名的⾃然语⾔处理库
⾃带语料库，词性分类库
⾃带分类，分词，等等功能
强⼤的社区⽀持
还有N多的简单版wrapper</p>

<p>唯一的缺憾是并不原生支持中文，但是中文与英文在分完词之后是相同的意思。</p>

<p>NLTK 至少在国外是必会的第三方库，在国内回它也是应该的事情，还要回一些在对应处理中文分词问题的一些库</p>

<p>安装 NLTK 的时候，遇到的一些bug 和error 很大部分是因为没有下载语料库。
在你安装好 NLTK 之后，进入自己的python console 然后输入：</p>

<pre><code class="language-python">import nltk
nltk.download()
</code></pre>

<p>然后运行，他就会显示这个界面：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/AFj8j7Jg74.png?imageslim" alt="mark" /></p>

<p>NLTK 功能一览：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/Hgg3K7m6aB.png?imageslim" alt="mark" /></p>

<p>NLTK 自带的语料库是什么？</p>

<p>有很多的预料信息，算法</p>

<pre><code class="language-python">&gt;&gt;&gt; from nltk.corpus import brown
&gt;&gt;&gt; brown.categories()
['adventure', 'belles_lettres', 'editorial',
'fiction', 'government', 'hobbies', 'humor',
'learned', 'lore', 'mystery', 'news', 'religion',
'reviews', 'romance', 'science_fiction']
&gt;&gt;&gt; len(brown.sents())
57340
&gt;&gt;&gt; len(brown.words())
1161192
</code></pre>

<p>语料在自然语言中还是比较重要的。<span style="color:red;">不知道这样的语料库与数据集有什么区别吗？</span></p>

<p>brown 是布朗大学的语料库，是一个分类语料库，他分成了冒险类、兴趣类等等。
也可以看到这个语料库里面有多少个句子，有多少个单词。</p>

<h1 id="完整的文本预处理过程">完整的文本预处理过程</h1>

<h2 id="那么我们自然语言处理大概的流程是什么">那么我们自然语言处理大概的流程是什么？</h2>

<p>比如说，我们拿到一篇文章，我们应该怎么处理它？</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/45536kf1Am.png?imageslim" alt="mark" /></p>

<p>比如上面这句歌词，Hello from the other side 这个是阿黛尔的一句歌。</p>

<p>我们来判断这句话是不是来自阿黛尔的歌：</p>

<p>那，我们的输入就是这个字符串，输出就是 0 或 1。</p>

<ul>
<li>我们用 Tokenize 这个方法将这个字符串分成了5个占有不同位置的小字符串。</li>
<li>然后通过一些预处理的方法 Preprocess 把不太需要的一些东西处理掉。比如 a girl 中的 a ，上句中的 the。</li>
<li>之后，我们把上面的机器位置变成数字化的表达形式，<span style="color:red;">什么是机器位置？怎么变成数字化的表达的？</span></li>
<li>我们得到这个数字化的向量之后就可以使用各种模型来进行label的判定。</li>
</ul>

<p>OK，介绍一下上面的几个步骤：</p>

<h2 id="tokenize-把长句-拆成有-意义-的-部件">Tokenize 把长句⼦拆成有“意义”的⼩部件</h2>

<p>这里中英文是不同的。</p>

<p>对于英文来说：可以直接用 work_tokenize 把单词分割开来。</p>

<pre><code class="language-python">&gt;&gt;&gt; import nltk
&gt;&gt;&gt; sentence = “hello, world&quot;
&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)
&gt;&gt;&gt; tokens
['hello', ',', 'world']
</code></pre>

<p>但是对于中文就比较麻烦了：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/Jf4DIJ0B8m.png?imageslim" alt="mark" /></p>

<p>因为中文的词中间没有空格，每个中文字相当于英文中的一个字母。
那么中文的分词怎么做呢？有两种方法：</p>

<ul>
<li>启发式 Heuristic 有一个字典，字典里面包含了不同的词汇，直接以最长 match 扫过去。就进行了分割</li>
<li>机器学习/统计方法：HMM、CRF</li>
</ul>

<p>在中文分词界比较简单好用的是：</p>

<ul>
<li>jieba 他速度比较快，而且没有那么复杂。</li>
<li>哈工大的NLP ，讯飞的NLP</li>
<li>在国际界的中文分词比较好的是 ：斯坦福的 core NLP，它是基于 CRF 的一种方式</li>
</ul>

<pre><code class="language-python">import jieba
seg_list = jieba.cut(&quot;我来到北京清华⼤学&quot;, cut_all=True)
print &quot;Full Mode:&quot;, &quot;/ &quot;.join(seg_list) # 全模式
seg_list = jieba.cut(&quot;我来到北京清华⼤学&quot;, cut_all=False)
print &quot;Default Mode:&quot;, &quot;/ &quot;.join(seg_list) # 精确模式
seg_list = jieba.cut(&quot;他来到了⽹易杭研⼤厦&quot;) # 默认是精确模式
print &quot;, &quot;.join(seg_list)
seg_list = jieba.cut_for_search(&quot;⼩明硕⼠毕业于中国科学院计算所，后在⽇本京都⼤学深造&quot;)# 搜索引擎模式
print &quot;, &quot;.join(seg_list)

【全模式】 : 我/ 来到/ 北京/ 清华/ 清华⼤学/ 华⼤/ ⼤学
【精确模式】 : 我/ 来到/ 北京/ 清华⼤学
【新词识别】：他, 来到, 了, ⽹易, 杭研, ⼤厦
(此处， “杭研”并没有在词典中，但是也被 Viterbi 算法识别出来了)
【搜索引擎模式】： ⼩明, 硕⼠, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算,
计算所, 后, 在, ⽇本, 京都, ⼤学, ⽇本京都⼤学, 深造
</code></pre>

<p>全模式的好处：用在搜索引擎模式的时候比较好，这样就不会漏掉一些可能被用户需要的信息
精确模式：只会把它包含的分割开来
新词识别：新词会被 Viterbi 算法识别出来
搜索引擎模式：比全模式提供更多信息，分出更多的分词。</p>

<p>一般我们使用的是精确模式。</p>

<p>分词后的效果：</p>

<p>[&lsquo;what&rsquo;, &lsquo;a&rsquo;, &lsquo;nice&rsquo;, &lsquo;weather&rsquo;, &lsquo;today&rsquo;]
[&lsquo;今天&rsquo;, &lsquo;天⽓&rsquo;, &lsquo;真&rsquo;, &lsquo;不错&rsquo;]</p>

<p>经过分词之后，每个输入的句子都变成了一个数组，数组</p>

<p>有时候tokenize没那么简单</p>

<p>⽐如社交⽹络上，这些乱七⼋糟的不合语法不合正常逻辑的语⾔很多：
拯救 @某⼈, 表情符号, URL, #话题符号</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180723/ADdcHl6mmK.png?imageslim" alt="mark" /></p>

<p><span style="color:red;">这种没有怎么细讲</span></p>

<p>要分割这些就要用到一些特殊的技艺，比如正则表达式。</p>

<pre><code class="language-python">from nltk.tokenize import word_tokenize
tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'
print(word_tokenize(tweet))
# ['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':',
# ’D', 'http', ':', '//ah.love', '#', '168cm']
</code></pre>

<p>上面这句在正常的语料库中都不会有
这种情况下使用正则表达式：</p>

<pre><code class="language-python">import re
emoticons_str = r&quot;&quot;&quot;
    (?:
        [:=;] # 眼睛
        [oO\-]? # ⿐⼦
        [D\)\]\(\]/\\OpP] # 嘴
    )&quot;&quot;&quot;
regex_str = [
    emoticons_str,
    r'&lt;[^&gt;]+&gt;', # HTML tags
    r'(?:@[\w_]+)', # @某⼈
    r&quot;(?:\#+[\w_]+[\w\'_\-]*[\w_]+)&quot;, # 话题标签
    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;amp;+]|[!*\(\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs
    r'(?:(?:\d+,?)+(?:\.?\d+)?)', # 数字
    r&quot;(?:[a-z][a-z'\-_]+[a-z])&quot;, # 含有 - 和 ‘ 的单词
    r'(?:[\w_]+)', # 其他
    r'(?:\S)' # 其他
]
</code></pre>

<p>使用这种正则表达式把预料中出现的一些神奇的字符都归纳出来。</p>

<p>这个是正则表达式的使用方法：对照表 <a href="http://www.regexlab.com/zh/regref.htm">http://www.regexlab.com/zh/regref.htm</a></p>

<pre><code class="language-python">tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)
emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)
def tokenize(s):
    return tokens_re.findall(s)

def preprocess(s, lowercase=False):
    tokens = tokenize(s)
    if lowercase:
        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]
return tokens

tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'
print(preprocess(tweet))
# ['RT', '@angelababy', ':', 'love', 'you', 'baby',
# ’!', ':D', 'http://ah.love', '#168cm']
</code></pre>

<p>上面这个就讲了怎么把 reg 结合到 tokenize 方法中。然后直接 preprocess 这个 tweet 就行。可见，想要的分词结果已经得到了。</p>

<h2 id="纷繁复杂的词形">纷繁复杂的词形</h2>

<p>在英文中，这个是比较复杂的：</p>

<p>Inflection 变化: walk =&gt; walking =&gt; walked 不影响词性</p>

<p>derivation 引申: nation (noun) =&gt; national (adjective) =&gt; nationalize (verb) 影响词性</p>

<p>在中文中也有：</p>

<p>比如茴香豆的茴字，有三种的写法，实际上表达的是相同的东西，那么这些对于我的计算的就是增加了负担。</p>

<p>因此，遇到的时候，想统一成一种茴香豆的茴字。这样只在算法中作为一个特征出现。</p>

<p>对于英文中的复杂词形的处理方法：</p>

<ul>
<li><p>Stemming 词⼲提取：⼀般来说，就是把不影响词性的 inflection 的⼩尾巴砍掉</p>

<ul>
<li>walking 砍ing = walk</li>
<li>walked 砍ed = walk</li>
</ul></li>

<li><p>Lemmatization 词形归⼀：把各种类型的词的变形，都归为⼀个形式</p>

<ul>
<li>went 归⼀ = go</li>
<li>are 归⼀ = be</li>
</ul></li>
</ul>

<p>词干提取就是直接砍掉，只提取公有的东西。
词形归一：这就是类似于对照字典的形式，他知道 went ，就归一成 go 的形式。</p>

<p>这两种都不过。</p>

<h2 id="nltk-实现-stemming">NLTK 实现 Stemming</h2>

<p>nltk 里面的 stemmer 就是词干提取器。</p>

<pre><code class="language-python">&gt;&gt;&gt; from nltk.stem.porter import PorterStemmer
&gt;&gt;&gt; porter_stemmer = PorterStemmer()
&gt;&gt;&gt; porter_stemmer.stem(‘maximum’)
u’maximum’
&gt;&gt;&gt; porter_stemmer.stem(‘presumably’)
u’presum’
&gt;&gt;&gt; porter_stemmer.stem(‘multiply’)
u’multipli’
&gt;&gt;&gt; porter_stemmer.stem(‘provision’)
u’provis’
</code></pre>

<pre><code class="language-python">&gt;&gt;&gt; from nltk.stem import SnowballStemmer
&gt;&gt;&gt; snowball_stemmer = SnowballStemmer(“english”)
&gt;&gt;&gt; snowball_stemmer.stem(‘maximum’)
u’maximum’
&gt;&gt;&gt; snowball_stemmer.stem(‘presumably’)
u’presum’
</code></pre>

<pre><code class="language-python">&gt;&gt;&gt; from nltk.stem.lancaster import LancasterStemmer
&gt;&gt;&gt; lancaster_stemmer = LancasterStemmer()
&gt;&gt;&gt; lancaster_stemmer.stem(‘maximum’)
‘maxim’
&gt;&gt;&gt; lancaster_stemmer.stem(‘presumably’)
‘presum’
&gt;&gt;&gt; lancaster_stemmer.stem(‘presumably’)
‘presum’
</code></pre>

<pre><code class="language-python">&gt;&gt;&gt; from nltk.stem.porter import PorterStemmer
&gt;&gt;&gt; p = PorterStemmer()
&gt;&gt;&gt; p.stem('went')
'went'
&gt;&gt;&gt; p.stem('wenting')
'went'
</code></pre>

<p>可见，在不同的 stemmer 中，认为的词根也不相同。各有各自的认为的处理方式。没有绝对的好坏之分</p>

<h2 id="nltk实现lemma">NLTK实现Lemma</h2>

<pre><code class="language-python">&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; wordnet_lemmatizer = WordNetLemmatizer()
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘dogs’)
u’dog’
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘churches’)
u’church’
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘aardwolves’)
u’aardwolf’
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘abaci’)
u’abacus’
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘hardrock’)
‘hardrock’
</code></pre>

<p>WordNet 是语言学家整理出来的全世界英文的一个前后对照关系表</p>

<p>Lemma的⼩问题</p>

<p>Went
v. go的过去式
n. 英⽂名：温特</p>

<p>可见，不能每次遇到 Went 就归一成 go，因为 温特与 go 还是差别很大的。
那么这种情况怎么办呢？为了更好的使用 Lemma，可以给这个词标注词性，这样归一化的结果就会跟着词性变化。</p>

<pre><code class="language-python"># ⽊有POS Tag，默认是NN 名词
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘are’)
‘are’
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘is’)
‘is’
# 加上POS Tag
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘is’, pos=’v’)
u’be’
&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(‘are’, pos=’v’)
u’be’
</code></pre>

<p>pos 就是 part of speech 简单的讲就是词性：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/IbEkdBbm3i.png?imageslim" alt="mark" /></p>

<p>那么怎么知道句子中的一个词是什么词性呢？NLTK 中有自耦定标注的方法：</p>

<p>NLTK标注POS Tag</p>

<pre><code class="language-python">&gt;&gt;&gt; import nltk
&gt;&gt;&gt; text = nltk.word_tokenize('what does the fox say')
&gt;&gt;&gt; text
['what', 'does', 'the', 'fox', 'say']
&gt;&gt;&gt; nltk.pos_tag(text)
[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]
</code></pre>

<p>奇怪了，这个pos_tag 难道不是使用的原始的字符串吗？使用数组的话会标注错吧？还是说其实是不考虑语境的？</p>

<h2 id="stopwords">Stopwords</h2>

<p>之前的 went 和 go 实际上指的都是 go ，但是这个 he 却可能指的不同的人。</p>

<p>这类词成为停止词。</p>

<p>如果我们的问题场景是基于意思这个场景，那么我们统一删除掉这些停止词，不让他出现在我们的场景中。如果是处理别的任务，比如文法判断，判断这个人写的论文语法用的好不好，与另一篇论文的相似度高不高，这里面就可能并不需要减去这个停止词。因为停止词的使用可能也包含了一些意义在里面。注重的是文本被创造的过程。</p>

<p>⼀千个 HE 有⼀千种指代
⼀千个 THE 有⼀千种指事</p>

<p>对于注重理解⽂本『意思』的应⽤场景来说
歧义太多
全体stopwords列表 <a href="http://www.ranks.nl/stopwords">http://www.ranks.nl/stopwords</a> 英文停止词</p>

<p>NLTK去除stopwords</p>

<p>注意，在使用之前要在 console ⾥⾯下载⼀下词库
或者 nltk.download(‘stopwords’)</p>

<pre><code class="language-python">from nltk.corpus import stopwords
# 先token⼀把，得到⼀个word_list
# ...
# 然后filter⼀把
filtered_words =
[word for word in word_list if word not in stopwords.words('english')]
</code></pre>

<p>上面就把停止词从 word_list 里面忽略掉了。</p>

<h2 id="总结一下">总结一下</h2>

<p>⼀条典型的⽂本预处理流⽔线</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/2f55E550lE.png?imageslim" alt="mark" /></p>

<p>Pos Tag 不一定用。标注之后更好。最后得到我们的 word_list</p>

<p>这就是一整套完整的文本预处理流水线。</p>

<h1 id="自然语言处理的几类问题的对应">自然语言处理的几类问题的对应</h1>

<p><span style="color:red;">这三类问题没有仔细讲，可以看看之前的七月的一些课程视频。</span></p>

<h2 id="那么什么是自然语言处理呢">那么什么是自然语言处理呢？</h2>

<p>⾃然语⾔ -&gt; 计算机数据</p>

<p>⽂本预处理让我们得到了什么？</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/hKil2k41Fm.png?imageslim" alt="mark" /></p>

<p>可见，在上面的文本预处理的过程之后就是 Feature 化。</p>

<h2 id="nltk在nlp上的经典应">NLTK在NLP上的经典应⽤</h2>

<ul>
<li>情感分析</li>
<li>⽂本相似度</li>
<li>⽂本分类</li>
</ul>

<p>这三类问题，课程都有对应的初级的代码应用。这里就暂时不讲了。到时候自己对应看下：</p>

<h3 id="情感分析">情感分析</h3>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/1dJBCFeIdm.png?imageslim" alt="mark" /></p>

<p>哪些是在夸？哪些是在黑？</p>

<p>最简单的 sentiment dictionary
like 1
good 2
bad -2
terrible -3
类似于关键词打分机制
⽐如： AFINN-111
<a href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010">http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010</a></p>

<p>NLTK 完成简单的情感分析</p>

<pre><code class="language-python">sentiment_dictionary = {}
for line in open('data/AFINN-111.txt')
    word, score = line.split('\t')
    sentiment_dictionary[word] = int(score)
# 把这个打分表记录在⼀个Dict上以后
# 跑⼀遍整个句⼦，把对应的值相加
total_score = sum(sentiment_dictionary.get(word, 0) for word in words)
# 有值就是Dict中的值，没有就是0

# 于是你就得到了⼀个 sentiment score
</code></pre>

<p>显然这个⽅法太Naive
新词怎么办？
特殊词汇怎么办？
更深层次的玩意⼉怎么办？</p>

<p>配上ML的情感分析</p>

<pre><code class="language-python">from nltk.classify import NaiveBayesClassifier
# 随⼿造点训练集
s1 = 'this is a good book'
s2 = 'this is a awesome book'
s3 = 'this is a bad book'
s4 = 'this is a terrible book'
def preprocess(s):
    # Func: 句⼦处理
    # 这⾥简单的⽤了split(), 把句⼦中每个单词分开
    # 显然 还有更多的processing method可以⽤
    return {word: True for word in s.lower().split()}
    # return⻓这样:
    # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}
    # 其中, 前⼀个叫fname, 对应每个出现的⽂本单词;
    # 后⼀个叫fval, 指的是每个⽂本单词对应的值。
    # 这⾥我们⽤最简单的True,来表示,这个词『出现在当前的句⼦中』的意义。
    # 当然啦, 我们以后可以升级这个⽅程, 让它带有更加⽜逼的fval, ⽐如 word2vec

# 把训练集给做成标准形式
training_data = [[preprocess(s1), 'pos'],
                  [preprocess(s2), 'pos'],
                  [preprocess(s3), 'neg'],
                  [preprocess(s4), 'neg']]

# 喂给model吃
model = NaiveBayesClassifier.train(training_data)
# 打出结果
print(model.classify(preprocess('this is a good book')))

</code></pre>

<h3 id="应-本相似度">应⽤：⽂本相似度</h3>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/Gj3HBek527.png?imageslim" alt="mark" /></p>

<p>⽤元素频率表⽰⽂本特征</p>

<table>
<thead>
<tr>
<th>we</th>
<th>you</th>
<th>he</th>
<th>work</th>
<th>happy</th>
<th>are</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>

<tr>
<td>1</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>

<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>

<p>余弦定理</p>

<p>$$similarity=cos(\theta)=\frac{A\cdot B}{||A||||B||}$$</p>

<p>Frequency 频率统计</p>

<pre><code class="language-python">import nltk
from nltk import FreqDist
# 做个词库先
corpus = 'this is my sentence ' \
          'this is my life ' \
          'this is the day'
# 随便tokenize⼀下
# 显然, 正如上⽂提到,
# 这⾥可以根据需要做任何的preprocessing:
# stopwords, lemma, stemming, etc.
tokens = nltk.word_tokenize(corpus)
print(tokens)
# 得到token好的word list
# ['this', 'is', 'my', 'sentence',
# 'this', 'is', 'my', 'life', 'this',
# 'is', 'the', 'day']

# 借⽤NLTK的FreqDist统计⼀下⽂字出现的频率
fdist = FreqDist(tokens)

# 它就类似于⼀个Dict
# 带上某个单词, 可以看到它在整个⽂章中出现的次数
print(fdist['is'])
# 3


# 好, 此刻, 我们可以把最常⽤的50个单词拿出来
standard_freq_vector = fdist.most_common(50)
size = len(standard_freq_vector)
print(standard_freq_vector)
# [('is', 3), ('this', 3), ('my', 2),
# ('the', 1), ('day', 1), ('sentence', 1),
# ('life', 1)


# Func: 按照出现频率⼤⼩, 记录下每⼀个单词的位置
def position_lookup(v):
    res = {}
    counter = 0
    for word in v:
        res[word[0]] = counter
        counter += 1
    return res
# 把标准的单词位置记录下来
standard_position_dict = position_lookup(standard_freq_vector)
print(standard_position_dict)
# 得到⼀个位置对照表
# {'is': 0, 'the': 3, 'day': 4, 'this': 1,
# 'sentence': 5, 'my': 2, 'life': 6}




# 这时, 如果我们有个新句⼦:
sentence = 'this is cool'
# 先新建⼀个跟我们的标准vector同样⼤⼩的向量
freq_vector = [0] * size
# 简单的Preprocessing
tokens = nltk.word_tokenize(sentence)
# 对于这个新句⼦⾥的每⼀个单词
for word in tokens:
    try:
        # 如果在我们的词库⾥出现过
        # 那么就在&quot;标准位置&quot;上+1
        freq_vector[standard_position_dict[word]] += 1
    except KeyError:
        # 如果是个新词
        # 就pass掉
        continue

print(freq_vector)
# [1, 1, 0, 0, 0, 0, 0]
# 第⼀个位置代表 is, 出现了⼀次
# 第⼆个位置代表 this, 出现了⼀次
# 后⾯都⽊有

</code></pre>

<h3 id="文本分类">文本分类</h3>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180724/HEJdA8jKDL.png?imageslim" alt="mark" /></p>

<p>TF-IDF</p>

<ul>
<li>TF: Term Frequency, 衡量⼀个term在⽂档中出现得有多频繁。
TF(t) = (t出现在⽂档中的次数) / (⽂档中的term总数).</li>
<li>IDF: Inverse Document Frequency, 衡量⼀个term有多重要。
有些词出现的很多，但是明显不是很有卵⽤。⽐如’is&rsquo;， ’the‘， ’and‘之类
的。
为了平衡，我们把罕见的词的重要性（weight）搞⾼，
把常见词的重要性搞低。
IDF(t) = log_e(⽂档总数 / 含有t的⽂档总数).</li>
<li>TF-IDF = TF * IDF</li>
</ul>

<p>举个栗⼦:
⼀个⽂档有100个单词，其中单词baby出现了3次。
那么， TF(baby) = (<sup>3</sup>&frasl;<sub>100</sub>) = 0.03.
好，现在我们如果有10M的⽂档， baby出现在其中的1000个⽂档中。
那么， IDF(baby) = log(10,000,000 / 1,000) = 4
所以， TF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 = 0.12</p>

<p>NLTK实现TF-IDF</p>

<pre><code class="language-python">from nltk.text import TextCollection
# ⾸先, 把所有的⽂档放到TextCollection类中。
# 这个类会⾃动帮你断句, 做统计, 做计算
corpus = TextCollection(['this is sentence one',
                          'this is sentence two',
                          'this is sentence three'])
# 直接就能算出tfidf
# (term: ⼀句话中的某个term, text: 这句话)
print(corpus.tf_idf('this', 'this is sentence four'))
# 0.444342

# 同理, 怎么得到⼀个标准⼤⼩的vector来表示所有的句⼦?

# 对于每个新句⼦
new_sentence = 'this is sentence five'
# 遍历⼀遍所有的vocabulary中的词:
for word in standard_vocab:
    print(corpus.tf_idf(word, new_sentence))
    # 我们会得到⼀个巨⻓(=所有vocab⻓度)的向量
</code></pre>

<p>接下来就是 ML 的过程：</p>

<p>可能的ML模型:
- SVM
- LR
- RF
- MLP
- LSTM
- RNN
- …</p>

<p>OK，我们看看Kaggle 上有哪些问题，可以怎么处理。</p>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/31-%E5%85%B6%E5%AE%83%E9%A1%B9%E7%9B%AE/%E4%B8%83%E6%9C%88kaggle/05-%E8%83%BD%E6%BA%90%E9%A2%84%E6%B5%8B%E4%B8%8E%E5%88%86%E9%85%8D%E7%B1%BB%E9%97%AE%E9%A2%98%E6%A1%88%E4%BE%8B/%E6%A1%88%E4%BE%8B1%E8%83%BD%E6%BA%90%E9%A2%84%E6%B5%8B/02_collect_weather_data/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">02_collect_weather_data</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/31-%E5%85%B6%E5%AE%83%E9%A1%B9%E7%9B%AE/%E4%B8%83%E6%9C%88kaggle/04-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%B1%BB%E9%97%AE%E9%A2%98/05-%E6%A1%88%E4%BE%8B2%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6-advance/">
            <span class="next-text nav-default">05 案例2：文本相似度 advance</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
