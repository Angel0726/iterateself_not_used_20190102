<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>采样 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="相关资料 七月学院 机器学习 机器学习中的采样(sampling)方法是要解决什么类型的问题？ 贝叶斯推理-采样与变分简介 LDA变分法和采样法 需要补" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/05-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/%E9%87%87%E6%A0%B7/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="采样" />
<meta property="og:description" content="相关资料 七月学院 机器学习 机器学习中的采样(sampling)方法是要解决什么类型的问题？ 贝叶斯推理-采样与变分简介 LDA变分法和采样法 需要补" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/05-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/%E9%87%87%E6%A0%B7/" /><meta property="article:published_time" content="2018-07-28T23:02:37&#43;00:00"/>
<meta property="article:modified_time" content="2018-07-28T23:02:37&#43;00:00"/>
<meta itemprop="name" content="采样">
<meta itemprop="description" content="相关资料 七月学院 机器学习 机器学习中的采样(sampling)方法是要解决什么类型的问题？ 贝叶斯推理-采样与变分简介 LDA变分法和采样法 需要补">


<meta itemprop="datePublished" content="2018-07-28T23:02:37&#43;00:00" />
<meta itemprop="dateModified" content="2018-07-28T23:02:37&#43;00:00" />
<meta itemprop="wordCount" content="3188">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="采样"/>
<meta name="twitter:description" content="相关资料 七月学院 机器学习 机器学习中的采样(sampling)方法是要解决什么类型的问题？ 贝叶斯推理-采样与变分简介 LDA变分法和采样法 需要补"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/recent/">
        <li class="mobile-menu-item">最新</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/recent/">最新</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">采样</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-07-28 </span>
        
        <span class="more-meta"> 3188 words </span>
        <span class="more-meta"> 7 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#相关资料">相关资料</a></li>
<li><a href="#需要补充的">需要补充的</a></li>
</ul></li>
<li><a href="#motive">MOTIVE</a></li>
<li><a href="#怎么在圆内随机取点">怎么在圆内随机取点？</a>
<ul>
<li><a href="#一个简单的采样问题">一个简单的采样问题</a></li>
<li><a href="#那么圆内均匀取点可以怎么做">那么圆内均匀取点可以怎么做？</a></li>
<li><a href="#还有什么办法呢">还有什么办法呢？</a></li>
<li><a href="#附-产生圆内随机数的其他方法">附：产生圆内随机数的其他方法</a></li>
</ul></li>
<li><a href="#带拒绝的采样分析">带拒绝的采样分析</a>
<ul>
<li><a href="#什么是拒绝采样">什么是拒绝采样？</a></li>
<li><a href="#进一步思考拒绝采样">进一步思考拒绝采样</a></li>
<li><a href="#对某概率分布函数进行采样的意义">对某概率分布函数进行采样的意义</a></li>
<li><a href="#应用bernoulli版本的大数定理">应用Bernoulli版本的大数定理</a></li>
<li><a href="#用采样改造em算法本身">用采样改造EM算法本身</a></li>
<li><a href="#重述采样">重述采样</a></li>
<li><a href="#举例">举例</a></li>
<li><a href="#马尔科夫随机过程的平稳分布">马尔科夫随机过程的平稳分布</a></li>
<li><a href="#马尔科夫随机过程与采样">马尔科夫随机过程与采样</a></li>
<li><a href="#细致平稳条件">细致平稳条件</a></li>
<li><a href="#二维gibbs采样算法">二维Gibbs采样算法</a></li>
</ul></li>
<li><a href="#comment">COMMENT：</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h2 id="相关资料">相关资料</h2>

<ol>
<li>七月学院 机器学习</li>
<li><a href="https://www.zhihu.com/question/40943513">机器学习中的采样(sampling)方法是要解决什么类型的问题？</a></li>
<li><a href="https://wenku.baidu.com/view/41f1b295ac51f01dc281e53a580216fc700a53b2.html?re=view">贝叶斯推理-采样与变分简介</a></li>
<li><a href="https://blog.csdn.net/deltaququ/article/details/45892063">LDA变分法和采样法</a></li>
</ol>

<h2 id="需要补充的">需要补充的</h2>

<ul>
<li><p><strong>采样方法，比如MCMC采样，Gibbs采样，拒绝采样等，到底是为了解决什么类型的问题？ </strong></p></li>

<li><p><strong>这一章与LDA有关系吗？是Gibbs采样有关系吗？</strong></p></li>

<li><p><strong>为什么MCMC采样没有放在这一章里？</strong></p></li>
</ul>

<h1 id="motive">MOTIVE</h1>

<p>对采样进行总结。</p>

<p>上次讲的LDA为什么可以通过采样的方式来进行收敛来进行交代。</p>

<hr />

<p>OK 复习一下上次的：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1gGBLbbcmb.png?imageslim" alt="mark" /></p>

<p>Beta分布是二项分布的共轭先验分布，它的概率密度的定义是：<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/m6gmEam49m.png?imageslim" alt="mark" />这个是归一化因子</p>

<p>而这个B可以表示称一个Gamma函数的形式：<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/KiAhEiB5b4.png?imageslim" alt="mark" /></p>

<p>厉害了，没想到Beta分布还与Gamma函数有关。</p>

<p>如果给定\alpha 和\beta 那么，这个x的期望是多少？ xf(x)</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Ic4KjkbaL1.png?imageslim" alt="mark" /></p>

<p>厉害了，没想到这个Beta分布的期望可以写成这样。</p>

<p>上面的Beta分布拿到主题模型那一张去</p>

<h1 id="怎么在圆内随机取点">怎么在圆内随机取点？</h1>

<h2 id="一个简单的采样问题">一个简单的采样问题</h2>

<p>给定区间([a_x,b_x]×[a_y,b_y])，使得二维随机点(x,y)落在等概率落在区间的某个点上。</p>

<p>分析：因为两个维度是独立的，分别生成两个随机数即可。</p>

<p>产生二维随机数代码与效果</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/B8hEDmBJK0.png?imageslim" alt="mark" /></p>

<h2 id="那么圆内均匀取点可以怎么做">那么圆内均匀取点可以怎么做？</h2>

<p>给定定点(O(x_0 ,y_0))和半径r，使得二维随机点(x,y)等概率落在圆内。</p>

<p>分析：直接使用 $x=x<em>\theta +r*cos\theta$，$y=y</em>\theta +r*sin\theta$ 是否可以呢？具体试验一下。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/iJGADLLbe8.png?imageslim" alt="mark" /></p>

<h2 id="还有什么办法呢">还有什么办法呢？</h2>

<p>显然上述做法是不对的。但可以使用二维随机点的做法，若落在圆外，则重新生成点。即有选择的取点，结果如下：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/0fC5Hj6507.png?imageslim" alt="mark" /></p>

<p>这个就是一种带拒绝的采样分析。</p>

<p><strong>嗯不错的方法。</strong></p>

<h2 id="附-产生圆内随机数的其他方法">附：产生圆内随机数的其他方法</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/eLkmj3gBa3.png?imageslim" alt="mark" /></p>

<p>关键是这个： sqrt(rand2500())</p>

<h1 id="带拒绝的采样分析">带拒绝的采样分析</h1>

<h2 id="什么是拒绝采样">什么是拒绝采样？</h2>

<p>Rejection sampling</p>

<p>在对某区域f(x,y)≤0抽样的过程中，若该区域f(x,y)≤0不容易直接求解，则寻找某容易采样的区域g(x,y)≤0 ， G为F的上界。当采样(x 0 ,y 0 )∈G且落在F内部时，接收该采样；否则拒绝之。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1ICA2I764L.png?imageslim" alt="mark" /></p>

<p>该例中， f(x,y)≤0是圆， g(x,y)≤0是该圆的外包围正方形。</p>

<p>注：区域f(x,y)≤0的可行解集合记做F；区域g(x,y)≤0的可行解集合记做G；显然F⊆G</p>

<p>MCMC是沿着这个思路走的。</p>

<h2 id="进一步思考拒绝采样">进一步思考拒绝采样</h2>

<p>上述方法能够一定程度的估算圆周率，老师试过，大概是千万次才能得到3.14。虽然精度很差，但是思路还是很重要的。</p>

<p>OK，现在我们想一下：上述抽样问题能否用来解决一般概率分布函数的抽样问题？如：根据均匀分布函数得到正态分布的抽样？</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/dGBh0iAJ1G.png?imageslim" alt="mark" /></p>

<p>比如说我们知道q(z)这种分布，然后乘以一个k之后kq(z)就比我的(\widetilde{p}(z)) 要大，那么我随机取一个点，在红色的时候就要，在黑色的时候就不要，这样我就可以把任何一种概率分布都去做采样了。</p>

<p>那么怎么选q(z)呢？最简单的一种做法就是找到(\widetilde{p}(z)) 的极大值然后画一条横线，然后说我选的q(z)是一个均匀分布。所以这个总是存在的。</p>

<p>但是这个就太粗糙了，因为拒绝率太高，所以可以根据实际情况。选外包的分布。</p>

<h2 id="对某概率分布函数进行采样的意义">对某概率分布函数进行采样的意义</h2>

<p>可以根据抽样结果估算该分布函数的参数，从而完成参数的学习。</p>

<ul>
<li><p>前提：系统已经存在，但参数未知；</p></li>

<li><p>方法：通过采样的方式，获得一定数量的样本，从而学习该系统的参数。</p></li>
</ul>

<p>举个例子，比如投硬币试验中，进行N次试验，n次朝上，N-n次朝下，可以认为，是进行了N次(独立)抽样。</p>

<p>假定朝上的概率为p，使用对数似然函数作为目标函数：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/bDa3c5mJkl.png?imageslim" alt="mark" /></p>

<h2 id="应用bernoulli版本的大数定理">应用Bernoulli版本的大数定理</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/3g2d10jba8.png?imageslim" alt="mark" /></p>

<p>附：Bernoulli版本的大数定理</p>

<p><strong>这个要拿到大数定理那里。</strong></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/idEJm6kHea.png?imageslim" alt="mark" /></p>

<p>现在我们想一下，正常的EM中：</p>

<h2 id="用采样改造em算法本身">用采样改造EM算法本身</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/7aDA13bj1F.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/f605D9l806.png?imageslim" alt="mark" />这个是能算出来的，我们可以按照这个概率去采样<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/ciEjGcDcHL.png?imageslim" alt="mark" />这个函数，假设采样了L次，我们用这L次的采样的期望值来近似真正的值</p>

<p>这种就是MC-EM算法。</p>

<p>上面是采样L次，那么最极端的情况就是采样一次。如果L次可以看作一个批处理方式的梯度下降的话，采样一次就是随机梯度下降。也就是说采样一次 是随机EM算法</p>

<p>而且这里用的是用MLE来推的EM算法。</p>

<p>而昨天说过有MAP的方式，极大后验概率，那么可以得到MAP版本的EM算法，即贝叶斯版本下的EM算法</p>

<p><strong>还是没明白这个拒绝采样是怎么用在EM算法上的？</strong></p>

<p>OK，在聊一下马尔科夫链这个问题：</p>

<p>如果你有一个概率分布p(x) 那么你怎么能够在计算机中生成一些样本出来呢？</p>

<p>它的方式就是马尔科夫链。</p>

<h2 id="重述采样">重述采样</h2>

<p>采样：给定概率分布p(x)，如何在计算机中生成它的若干样本？</p>

<p>方法：马尔科夫链模型</p>

<p>考虑某随机过程π，它的状态有n个，用1~n表示。记在当前时刻t时位于i状态，它在t+1时刻位于j状态的概率为P(i,j)=P(j|i)：即状态转移的概率只依赖于前一个状态。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/3fCF8Cj0CK.png?imageslim" alt="mark" /></p>

<h2 id="举例">举例</h2>

<p>假定按照经济状况将人群分成上、中、下三个阶层，用1、2、3表示。假定当前处于某阶层只和上一代有关，即：考察父代为第i阶层，则子代为第j阶层的概率。假定为如下转移概率矩阵：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/GFc2BejkmB.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/aFca980cJj.png?imageslim" alt="mark" /></p>

<p>概率转移矩阵</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/LB5Cg4H7cd.png?imageslim" alt="mark" /></p>

<p>不同初始概率时候的迭代结果：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/82IHEmBbli.png?imageslim" alt="mark" /></p>

<p>相当于这个稳定状态的分布与原始状态没有关系，与状态转移矩阵有关系。</p>

<h2 id="马尔科夫随机过程的平稳分布">马尔科夫随机过程的平稳分布</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Bc90EG4F94.png?imageslim" alt="mark" /></p>

<p>马尔科夫随机过程的平稳分布</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Igh513G34l.png?imageslim" alt="mark" /></p>

<p>也就是和说：如果状态转移矩阵确定了，求n次幂，归一到一个状态上去了，那么这个分布就是一个稳定分布</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/8HDk04Ca9D.png?imageslim" alt="mark" />这个x是一个行向量，这个非负解也就是P的特征值为1 的特征向量。</p>

<h2 id="马尔科夫随机过程与采样">马尔科夫随机过程与采样</h2>

<p>上述平稳分布的马尔科夫随机过程对采样带来很大的启发：对于某概率分布π ，生成一个能够收敛到概率分布π的马尔科夫状态转移矩阵P，则经过有限次迭代，一定可以得到概率分布π。</p>

<p>该方法可使用Monte Carlo模拟来完成，称之为MCMC(Markov Chain Monte Carlo)。</p>

<h2 id="细致平稳条件">细致平稳条件</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/h9d9l4JhCk.png?imageslim" alt="mark" /></p>

<p>P(i,j)是从i号状态转移到j号状态的概率，它去乘以当前位于第i号状态的概率。</p>

<p>而这个等式意味着我从i状态转移除去的概率等于转移进i状态的概率，而 (\forall i,j)  就是任何两个都满足这个，那么这个一出一进就得到稳态了。</p>

<p>细致平稳条件（detailed balance condition）</p>

<p>任何的两个状态都是平稳的。</p>

<p>刚才是想象的大概是这样，那么下面推一下：</p>

<p>细致平稳条件和平稳分布的关系</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/KFiCaEclEC.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/bbHEff0A5K.png?imageslim" alt="mark" />这个是讲<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/GGhJ8kh427.png?imageslim" alt="mark" />用细致平稳条件替换。而这个就是<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/lFJFIl09Ij.png?imageslim" alt="mark" />的离散化版本。</p>

<p>也就是说，如果满足细致平稳条件，那么最终满足P收敛到\pi 这个分布上去了。</p>

<p>所以，细致平稳条件真的是一个条件。</p>

<p>现在我们来想象下：</p>

<p>设定接受率</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/4c3210Kl3L.png?imageslim" alt="mark" /></p>

<p>利害，还可以把不满足改造成满足这个相等条件的。</p>

<p>厉害了<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/H10GjHcgbb.png?imageslim" alt="mark" />，这个的确是最简单的情况。</p>

<p>使用\alpha 改造对应的状态转移矩阵。嗯。改了之后，就能向细致平稳条件去接近。</p>

<p><strong>到底怎么实现的？而且满足了之后\alpha 怎么处理？为什么改造的\alpha 是接受率？</strong></p>

<p>现在我们再来想一下：</p>

<p>MCMC：Metropolis-Hastings算法</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/fkAchA8C3J.png?imageslim" alt="mark" /></p>

<p>我让(a(i,j))，(a(j,i)) 这两个值，这个\alpha 是接受率，这个接受率肯定是小于1的一个数。所以我的(a(i,j))就取<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/DF60lDiLG4.png?imageslim" alt="mark" />，而(a(j,i)) 就对应的就行？这个(a(j,i)) 是恒为1 还是什么？因为<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/jlI3ABB6B5.png?imageslim" alt="mark" />中的(a(i,j))越小，比如之前的圆内均匀采样的时候，只有78.5%的概率成功。</p>

<p>Metropolis  在模拟退火算法也是它的名字。<strong>是这样吗？什么是模拟退火算法？</strong></p>

<p>Metropolis-Hastings算法</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/haKe5gHbFi.png?imageslim" alt="mark" /></p>

<p>最终就能让这个马尔科夫过程得到我们想要的稳态</p>

<p>改造MCMC算法</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/jLc248Ed4K.png?imageslim" alt="mark" /></p>

<p>如果把1看作当前的维度，2 看作其它的维度，就得到了<img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/0259hh5liK.png?imageslim" alt="mark" /></p>

<h2 id="二维gibbs采样算法">二维Gibbs采样算法</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/gaeJjALGi3.png?imageslim" alt="mark" /></p>

<p>得到新的值之后，采样得到其它的值。</p>

<p>x得到y，y得到x。<strong>什么意思？</strong></p>

<p>x<em>t固定的时候，算y的概率，y</em>{t+1}固定的时候，算x的概率</p>

<p>将二维Gibbs采样推广到高维</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/ej8AmcG68K.png?imageslim" alt="mark" /></p>

<p>可见，为什么要讲这个采样呢？因为它可以解释Gibbs采样</p>

<p>其实这个有点像EM，也有点像SMO，也有点像坐标上升。</p>

<p><strong>没有明白？基本没明白？</strong></p>

<p>变分没有讲。重点是变分推导/似然下界变分推导和今天讲的Gibbs采样也有异曲同工之妙</p>

<h1 id="comment">COMMENT：</h1>

<p><strong>采样是在什么时候使用呢？</strong></p>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D/dl-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%AF%B9%E6%AF%94/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">DL 深度学习的框架介绍与对比</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E9%80%92%E5%BD%92%E7%BD%91%E7%BB%9C-rnn/">
            <span class="next-text nav-default">递归网络 RNN</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
