<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>最大熵模型 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="最大熵模型 对最大熵模型进行总结 学习的目的 目的，以另外一种角度来理解Logistic回归。而且，最大熵模型这个熵这个概念本身在后面都会用到。其" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="最大熵模型" />
<meta property="og:description" content="最大熵模型 对最大熵模型进行总结 学习的目的 目的，以另外一种角度来理解Logistic回归。而且，最大熵模型这个熵这个概念本身在后面都会用到。其" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" /><meta property="article:published_time" content="2018-08-12T20:09:22&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-12T20:09:22&#43;00:00"/>
<meta itemprop="name" content="最大熵模型">
<meta itemprop="description" content="最大熵模型 对最大熵模型进行总结 学习的目的 目的，以另外一种角度来理解Logistic回归。而且，最大熵模型这个熵这个概念本身在后面都会用到。其">


<meta itemprop="datePublished" content="2018-08-12T20:09:22&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-12T20:09:22&#43;00:00" />
<meta itemprop="wordCount" content="4026">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="最大熵模型"/>
<meta name="twitter:description" content="最大熵模型 对最大熵模型进行总结 学习的目的 目的，以另外一种角度来理解Logistic回归。而且，最大熵模型这个熵这个概念本身在后面都会用到。其"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">最新</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">最新</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">最大熵模型</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-12 </span>
        
        <span class="more-meta"> 4026 words </span>
        <span class="more-meta"> 9 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#最大熵模型">最大熵模型</a></li>
<li><a href="#学习的目的">学习的目的</a></li>
<li><a href="#学习目标">学习目标</a></li>
<li><a href="#需要具备的基础知识">需要具备的基础知识</a></li>
<li><a href="#一个例子作为引子">一个例子作为引子</a>
<ul>
<li><a href="#下面是求解过程">下面是求解过程：</a></li>
<li><a href="#p-i-画出来如下"> (p_i) 画出来如下：</a></li>
<li><a href="#结论与问题">结论与问题：</a></li>
</ul></li>
<li><a href="#一个称假币的问题">一个称假币的问题</a>
<ul>
<li><a href="#问题解答">问题解答：</a></li>
<li><a href="#确保找到假币-的理论次数下界-是多少">确保找到假币 的理论次数下界 是多少</a></li>
<li><a href="#ok-我们把它梳理到一般意义上">OK，我们把它梳理到一般意义上</a></li>
</ul></li>
<li><a href="#我们还知道了硬币可能是假币的概率">我们还知道了硬币可能是假币的概率</a>
<ul>
<li><a href="#解答">解答</a></li>
</ul></li>
<li><a href="#huffman编码">Huffman编码</a>
<ul>
<li><a href="#解释huffman编码">解释Huffman编码</a></li>
<li><a href="#代码如下">代码如下</a></li>
</ul></li>
<li><a href="#最大熵模型-1">最大熵模型</a>
<ul>
<li><a href="#最大熵模型的原则">最大熵模型的原则</a></li>
<li><a href="#举个例子">举个例子</a></li>
<li><a href="#而maxent的一般式">而Maxent的一般式</a></li>
<li><a href="#最大熵模型-最大条件熵">最大熵模型：最大条件熵</a></li>
<li><a href="#最大熵模型总结">最大熵模型总结</a></li>
</ul></li>
<li><a href="#最大熵模型与-logistic-softmax-回归">最大熵模型与 Logistic/Softmax 回归</a>
<ul>
<li><a href="#λ的求解">λ的求解</a></li>
</ul></li>
<li><a href="#再次强调一下">再次强调一下：</a></li>
<li><a href="#总结">总结：</a></li>
<li><a href="#附-iis算法公式推导-这个没讲">附：IIS算法公式推导 这个没讲</a>
<ul>
<li><a href="#iis的思想">IIS的思想</a></li>
</ul></li>
<li><a href="#comment">COMMENT：</a>
<ul>
<li><a href="#相关资料">相关资料</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="最大熵模型">最大熵模型</h1>

<p>对最大熵模型进行总结</p>

<h1 id="学习的目的">学习的目的</h1>

<p>目的，以另外一种角度来理解Logistic回归。而且，最大熵模型这个熵这个概念本身在后面都会用到。其实今天的内容很多都是信息论里面的内容。<strong>看来信息论要仔细掌握下。</strong></p>

<h1 id="学习目标">学习目标</h1>

<ul>
<li>理解并掌握熵 Entropy 的定义

<ul>
<li>理解 “Huffman编码是所有编码中总编码长度最短的” 熵含义</li>
</ul></li>
<li>理解联合熵H(X,Y)、相对熵D(X||Y) 、条件熵H(X|Y)、互信息I(X,Y)的定义和含义，并了解如下公式：

<ul>
<li>(H(X\mid Y) = H(X,Y) - H(Y)=H(X) - I(X,Y))</li>
<li>(H(Y\mid X) = H(X,Y) - H(X)=H(Y) – I(X,Y))</li>
<li>(I(X,Y) = H(X) - H(X\mid Y) = H(X) + H(Y) - H(X,Y) \geq 0)</li>
</ul></li>
<li>掌握最大熵模型 Maxent</li>
<li> Maximum Entropy Models</li>
<li>了解最大熵在自然语言处理NLP中的应用</li>
<li>最大熵模型和极大似然估计MLE的关系</li>
<li>副产品：了解数据分析、函数作图的一般步骤</li>
</ul>

<h1 id="需要具备的基础知识">需要具备的基础知识</h1>

<p><strong>这个在后面的证明中会用到，因此写在这里，实际上可以写在后面，挪到后面去。</strong></p>

<p>预备定理</p>

<p><img src="http://images.iterate.site/blog/image/180728/F34f9GgC6g.png?imageslim" alt="mark" /></p>

<h1 id="一个例子作为引子">一个例子作为引子</h1>

<p>对给定的某个骰子，经过N次投掷后发现，点数的均值为5.5，请问：再投一次出现点5的概率有多大？</p>

<h2 id="下面是求解过程">下面是求解过程：</h2>

<p>令6个面朝上的概率是 ((p_1,p_2,\codts p_6))，用向量p表示。</p>

<p>那么目标函数是：(H(\overrightarrow{p})=-\sum_{i=1}^{6}p_ilnp_i) <strong>为什么目标函数是这么一个目标函数？</strong></p>

<p>约束条件是：(\sum_{i=1}^{6}p<em>i=1)。(\sum</em>{i=1}^{6}i\cdot p_i=5.5)</p>

<p>也就是说，这是一个在约束条件下求极值的问题，因此可以直接使用Lagrange乘子法，写出Lagrange函数：</p>

<p>[L(\overrightarrow{p},\lambda_1,\lambda<em>2)=-\sum</em>{i=1}^{6}p_ilnp_i+\lambda<em>i(1-\sum</em>{i=1}^{6}p_i)+\lambda<em>2(5.5-\sum</em>{i=1}^{6}i\cdot p_i)]</p>

<p>直接对(p_i)进行求导：</p>

<p>[\begin{align<em>}\frac{\partial L}{\partial p_i}&amp;=lnp_i+1-\lambda_1-i\cdot \lambda_2==0\&amp;\Rightarrow p_i=e^{1-\lambda_1-i\cdot \lambda_2}\end{align</em>}]</p>

<p>就求出了 (p_i) ，然后将这个 (p_i) 带入到两个约束条件中，就求出了对应的(\lambda)</p>

<p>[\lambda_1=5.932,\lambda_2=-1.087]</p>

<p>将测试的 (p_i) 画出来如下：</p>

<h2 id="p-i-画出来如下"> (p_i) 画出来如下：</h2>

<p><img src="http://images.iterate.site/blog/image/180728/jhhalEdh5f.png?imageslim" alt="mark" /></p>

<h2 id="结论与问题">结论与问题：</h2>

<p>可见出现6 的概率是很高的。而且上面这个目标函数求的结果与我们的直观想象还是很接近的，但是这个目标函数 (H(\overrightarrow{p})=-\sum_{i=1}^{6}p_ilnp_i) 是什么呢？那里来的呢？</p>

<p>好了，下面正式开始：</p>

<h1 id="一个称假币的问题">一个称假币的问题</h1>

<p>假设有5个硬币：1,2,3,4,5，其中一个是假的，且比真币轻。有一架没有砝码的天平，天平每次能比较两堆硬币，得出的结果可能是以下三种之一：</p>

<ul>
<li><p>左边比右边轻</p></li>

<li><p>右边比左边轻</p></li>

<li><p>两边同样重</p></li>
</ul>

<p>问：至少要几次称量才能确保找到假币？</p>

<h2 id="问题解答">问题解答：</h2>

<p>称量方法如下图：</p>

<p><img src="http://images.iterate.site/blog/image/180728/fhKKFDmme6.png?imageslim" alt="mark" /></p>

<p>因此，答案为：至少称 2 次就可以找出假币。</p>

<p>但是，现在就有一个问题了，为什么一定是至少称量2次才能<strong>确保</strong>找到假币呢？怎么确定没有更少的次数呢？是呀？</p>

<p>OK，下面就是讲怎么知道2是称量来确保找到假币的下界的：</p>

<h2 id="确保找到假币-的理论次数下界-是多少">确保找到假币 的理论次数下界 是多少</h2>

<p>我们先假设 (x) 就是假硬币的序号，那么 (x) 满足于 (x\in X={1,2,3,4,5})。</p>

<p>然后呢，我们可以用 (y_i) 来表示第 (i) 次使用天平得到的结果，同样的， (y_i) 满足于 (y\in Y={1,2,3}) 其中的 1 表示”左轻“，2表示“平衡”，3表示“右轻”。当我们用天平进行称量的时候，如果称了 n 次，那么得到的结果序列就是 (y_1y_2\cdots y_n)。</p>

<p>由于 (y\in Y={1,2,3})  ，也就是说 (y_i) 每次只有三种选择 ，因此 (y_1y_2\cdots y_n) 的所有可能组合数目为(3^n)。</p>

<p>下面就是一个比较难理解的东西了：</p>

<p>之前的题目的意思可以概括称：通过一些称量来得到假币的序号。而现在我已经一系列的称量： (y_1y_2\cdots y_n) ，也有了假币的序号： (x) 。</p>

<p>虽然我不知道这些序号都是些什么，但是我可以理解为：当我能够通过称量得到假币的序号的时候，这个时候的 (y_1y_2\cdots y_n) 就确定了一个 (x) 。虽然我这个时候仍然不知道 (x) 具体是什么， (y_1y_2\cdots y_n) 具体是什么。</p>

<p>而这种 (y_1y_2\cdots y_n) 就确定了一个 (x) 就可以写成一个映射：(map(y_1y_2\cdots y_n)=x)。</p>

<p>那么这个时候就需要注意了：我的这个映射的前提是什么？是这个 (x) 能够被确定下来。而假如说我的y序列只有一步 (y_1) ，那么我最多只能映射出 (3^1) 种的 (x)，少于5。也就是说，我只称量一次的时候，这个时候的称量序列不足以确定所有的 (x) ，因此，要想确认出所有的 (x) ，我的称量序列的长度至少也需要是2，即：(3^2\geq 5) 。</p>

<p>也就是说：(y_1y_2\cdots y_n) 的变化数目要大于等于 x的变化数目。即要：(3^n\geq 5)。</p>

<h2 id="ok-我们把它梳理到一般意义上">OK，我们把它梳理到一般意义上</h2>

<p>我们把它扩展到一般意义上 ：<strong>为什么可以这样扩展？需要什么限制条件吗？</strong></p>

<p>[\mid Y\mid ^n\geq\mid X\mid]</p>

<p>即：</p>

<p>[\mid Y\mid^n\geq \mid X\mid\Rightarrow nlog\mid Y\mid \geq log\mid X\mid \Rightarrow n\geq frac{log\mid X\mid}{log \mid Y\mid}]</p>

<p>也就是说，我们可以这样认为：</p>

<p>我们想用 (y_1y_2\cdots y_n)  来表达(x)。即相当于将x的信息通过编码来实现： (x:y_1y_2\cdots y_n)  <strong>（注意，这种编码的思想在HMM中仍然会进行考察，到时候回来补充一下）</strong>。</p>

<p>所以，可以这么看：x 是不确定程度的一个度量，y 是一次编码的表达能力，那么：</p>

<ul>
<li><p>X的总不确定度是：(H(x)=log\mid X\mid=log 5)</p></li>

<li><p>Y的表达能力是：(H(Y)=log\mid Y\mid=log3)</p></li>
</ul>

<p>那么至少需要多少个Y才能准确的表示X？</p>

<p>[\frac{H(X)}{H(Y)}=\frac{log5}{log3}=1.46]</p>

<p>** 厉害啊，这种编码的思想，利害，要再理解下。 **</p>

<p>OK，到这里，我们就大概知道了这种基本的称假币问题，所引起的编码的最小长度的问题。嗯还是很厉害的。</p>

<p>下面，我们针对这个问题进行变化：</p>

<h1 id="我们还知道了硬币可能是假币的概率">我们还知道了硬币可能是假币的概率</h1>

<p>假设有5个硬币：1,2,3,4,5，其中一个是假的，比其他的硬币轻。已知第1、2个硬币是假硬币的概率都是三分之一；第2、3、4个是假硬币的概率都是九分之一。有一架没有砝码的天平，假设使用天平n次能够找到假币。问n的期望值至少是多少？</p>

<p>与上面的问题相比，我们不仅知道有一个假币，还知道了这几个硬币可能是假币的概率。</p>

<p>那么怎么求解呢？</p>

<h2 id="解答">解答</h2>

<p><sup>1</sup>&frasl;<sub>3</sub> 概率的硬币有2个，1/9 概率的硬币有3个，也就是说：一个天平的表达能力是log3,1/3概率的时候 是 -log(<sup>1</sup>&frasl;<sub>3</sub>) 也就是log3。从而可以算出期望是4/3。</p>

<p>那么：</p>

<p>[(\frac{1}{3}+\frac{1}{3})\times \frac{log3}{log3}+3\frac{1}{9}\times \frac{log9}{log3}=\frac{4}{3}]</p>

<p><strong>为什么可以这么算？没明白？为什么提到了期望？</strong></p>

<p>思考一下：(log_2p)是什么？<strong>为什么是以2为底数？</strong></p>

<h1 id="huffman编码">Huffman编码</h1>

<p>还记得Huffman编码吗？刚才的4/3其实本质上就是Huffman编码至少的平均长度。<strong>什么是Huffman编码？那里讲到的？为什么本质上是它的至少的平均长度？</strong></p>

<h2 id="解释huffman编码">解释Huffman编码</h2>

<p><img src="http://images.iterate.site/blog/image/180728/l67ebbBLFA.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/53gcC6H9EL.png?imageslim" alt="mark" /></p>

<p>先把至少的拿出来，再把其次的拿出来，因为这里面是天平，所以是三叉树，选三个最小的拿出来，得到结果之后再选三个最小的拿出来就得到了。<strong>没明白？什么是Huffman编码？与这个地方有什么关系吗？</strong></p>

<h2 id="代码如下">代码如下</h2>

<p><strong>需要自己实现一遍</strong></p>

<p><img src="http://images.iterate.site/blog/image/180728/fGK8GebH8G.png?imageslim" alt="mark" /></p>

<p>熵的那部分我拿出来了。</p>

<p>下面开始解释最大熵模型：</p>

<h1 id="最大熵模型-1">最大熵模型</h1>

<p>Maximum Entropy</p>

<h2 id="最大熵模型的原则">最大熵模型的原则</h2>

<ul>
<li><p>承认已知事物(知识)</p></li>

<li><p>对未知事物不做任何假设，没有任何偏见</p></li>
</ul>

<p>不进行任何的假设就相当于熵最大。</p>

<h2 id="举个例子">举个例子</h2>

<p><img src="http://images.iterate.site/blog/image/180728/hmI7aAihKK.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/14DLHmHIk2.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/Kl29cCIF61.png?imageslim" alt="mark" /></p>

<p>怎么解呢？</p>

<p>首先，我们知道：概率平均分布等价于熵最大。</p>

<p>因此问题转化为：计算X和Y的分布，使 H(Y|X) 达到最大值，并且满足如下条件：</p>

<p><img src="http://images.iterate.site/blog/image/180728/Be77lfh906.png?imageslim" alt="mark" /></p>

<p>即可以写成：</p>

<p><img src="http://images.iterate.site/blog/image/180728/3Cbh4dEiF2.png?imageslim" alt="mark" /></p>

<p>OK，这样就可以使用 Lagrange 方法。</p>

<h2 id="而maxent的一般式">而Maxent的一般式</h2>

<p><img src="http://images.iterate.site/blog/image/180728/e0haI09mI4.png?imageslim" alt="mark" /></p>

<p>其中得特征(Feature)和样本(Sample)：</p>

<p>特征：(x,y)</p>

<ul>
<li>y:这个特征中需要确定的信息</li>
<li>x:这个特征中的上下文信息</li>
</ul>

<p>样本：关于某个特征(x,y)的样本，特征所描述的语法现象在标准集合里的分布：</p>

<ul>
<li>(xi,yi)对</li>
<li>yi 是 y 的一个实例</li>
<li>xi 是 yi 的上下文</li>
<li>(x1,y1)  (x2,y2)  (x3,y3) ……</li>
</ul>

<p>特征函数</p>

<p><img src="http://images.iterate.site/blog/image/180728/JDA7Fig1BG.png?imageslim" alt="mark" /></p>

<p>条件 Constraints</p>

<p><img src="http://images.iterate.site/blog/image/180728/a7aE0lhbJE.png?imageslim" alt="mark" /></p>

<p>那么我们想在这个模型熵取期望的话：</p>

<p>特征 $f$ 在模型中的期望值：</p>

<p><img src="http://images.iterate.site/blog/image/180728/KGb0C38b4b.png?imageslim" alt="mark" /></p>

<p>其中 $p(xi)$ 替换为实验中给定的。</p>

<h2 id="最大熵模型-最大条件熵">最大熵模型：最大条件熵</h2>

<p><img src="http://images.iterate.site/blog/image/180728/cb5DJmfL69.png?imageslim" alt="mark" /></p>

<p>最大熵模型在NLP中的完整提法</p>

<p><img src="http://images.iterate.site/blog/image/180728/0A5IJkI8GH.png?imageslim" alt="mark" /></p>

<h2 id="最大熵模型总结">最大熵模型总结</h2>

<p><img src="http://images.iterate.site/blog/image/180728/f5hBLLlGKa.png?imageslim" alt="mark" /></p>

<p>OK，这里做一些解释：为什么要求maxH(Y|X)，不求maxH(x,y) 因为我们给定了样本，给定了x去求y 。</p>

<p>特征函数是什么？为额什么会有这些特征函数？</p>

<p><img src="http://images.iterate.site/blog/image/180728/5a7A3e5A07.png?imageslim" alt="mark" /></p>

<p>是样本上某个(x,y)的频率，可以近似于概率。</p>

<p><img src="http://images.iterate.site/blog/image/180728/GgbDCH4hHK.png?imageslim" alt="mark" />这个是实际的这个，想让它等于样本上的(\widetilde{E}(f_i)) 。</p>

<p>因此可以列出Lagrange函数：</p>

<p><img src="http://images.iterate.site/blog/image/180728/9gCFaAimdd.png?imageslim" alt="mark" /></p>

<p>最大熵模型MaxEnt的目标拉格朗日函数L</p>

<p><img src="http://images.iterate.site/blog/image/180728/BHCdDhCAIi.png?imageslim" alt="mark" /></p>

<p>上面求偏导的过程比较麻烦，因为是一个函数对一个函数求导。</p>

<p>因此这个地方补充一下为什么是可以求导的：</p>

<p>“泛函求导”** 泛函求导这部分可以单独拿出来。**</p>

<p><img src="http://images.iterate.site/blog/image/180728/4BimLeg40I.png?imageslim" alt="mark" /></p>

<p>泛函求导——“类比”</p>

<p><img src="http://images.iterate.site/blog/image/180728/h3LGdmmCK8.png?imageslim" alt="mark" /></p>

<p>嗯，补充完了，这个就说明了L的第一个括号里面求导之后的结果为什么是那样的。</p>

<p>我们再重新强调一下求出的结果：</p>

<p>最优解形式Exponential：求偏导，等于0</p>

<p><img src="http://images.iterate.site/blog/image/180728/fCbHAcL0eg.png?imageslim" alt="mark" /></p>

<p>这个地方解释一下：把(exp(1-\lambda<em>0)) 写成 (Z</em>{\lambda}(x))，然后把得到的 (p(y|x)) 带回到约束条件(\sum_{y\in Y}^{} p(y|x)=1) 里面，让它加和等于去，这样的话就是：</p>

<p><img src="http://images.iterate.site/blog/image/180728/hBBCHHkb7J.png?imageslim" alt="mark" /></p>

<p>由于是关于 y 的求和，而  (Z_{\lambda}(x)) 与 y 无关，因此，可以提出来，即：</p>

<p><img src="http://images.iterate.site/blog/image/180728/H4Fld0ki4m.png?imageslim" alt="mark" /></p>

<p>那么我们之前的Logistic回归的模型是什么样子呢？</p>

<h1 id="最大熵模型与-logistic-softmax-回归">最大熵模型与 Logistic/Softmax 回归</h1>

<p><img src="http://images.iterate.site/blog/image/180728/Jeh7Hf8A3F.png?imageslim" alt="mark" /></p>

<p>Logistic回归的这个式子不管c等于0还是1，都可以写成：</p>

<p>而我们求的最大熵模型也是e的某某线性加和。</p>

<p>如果从Logistic回归变成softmax回归，即从两点分布变成k点分布：</p>

<p>直接推广可以得到：</p>

<p><img src="http://images.iterate.site/blog/image/180728/D2L3jCm32f.png?imageslim" alt="mark" /></p>

<p>底下是规划因子，上面就是这个e的某某，所以Logistic回归它给出的结论也是在那种情况之下的熵最大的情况，求得那个分界面，分割线，就是在那种情况下熵最大的一种分类。</p>

<p>可见，这二者是完全统一的。<strong>再看下。</strong></p>

<p>之前我们在讲极大似然估计的时候：说最大似然估计是：</p>

<p><img src="http://images.iterate.site/blog/image/180728/4BAlIKcfEh.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/lLg38GABJe.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/bmbJC9DHd2.png?imageslim" alt="mark" /></p>

<p>上面第二个式子是二元情况。第二项是常数（因为凡是加一个横杠的指的是我们的样本空间），可以忽略，第一项就是一开始从其他方法求出的条件熵的定义式。</p>

<p>因此我们算极大似然估计取极大值和我们算熵取极大值，本质上是一个东西。</p>

<p>也就是说：MLE与条件熵：</p>

<p><img src="http://images.iterate.site/blog/image/180728/Bgi8Jl4Iik.png?imageslim" alt="mark" /></p>

<p>求L的对偶函数</p>

<p><img src="http://images.iterate.site/blog/image/180728/mE90I196jg.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/D1iaBk1gGl.png?imageslim" alt="mark" /></p>

<p>最终得出的结论：</p>

<p><img src="http://images.iterate.site/blog/image/180728/AIAkEGfA86.png?imageslim" alt="mark" /></p>

<p>可以看出，熵和似然是通过两个思路看待同一个问题。</p>

<p>关于(\lambda)怎么求解呢？</p>

<h2 id="λ的求解">λ的求解</h2>

<p><img src="http://images.iterate.site/blog/image/180728/Lj7jLGBmKF.png?imageslim" alt="mark" /></p>

<p>IIS要了解下。</p>

<p>另外 softmax目标函数是这个样子：</p>

<p><img src="http://images.iterate.site/blog/image/180728/jHgKjKAHmb.png?imageslim" alt="mark" /></p>

<h1 id="再次强调一下">再次强调一下：</h1>

<p><img src="http://images.iterate.site/blog/image/180728/3KIHl56jJC.png?imageslim" alt="mark" /></p>

<h1 id="总结">总结：</h1>

<p>最大熵模型出了与softmax是一直的，而且在NLP上还是用的很多的。</p>

<p><img src="http://images.iterate.site/blog/image/180728/2cj4mglG7d.png?imageslim" alt="mark" /></p>

<h1 id="附-iis算法公式推导-这个没讲">附：IIS算法公式推导 这个没讲</h1>

<h2 id="iis的思想">IIS的思想</h2>

<p>假设最大熵模型当前的参数向量是λ，希望找到新的参数向量λ+δ，使得模型的对数似然函数值L增加。重复这一过程，直至找到对数似然函数的最大值。</p>

<p><img src="http://images.iterate.site/blog/image/180728/dK0hadBjI2.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/CgDb6G4IEH.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/bEJfJck514.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/978dLh0839.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/jJH71HbegL.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/3K0KGGjD1k.png?imageslim" alt="mark" /></p>

<p><img src="http://images.iterate.site/blog/image/180728/edhGBD2HBj.png?imageslim" alt="mark" /></p>

<h1 id="comment">COMMENT：</h1>

<p>参考文献也都要看下。</p>

<ul>
<li><p>Elements of Information Theory (Cover &amp; Thomas)</p></li>

<li><p><a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92">http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92</a></p></li>

<li><p>A maximum entropy approach to natural language processing (Adam Berger)</p></li>

<li><p>A Brief MaxEnt Tutorial (Adam Berger)</p></li>

<li><p>Learning to parse natural language with maximum entropy models (Adwait Ratnaparkhi)</p></li>

<li><p>A simple Introduction to Maximum Entropy Models for Natural Language Processing (Adwait Ratnaparkhi)</p></li>

<li><p>统计学习方法，李航著，清华大学出版社，2012年</p></li>
</ul>

<p>要结合 李航的《统计学习方法》那本书再看一下这个地方。</p>

<p><strong>还是由很多地方不明白的。</strong></p>

<h2 id="相关资料">相关资料</h2>

<ol>
<li>七月在线 机器学习</li>
</ol>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%89%80%E6%9C%89%E7%AE%97%E6%B3%95%E6%94%B6%E9%9B%86/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">机器学习所有算法收集</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%91%E4%BC%BC%E8%AF%AF%E5%B7%AE%E5%92%8C%E4%BC%B0%E8%AE%A1%E8%AF%AF%E5%B7%AE/">
            <span class="next-text nav-default">什么是近似误差和估计误差？</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
