<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>10 隐马尔可夫模型 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="第10章隐马尔可夫模型 隐马尔可夫模型（hidden Markov model, HMM）是可用于标注问题的统计学 习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/10-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="10 隐马尔可夫模型" />
<meta property="og:description" content="第10章隐马尔可夫模型 隐马尔可夫模型（hidden Markov model, HMM）是可用于标注问题的统计学 习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/10-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/" /><meta property="article:published_time" content="2018-06-26T19:22:01&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-26T19:22:01&#43;00:00"/>
<meta itemprop="name" content="10 隐马尔可夫模型">
<meta itemprop="description" content="第10章隐马尔可夫模型 隐马尔可夫模型（hidden Markov model, HMM）是可用于标注问题的统计学 习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，">


<meta itemprop="datePublished" content="2018-06-26T19:22:01&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-26T19:22:01&#43;00:00" />
<meta itemprop="wordCount" content="9826">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="10 隐马尔可夫模型"/>
<meta name="twitter:description" content="第10章隐马尔可夫模型 隐马尔可夫模型（hidden Markov model, HMM）是可用于标注问题的统计学 习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">10 隐马尔可夫模型</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-26 </span>
        
        <span class="more-meta"> 9826 words </span>
        <span class="more-meta"> 20 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#第10章隐马尔可夫模型">第10章隐马尔可夫模型</a>
<ul>
<li><a href="#10-1隐马尔可夫模型的基本概念">10.1隐马尔可夫模型的基本概念</a></li>
<li><a href="#10-2概率计算算法">10.2概率计算算法</a></li>
<li><a href="#2-卿">2&gt;，(•/卿</a></li>
<li><a href="#10-3学习算法">10.3学习算法</a></li>
</ul></li>
</ul></li>
</ul></li>
<li><a href="#i沾-1">I沾) &lsquo;■1</a>
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#10-4预测算法">10.4预测算法</a></li>
<li><a href="#本章概要">本章概要</a></li>
<li><a href="#继续阅读">继续阅读</a></li>
<li><a href="#参考文献">参考文献</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h5 id="第10章隐马尔可夫模型">第10章隐马尔可夫模型</h5>

<p>隐马尔可夫模型（hidden Markov model, HMM）是可用于标注问题的统计学 习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型.本 章首先介绍隐马尔可夫模型的基本概念，然后分别叙述隐马尔可夫模型的概率计 算算法、学习算法以及预测算法.隐马尔可夫模型在语音识别、自然语言处理、 生物信息、模式识别等领域有着广泛的应用.•</p>

<h6 id="10-1隐马尔可夫模型的基本概念">10.1隐马尔可夫模型的基本概念</h6>

<p>10.1.1隐马尔可夫模型的定义</p>

<p>定义10.1 （隐马尔可夫模型）隐马尔可夫模型是关于时序的概率模型，描 述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生 成一个观测而产生观测随机序列的过程.隐藏的马尔可夫链随机生成的状态的序 列，称为状态序列（state sequence）；每个状态生成一个观测，而由此产生的观测 的随机序列，称为观测序列（observation sequence）.序列的每一个位置又可以看 作是一个时刻.</p>

<p>隐马尔可夫模型由初始概率分布、状态转移概率分布以及观测概率分布确 定.隐马尔可夫模型的形式定义如下：</p>

<p>设（2是所有可能的状态的集合，r是所有可能的观测的集合.</p>

<p>G = {?1，?2,    «    ^ = {V1»V2» —，VAf）</p>

<p>其中，是可能的状态数，似是可能的观测数.</p>

<p>I是长度为r的状态序列，o是对应的观测序列.</p>

<p>Z = （“，•••，&amp;），0==（°i，o2,…，0r）</p>

<p>W是状态转移概率矩阵：</p>

<p>吨］脚    （瓜1）</p>

<p>其中，</p>

<p>〜=叫+1=川卜9：），    / =    7=1,2,-,^    （10.2）</p>

<p>是在时刻f处于状态9,的条件下在时刻f +1转移到状态*的概率.</p>

<p>B是观测概率矩阵:</p>

<p>(io.3)</p>

<p>其中，</p>

<p>bj(k) = P(ot =vk\il=qj),    j =    (10.4)</p>

<p>是在时刻f处于状态a的条件下生成观测vt的概率.</p>

<p>龙是初始状态概率向量：</p>

<p>龙=沐)    (10.5)</p>

<p>其中，</p>

<p>^1=^1 =?,)» *&rsquo; = 1，2,…，7V    (10.6)</p>

<p>是时刻f = l处于状态9,的概率.</p>

<p>隐马尔可夫模型由初始状态概率向量/T、状态转移概率矩阵4和观测概率矩 阵5决定.*和3决定状态序列，5决定观测序列.因此，隐马尔可夫模型又可 以用三元符号表示，即</p>

<p>A = (A,B,7T)    (10.7)</p>

<p>次况疋称为隐马尔可夫模型的三要素.</p>

<p>状态转移概率矩阵X与初始状态概率向量;r确定了隐藏的马尔可夫链，生成 不可观测的状态序列.观测概率矩阵5确定了如何从状态生成观测，与状态序列 综合确定了如何产生观测序列.</p>

<p>从定义可知，隐马尔可夫模型作了两个基本假设：</p>

<p>(1)    齐次马尔可夫性假设，即假设隐藏的马尔可夫链在任意时刻T的状态只 依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻f无关.</p>

<p>/ = l,2,-,r    (10.8)</p>

<p>(2)    现测独立性假设，即假设任意时刻的观测只依赖于该时刻的马尔可夫链 的状态，与其他观测及状态无关.</p>

<p>P(o, |/r,Qr,»^o,) = P(o, 10    (10.9)</p>

<p>隐马尔可夫模型可以用于标注，这时状态对应着标记.标注问题是给定观测 的序列预测其对应的标记序列.可以假设标注问题的数据是由隐马尔可夫模型生 成的.这样我们可以利用隐马尔可夫模型的学习与预测算法进行标注.</p>

<p>下面看一个隐马尔可夫模型的例子.</p>

<p>例10.1 （盒子和球模型）假设有4个盒子，每个盒子里都装有红白两种颜 色的球，盒子里的红白球数由表10.1列出，</p>

<p>表10.1各盒子的红白球数</p>

<table>
<thead>
<tr>
<th>盒子</th>
<th>1 2</th>
<th>3</th>
<th>4</th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>红球数</td>
<td>5</td>
<td>3</td>
<td>6</td>
<td>8</td>
</tr>

<tr>
<td>白球数</td>
<td>5</td>
<td>7</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>

<p>按照下面的方法抽球，产生一个球的颜色的观测序列：开始，从4个盒子里 以等概率随机选取1个盒子，从这个盒子里随机抽出1个球，记录其颜色后，放 回；然后，从当前盒子随机转移到下一个盒子，规则是：如果当前盒子是盒子1， 那么下一盒子一定是盒子2,如果当前是盒子2或3,那么分别以概率0.4和0.6 转移到左边或右边的盒子，如果当前是盒子4,那么各以0.5的概率停留在盒子4 或转移到盒子3•:确定转移的盒子后，再从这个盒子里随机抽出1个球，记录其 颜色，放回；如此下去，重复进行5次，得到一个球的颜色的观测序列：</p>

<p>0 = ｛紅，红，白，白，红｝</p>

<p>在这个过程中，观察者只能观测到球的颜色的序列，观测不到球是从哪个盒子取 出的，即观测不到盒子的序列.</p>

<p>在这个例子中有两个随机序列，一个是盒子的序列（状态序列），一个是球 的颜色的观测序列（观测序列）.前者是隐藏的，只有后者是可观测的.这是一 个隐马尔可夫模型的例子，根据所给条件，可以明确状态集合、观测集合、序列 长度以及模型的三要素.</p>

<p>盒子对应状态，状态的集合是</p>

<p>e = ｛盒子1,盒子2,盒子3,盒子4｝，AT = 4</p>

<p>球的颜色对应观测.观测的集合是</p>

<p>r = ｛红，白M = 2</p>

<p>状态序列和观测序列长度T = 5.</p>

<p>初始概率分布为</p>

<p>^ = （0.25,0.25,0.25,0.25）T</p>

<p>状态转移概率分布为</p>

<p>观测概率分布为</p>

<table>
<thead>
<tr>
<th>■0.5</th>
<th>0.5&rsquo;</th>
</tr>
</thead>

<tbody>
<tr>
<td>0.3</td>
<td>0.7</td>
</tr>

<tr>
<td>0.6</td>
<td>0.4</td>
</tr>

<tr>
<td>0.8</td>
<td>0.2</td>
</tr>
</tbody>
</table>

<p>10.1.2观測序列的生成过程</p>

<p>根据隐马尔可夫模型定义，可以将一个长度为r的观测序列o=(O|，o2，…，Of) 的生成过程描述如下，</p>

<p>算法10.1 (观测序列的生成)</p>

<p>输入：隐马尔可夫模型A=(45，幻，观测序列长度 输出：观测序列O = (opo2,…，or) •</p>

<p>(1)    按照初始状态分布;T产生状态4</p>

<p>(2)    令f = l</p>

<p>(3)    按照状态f,的观测概率分布久(Jt)生成o,</p>

<p>(4〉按照状态f,的状态转移概率分布｛&lt;?^｝产生状态f,+1, /&lt;+1=l,2,-,JV (5)令f = / + l:如果f&lt;r,转步(3): 则，终止    ■</p>

<p>10.13隐马尔可夫模型的3个基本问题</p>

<p>隐马尔可夫模型有3个基本问题：</p>

<p>(1)    概率计算问题.给定模型2 = G4W)和观测序列6? = (OpO2，…，Or)，计 算在模型A下观测序列O出现的概率14 .</p>

<p>(2)    学习问题.己知观测序列(? = (o,，o2，…，or),估计模型2 = 04,5,龙)参数， 使得在该模型下观测序列概率P(O | A)最大.即用极大似然估计的方法估计参数.</p>

<p>(3)    预测问题，也称为解码(decoding)问题.已知模型A = (J,5，;r)和观测</p>

<p>序列O = (ot,o2, -,oT),求对给定观测序列条件概率P(门6?)最大的状态序列 / =    即给定观测序列，求最有可能的对应的状态序列.</p>

<p>下面各节将逐一介绍这些基本问题的解法.</p>

<h6 id="10-2概率计算算法">10.2概率计算算法</h6>

<p>本节介绍计算观测序列概率P((9|2)的前向(forward)与后向(backward) 算法.先介绍概念上可行但计算上不可行的直接计算法.</p>

<p>10.2.1直接计算法</p>

<p>给定模型又=04,足龙)和观测序列O = (O)，Oj，…，or)，计算观测序列0出现的 概率P(O|2).最直接的方法是按概率公式直接计算.通过列举所有可能的长度 为r的状态序列/=(^2，…劣)，求各个状态序列J与观测序列o=(q，02，…，&amp;) 的联合概率P(o,/1A),然后对所有可能的状态序列求和，得到P(O|2).</p>

<p>状态序列J=(^2，一,y的概率是</p>

<p>(10.10)</p>

<p>对固定的状态序列，观测序列O = (O|,o2，…，七)的概率是 P{O\1,X),</p>

<p>P(Q | /，乂)=《峨(o2)-(or)    (10.11)</p>

<p>&lt;?和/同时出现的联合概率为</p>

<p>P(C?，/|A) = P(O|/，A)P(/| 乂)</p>

<p>(o2).&ldquo;awA (°r)    (10.12)</p>

<p>然后，对所有可能的状态序列/求和，得到观测序列O的概率P(O|A)，即</p>

<p>P(O|2) = JP(O|/,A)P(/|A)</p>

<p>1</p>

<p>=L nKbh    (Or)    (10.13)</p>

<p>H…，</p>

<p>但是，利用公式(10.13)计算量很大，是O(77\TT)阶的，这种算法不可行.</p>

<p>下面介绍计算观测序列概率/&gt;(O|A)的有效算法：前向-后向算法(forward-backward algorittim).</p>

<p>10.2.2前向算法</p>

<p>首先定义前向概率.</p>

<p>定义10J (前向概率)给定隐马尔可夫模型定义到时刻/部分观测序列 为o,，o2，…，o,且状态为弘的概率为前向概率，记作</p>

<p>^l(i) = P(oi,o2,&ndash;,o„il =qt\X)    (10.14)</p>

<p>可以递推地求得前向概率«,(/)及观测序列概率| A).</p>

<p>算法10.2 (观测序列概率的前向算法)</p>

<p>输入：隐马尔可夫模型A ,观测序列O;</p>

<p>输出：观测序列概率P(O|A).</p>

<p>(1)初值</p>

<p>al(i) = ^ib,(ol),    i=l，2,…，2V</p>

<p>(10.15)</p>

<p>(2)递推对z=i，2,…，r-i，</p>

<p>«(+,0)=</p>

<p>6&rsquo;(o&rsquo;+i)， j = 1，D</p>

<p>(10.16)</p>

<p>(3)终止</p>

<p>P(O|2) = £ar(f)</p>

<p>(10.17) ■</p>

<p>前向算法，步骤(1)初始化前向概率，是初始时刻的状态/,= 9,和观测o,的 联合概率.步骤(2)是前向概率的递推公式，计算到时刻Z + 1部分观测序列为 01，02,&ldquo;‘，0,，0,+1且在时刻&lt; + 1处于状态见的前向概率，如图10.1所示.在式(10.16) 的方括弧里，既然a,(j)是到时刻/观测到并在时刻/处于状态<em>的前 向概率，那么乘积a,(»a&gt;(就是到时刻r观测到o,，o2,.• 并在时刻f处于状态a 而在时刻f +1到达状态的联合概率.对这个乘积在时刻/的所有可能的AT个状 态％求和，其结果就是到时刻/观测为Ol,o2,-,o,并在时刻f +1处于状态&amp;的联 合概率.方括弧里的值与观测概率6,(o,+l)的乘积恰好是到时刻f + 1观测到 0、，02，…，o,，ol+l并在时刻</em> + 1处于状态必的前向概率a,+1⑺.步骤(3)给出P(O|2) 的计算公式.因为</p>

<p>Oj.(i) = P(Oi ,o2,&ndash;,oT,iT=qi\X)</p>

<p>N</p>

<p>p(oia)=2^.(o</p>

<p>t</p>

<p>a,(J)</p>

<p>r+l</p>

<p>a川0)</p>

<p>图10.1前向概率的递推公式</p>

<p>如图10.2所示，前向算法实际是基于“状态序列的路径结构”递推计算 的算法.前向算法髙效的关键是其局部计算前向概率，然后利用路径结构将前向 概率“递推”到全局，得到P(&lt;9|A).具体地，在时刻f = l,计算《,(&lt;•)的W个值</p>

<p>(i = l，2,…，JV);在各个时刻f = l，2,…，r-1，计算a&lt;+1(f)的 TV■个值G = l»2，…，AT), 而且每个a,+1(0的计算利用前一时刻况个珥(/).减少计算量的原因在于每一次 计算直接引用前一个时刻的计算结果，避免重复计算.这样，利用前向概率计算 P(O\Xj的计算量是O(N2T)阶的，而不是直接计算的阶.</p>

<p>12    3    T</p>

<p>图10.2观测序列路径结构</p>

<p>例10.2考虑盒子和球模型又=,状态集合0 = {1,2,3}.观测集合K = {红，白},</p>

<table>
<thead>
<tr>
<th>A =</th>
<th>•0.5 0.2 0.3-0.3 0.5 0.20.2 0.3 0.5</th>
<th>，B =</th>
<th>0.5 0.5&rsquo; 0.4 0.60.7 0,3</th>
<th>，^ = (0.2,0.4,0.4)t</th>
</tr>
</thead>

<tbody>
<tr>
<td>设r=3, 0=(红，白，红)</td>
<td>试用前向算法计算P((91义).</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<p>解按照算法10.2</p>

<p>(1)    计算初值</p>

<p>«i(i)=^A(0i)=o.io</p>

<p>«i(2) = ^262(0,) = 0.16 叫⑶= ^3^(°i)= 0.28</p>

<p>(2)    递推计算</p>

<table>
<thead>
<tr>
<th>«2(1)=</th>
<th>./=!</th>
<th>^(&lt;^) = 0.154x0^=0.077</th>
</tr>
</thead>

<tbody>
<tr>
<td>«2(2)=</td>
<td>3./-l    .</td>
<td>Z»2(o2) = 0.184x0.6 = 0.1104</td>
</tr>

<tr>
<td>巧⑶=</td>
<td>.1-1    .</td>
<td>i&gt;3(o2) = 0.202 x 0.3 = 0.0606</td>
</tr>

<tr>
<td>a3(l) =</td>
<td>32X⑺％_(-i    •</td>
<td>il(o3) = 0.04187</td>
</tr>

<tr>
<td>叫(2)=</td>
<td>31X(加j</td>
<td>*2(^3) = 0-03551</td>
</tr>
</tbody>
</table>

<p>^(3)= [Za20X^3(o3) = 0.05284</p>

<p>(3)终止</p>

<p>3</p>

<p>P(O I 义)=2X(:) = 0.13022</p>

<p>i=i</p>

<p>10.2.3后向算法</p>

<p>定义1(L3 (后向概率)给定隐马尔可夫模型Z,定义在时刻状态为g,的 条件下，从/+I到7的部分观测序列为心，o，+2，…，外的概率为后向概率，记作</p>

<p>P,(t) = P(oM,oM,—,oT\i, =q„X)    (10.18)</p>

<p>可以用递推的方法求得后向概率肩0及观测序列概率P(O\X).</p>

<p>算法10.3 (观测序列概率的后向算法)</p>

<p>输入：隐马尔可夫模型；I，观测序列O:</p>

<p>输出：观测序列概率P(O|A).</p>

<p>(1)</p>

<p>A_(f) = l，» = 1,2,-,2V    (10.19)</p>

<p>⑵ 5^/=r-i,r-2,-,i</p>

<p>N .</p>

<p>⑽=1沙i = l,2,-,N    (10.20)</p>

<p>7=1</p>

<p>(3)</p>

<p>(10.21) ■ i=l</p>

<p>步骤(1)初始化后向概率，对最终时刻的所有状态9,•规定疼(:-)=1.步骤(2) 是后向概率的递推公式.如图10.3所示，为了计算在时刻f状态为弘条件下时刻 t + 1之后的观测序列为Ol+I,Ol+2,-,Or的后向概率爲(0,只需考虑在时刻f +1所有</p>

<p>&rsquo;    t+1</p>

<p>A(0    ^i(J)</p>

<p>图10.3后向概率递推公式</p>

<p>可能的个状态％的转移概率（即~项），以及在此状态下的观测0,+1的观测概 率（即6/0,+1）项），然后考虑状态％之后的观测序列的后向概率（即戊+1（力项）.步 骤（3）求的思路与步骤（2〉一致，只是初始概率^代替转移概率.</p>

<p>利用前向概率和后向概率的定义可以将观测序列概率P（O| 乂）统一写成</p>

<p>N N</p>

<p>P（OIX} =    （°&lt;+i）A+&gt;（7）-，= 1，2,…，r-1    （10.22）</p>

<p>i=l j=</p>

<p>此式当r=1和r = r-1时分别为式（10.17）和式（10.21）.</p>

<p>10.2.4 —些概率与期望值的计算</p>

<p>利用前向概率和后向概率，可以得到关于单个状态和两个状态概率的计算公式.</p>

<p>1.给定模型A和观测O,在时刻f处于状态％的概率.记</p>

<p>Z，（0 = P（d|O,2）    （10.23）</p>

<p>可以通过前向后向概率计算.事实上，</p>

<p>r,（i） = P（i, =?,|0,4）=气冗广）</p>

<p>由前向概率a,⑺和后向概率戍（f）定义可知：</p>

<p>«,（0A0&rsquo;）=P0,=?/,o|A）</p>

<p>于是得到:</p>

<p>/,(/)=</p>

<p>«,佩(j)</p>

<p>N</p>

<h6 id="2-卿">2&gt;，(•/卿</h6>

<p>(10.24)</p>

<p>2.给定模型义和观测（?，在时刻，处于状态孓且在时刻/ + 1处于状态受.的概 率.记</p>

<p>^(iJ) = P(.it= =    0,又)    (10.25)</p>

<p>可以通过前向后向概率计算：</p>

<p>尸(6=劣，,什1 =4,0|义)</p>

<p>= ?(»»/+! =?y,O|A)</p>

<p>&lt;=l 7=1</p>

<p>p0«= 9,，i,+i = g},O\X) = a,OX^(o,+1)A+i(y)</p>

<p>所以</p>

<p>^(M)=    -    (10.26)</p>

<p>ZSa/ (加A (A+爲1 (•/) i-l 7=1</p>

<p>3.将以0和s(/，y)对各个时刻求和，可以得到一些有用的期望值：</p>

<p>(1)    在观测O下状态f出现的期望值</p>

<p>Ez,(0    (10.27)</p>

<p>h»l</p>

<p>(2)    在观测O下由状态f转移的期望值</p>

<p>r-i</p>

<p>(10.28)</p>

<p>&lt;=i</p>

<p>(3)    在观测O下由状态转移到状态/的期望值</p>

<p>(10.29)</p>

<h6 id="10-3学习算法">10.3学习算法</h6>

<p>隐马尔可夫模型的学习，根据训练数据是包括观测序列和对应的状态序列还 是只有观测序列，可以分别由监督学习与非监督学习实现.本节首先介绍监督学 习算法，而后介绍非监督学习算法——Baum-Weich算法(也就是EM算法).</p>

<p>10.3.1监督学习方法</p>

<p>假设已给训练数据包含S个长度相同的观测序列和对应的状态序列 {(O,，A)，(O2 J2),&ldquo;s(Os，心)}，那么可以利用极大似然估计法来估计隐马尔可夫模 型的参数.具体方法如下.</p>

<p>\1.    转移概率％的估计</p>

<p>设样本中时刻Z处于状态Z•时刻f +1转移到状态y的频数为4 ,那么状态转 移概率％的估计是</p>

<p>-» i = \,2,&mdash;,N ■, j =    (10.30)</p>

<p>j=i</p>

<p>\2.    观测概率的估计</p>

<p>设样本中状态为/并观测为4的频数是那么状态为y观测为的概率</p>

<p>b人k、的估计是</p>

<p>咖，7 = 1，2,…，V: k = \,2, -,M    （10.31）</p>

<p>3.初始状态概率;r,的估计弋为5个样本中初始状态为孓的频率</p>

<p>由于监督学习需要使用训练数据，而人工标注训练数据往往代价很髙，有时</p>

<p>就会利用非监督学习的方法.</p>

<p>10.3.2 Baum-Welch 算法</p>

<p>假设给定训练数据只包含s个长度为r的观测序列｛o„o2&gt;&ndash;,os｝而没有对 应的状态序列，目标是学习隐马尔可夫模型2 =    的参数.我们将观测序</p>

<p>列数据看作观测数据0,状态序列数据看作不可观测的隐数据/，那么隐马尔可 夫模型事实上是一个含有隐变量的概率模型</p>

<p>P（O| A） = Yp（P\1,X）P（I\X）    （10.32）</p>

<p>它的参数学习可以由EM算法实现.</p>

<p>\1.    确定完全数据的对数似然函数</p>

<p>所有观测数据写成O = （Ol，02，…，外），所有隐数据写成/«乂，…，&amp;），完全 数据是（0，/） = （01，02，&rdquo;，，七，/|，4，&rdquo;4）.完全数据的对数似然函数是108戸（0,/|2）.</p>

<p>\2.    EM算法的E步：求2函数2（又，X）®</p>

<p>0（人 J） =    | MP（O,I\1）    （10.33）</p>

<p>i</p>

<p>其中，I是隐马尔可夫模型参数的当前估计值，A是要极大化的隐马尔可夫模型 参数.</p>

<p>P（O，I\ 又）：W （hW 于是函数eoU）可以写成：</p>

<p>1</p>

<p>+    y（OJ\X） + Z（Zlog 久（o,）y（O,/|I）</p>

<p>(10.34)</p>

<p>式中求和都是对所有训练数据的序列总长度r进行的.</p>

<p>①按照12函数的定义    _</p>

<p>e（A,J） = £/[logP（O,/|A）|Oj]</p>

<p>式（10.33）略去了对A而言的常数因子1/P（O|Z）.</p>

<p>\3. EM算法的M步：极大化2函数2GU)求模型参数迭况疋 由于要极大化的参数在式(10.34)中单独地出现在3个项中，所以只需对各</p>

<p>项分别极大化.</p>

<p>(1)式(10.34)的第1项可以写成：</p>

<p>_ N</p>

<p>£iog^p(o,^ =/|X)</p>

<p>/    i=l</p>

<p>N</p>

<p>注意到％满足约束条件£^=i,利用拉格朗日乘子法，写出拉格朗日函数：</p>

<p>(=i</p>

<p>Xlog^P(O,^ =/| J) +</p>

<p>1=1    V&lt;=i )</p>

<p>对其求偏导数并令结果为0</p>

<p>去［写 log^P叫=||X) +    -1^ = 0</p>

<p>(10.35)</p>

<p>(10.36)</p>

<p>得</p>

<p>P(O，ij =/| J) + ^rf =0</p>

<p>对z‘求和得到/</p>

<p>Z=-P(O|J)</p>

<p>代入式(10.35)即得</p>

<p>1 P(O\I)</p>

<p>(2)式(10.34)的第2项可以写成</p>

<p>f    A    _ n n r-i</p>

<p>Z    \P(O,I I J) = 2：22：log a9p(o,i, = /,/f+I =7|J)</p>

<p>! \»=1    J    /=1 }=\ t=</p>

<p>N</p>

<p>类似第1项，应用具有约束条件=1的拉格朗日乘子法可以求出</p>

<p>y=l</p>

<p>tp(o，i, =“+i =7)^)</p>

<p>av=J^~n-Z—    (10.37)</p>

<p>(3〉式(10.34)的第3项为</p>

<p>Z|Zlog^ (o,)W/| ^)=ZElogdy(O&lt;)P(O,/, =y|X)</p>

<p>/、&rsquo;=i    J    j=i(=i</p>

<p>M</p>

<p>同样用拉格朗日乘子法，约束条件是艺~(幻=1.注意，只有在0,=%时6/0，)对</p>

<p>*=i</p>

<p>6/幻的偏导数才不为0,以/(o,=v*)表示.求得</p>

<p>^p(o,i(=y|Xy(o, =v*)</p>

<p>bj(k) = ^-T-=——    (10.38)</p>

<p>(-1</p>

<p>10.3.3 Baum-Welch模型参数估计公式</p>

<p>将式(10.36)〜式(10.38)中的各概率分别用X(0，么G，7)表示，则可将相应 的公式写成：</p>

<p>%=丰、——    (10.39)</p>

<p>!&gt;(&lsquo;•)</p>

<p>(-1</p>

<p>S X/O)</p>

<p>bj(k) = ^^——    (10.40)</p>

<p>&lsquo;Lr.(j)</p>

<p>^ = zi(0</p>

<p>(10.41)</p>

<p>其中,《(:•，力分别由式(10.24)及式(10.26)给出.式(10.39)〜式(10.41)就 是Baum-Welch算法(Baum-Welch algorithm),它是EM算法在隐马尔可夫模型 学习中的具体实现，由Baum和Welch提出.</p>

<p>算法 10.4 (Baum-Welch 算法)</p>

<p>输入：观测数据^)：^©!^，…^);</p>

<p>输出：隐马尔可夫模型参数.</p>

<p>(1)    初始化</p>

<p>对》= 0,选取〜w, bj(k)m, &lt;)，得到模型</p>

<p>(2)    递推.对”二口，…，</p>

<p>av</p>

<h3 id="i沾-1">I沾) &lsquo;■1</h3>

<p>IL r&rsquo;G )</p>

<p>——</p>

<p>1＞(力</p>

<p>z=i</p>

<p>龙” W)</p>

<p>右端各值按观测^^久么…:…和模型沪^^^^^^计算.式中y,(i), 4；(/,y)由式(10.24)和式(10.26)给出.</p>

<p>(3)终止.得到模型参数义&rdquo;^=04°^，沪■</p>

<h6 id="10-4预测算法">10.4预测算法</h6>

<p>下面介绍隐马尔可夫模型预测的两种算法：近似算法与维特比算法(VKerbi algorithm).</p>

<p>10.4.1近似算法</p>

<p>近似算法的想法是，在每个时刻f选择在该时刻最有可能出现的状态＜，从 而得到一个状态序列尸=(«，…＜)，将它作为预测的结果.</p>

<p>给定隐马尔可夫模型A和观测序列0，在时刻/处于状态g,的概率z,G)是</p>

<p>⑽ A(&lsquo;)</p>

<p>N</p>

<p>7-1</p>

<p>A(砌 尸(0|1)</p>

<p>M&rsquo;) =</p>

<p>(10.42)</p>

<p>在每一时刻f最有可能的状态/；是</p>

<p>»；= argmax[r(0&rsquo;)]» t = l,2,-,r    (10.43)</p>

<p>从而得到状态序列/• =(«，•••，/；).</p>

<p>近似算法的优点是计算简单，其缺点是不能保证预测的状态序列整体是最有 可能的状态序列，因为预测的状态序列可能有实际不发生的部分.事实上，上述 方法得到的状态序列中有可能存在转移概率为0的相邻状态，即对某些 %=0时.尽管如此，近似算法仍然是有用的.</p>

<p>10.4.2维特比算法</p>

<p>维特比算法实际是用动态规划解隐马尔可夫模型预测问题，即用动态规划 (dynamic programming)求概率最大路径(最优路径).这时一条路径对应着一个 状态序列.</p>

<p>根据动态规划原理，最优路径具有这样的特性：如果最优路径在时刻/通过 结点＜，那么这一路径从结点＜到终点与的部分路径，对于从＜到$的所有可能</p>

<p>的部分路径来说，必须是最优的.因为假如不是这样，那么从(到i；就有另一条 更好的部分路径存在，如果把它和从＜到达I了的部分路径连接起来，就会形成一 条比原来的路径更优的路径，这是矛盾的.依据这一原理，我们只需从时刻f = l开 始，递推地计算在时刻f状态为/的各条部分路径的最大概率，直至得到时刻f=r 状态为/的各条路径的最大概率.时刻f=r的最大概率即为最优路径的概率P*, 最优路径的终结点$也同时得到.之后，为了找出最优路径的各个结点，从终结 点芩开始，由后向前逐步求得结点么，…，/；，得到最优路径尸=(«，…，//).这 就是维特比算法.</p>

<p>首先导入两个变量J和定义在时刻/状态为I•的所有单个路径 中概率最大值为</p>

<p>^,(0 = 6=    I    i = l,2,—,N    (10.44)</p>

<p>由定义可得变量多的递推公式：</p>

<p>=    z = I,2,-,M f = l，2,…，T-l (10.45)</p>

<p>定义在时刻f状态为/的所有单个路径(以，…，U)中概率最大的路径的第 t-l个结点为</p>

<p>V, (0 = arg    (J)aji 1»    »&rsquo; = 1,2,-,JV    (10.46)</p>

<p>下面介绍维特比算法.</p>

<p>算法10.5 (维特比算法)</p>

<p>输入：模型义二队丑⑻和观测^二以，^^，…^);</p>

<p>输出：最优路径/•=(«，•••&lt;).</p>

<p>⑴初始化</p>

<p>(0 = ^(0,) , i = l,2,&mdash;,N ^(i) = 0,    i = l,2,&mdash;,N</p>

<p>(2)    递推.对f = 2,3,…，r</p>

<p>仙)=    久(A)， i = \2,-,N</p>

<p>K⑺=arg    (j)ayj], i = \,2,—,N</p>

<p>(3)    终止</p>

<p>h =argmax[^r(0]</p>

<p>(4)最优路径回溯.对r = r-l，r-2,…，1</p>

<p>&lt; =^/+i(Ci)</p>

<p>求得最优路径r=(K, •••，/；).</p>

<p>下面通过一个例子来说明维特比算法.</p>

<p>^■ = (0.2,0.4,0.4)t</p>

<p>已知观测序列0 =(红，白，红)，试求最优状态序列，即最优路径广</p>

<p>解如图10.4所示，要在所有可能的路径中选择一条最优路径，按照以下步</p>

<p>骤处理：</p>

<p>(1)初始化.在/ = 1时，对每一个状态；，/ = 1,2,3»求状态为/观测o,为红 的概率，记此概率为戎(0,则</p>

<p>相) = 物)=林(红)，/ = 1,2,3</p>

<p>代入实际数据</p>

<p>4(1) = 0.10, &lt;55(2) = 0.16, &lt;55(3) = 0.28 记的(i) = 0， i = l，2,3.</p>

<p>状态</p>

<p>0.28    0.042    0.0147</p>

<p>3</p>

<p>2</p>

<p>2    3    时间</p>

<p>图10.4求最优路径</p>

<p>(2)在f = 2时，对每个状态i = l,2,3-求在r = l时状态为j•观测为红并在 t = 2时状态为f观测o2为白的路径的最大概率，记此最大概率为J2(f)，则</p>

<p>=    [^iC/)ayJ6,(o2)</p>

<p>同时，对每个状态:•，i = 1,2,3,记录概率最大路径的前一个状态 ^2(0 = argm^ lAG)%]，i = 1，2,3</p>

<p>计算:</p>

<p>么(1)=氏贸OXJ^CoJ</p>

<p>=max{0.10 x 0.5,0.16 x 0.3,0.28 x 0.2} x 0.5 = 0.028</p>

<p>妁(1) = 3</p>

<p>式(2) = 0.0504,的(2) = 3 式(3) = 0.042，％(3) = 3</p>

<p>同样，在/ = 3时，</p>

<p>^(0 = max[^2 ⑺…,]恤)</p>

<p>^3(i) = argmax[S2 (j)^]</p>

<p>戎(1) = 0.00756，(I) = 2 么(2) = 0.01008，叭(2) = 2 戎(3) = 0.0147，(3) = 3</p>

<p>(3)以产表示最优路径的概率，则</p>

<p>P* = mot J3 (/) = 0.0147</p>

<p>最优路径的终点是f3*:</p>

<p>C =argmax [么(f)] = 3</p>

<p>(4)由最优路径的终点    逆向找到« :</p>

<p>在 ^ = 2 时，= ⑶=3 在/ = 1 时，= K ⑶=3</p>

<p>于是求得最优路径.即最优状态序列广=««) = (3,3,3).    ■</p>

<h6 id="本章概要">本章概要</h6>

<p>1.隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链 随机生成不可观测的状态的序列，再由各个状态随机生成一个观测而产生观测的 序列的过程.</p>

<p>隐马尔可夫模型由初始状态概率向量;T、状态转移概率矩阵和观测概率矩 阵5决定.因此，隐马尔可夫模型可以写成Z =（忒5,疋）.</p>

<p>隐马尔可夫模型是一个生成模型，表示状态序列和观测序列的联合分布，但 是状态序列是隐藏的，不可观测的.</p>

<p>隐马尔可夫模型可以用于标注，这时状态对应着标记.标注问题是给定观测 序列预测其对应的标记序列.</p>

<p>\2.    概率计算问题.给定模型4 = G4，B，;r）和观测序列O = （0|，o2，…，叫），计算 在模型/I下观测序列O出现的概率1 zl）.前向-后向算法是通过递推地计算前 向-后向概率可以髙效地进行隐马尔可夫模型的概率计算.</p>

<p>\3.    学习问题.已知观测序列o=（Ol，o2，…，or）,估计模型a=（y）参数， 使得在该模型下观测序列概率最大.即用极大似然估计的方法估计参 数.Baum-Welch算法，也就是EM算法可以髙效地对隐马尔可夫模型进行训练.它 是一种非监督学习算法.</p>

<p>\4.    预测问题.已知模型2 = （45，龙）和观测序列＜? = （01，02，&hellip;，叫），求对给定 观测序列条件概率P（/10）最大的状态序列/ =认人，…人）.维特比算法应用动态 规划髙效地求解最优路径，即概率最大的状态序列.</p>

<h6 id="继续阅读">继续阅读</h6>

<p>隐马尔可夫模型的介绍可见文献［1,2］,特别地，文献［1］是经典的介绍性论文. 关于Baum-Welch算法可见文献［3,4］.可以认为概率上下文无关文法（probabilistic context-free grammar）是隐马尔可夫模型的~种推广，隐马尔可夫模型的不可观 测数据是状态序列，而概率上下文无关文法的不可观测数据是上下文无关文法树［5】. 动态贝叶斯网络（dynamic Bayesian network）是定义在时序数据上的贝叶斯网络， 它包含隐马尔可夫模型，是一种特例［6］.</p>

<p>10.1给定盒子和球组成的隐马尔可夫模型;l =    其中，</p>

<table>
<thead>
<tr>
<th></th>
<th>0.5 0.2 0.3n</th>
<th></th>
<th>0.5 0.5&rsquo;</th>
</tr>
</thead>

<tbody>
<tr>
<td>A =</td>
<td>0.3 0.5 0.2</td>
<td>，B =</td>
<td>0.4 0.6</td>
</tr>

<tr>
<td></td>
<td>0.2 0.3 0.5</td>
<td></td>
<td>0.7 0.3</td>
</tr>
</tbody>
</table>

<p>龙= (0.2,0.4,0.4)t</p>

<p>设r=4，o=（红，白，红，白），试用后向算法计算p（o|X）.</p>

<p>10.2考虑盒子和球组成的隐马尔可夫模型2 = （X，5，；r）,其中，</p>

<table>
<thead>
<tr>
<th></th>
<th>0.5 0.1 0.4&rdquo;</th>
<th></th>
<th>&lsquo;0.5 0.5&rsquo;</th>
</tr>
</thead>

<tbody>
<tr>
<td>A =</td>
<td>0.3 0.5 0.2</td>
<td>，B =</td>
<td>0.4 0.6</td>
</tr>

<tr>
<td></td>
<td>0.2 0.2 0.6</td>
<td></td>
<td>0.7 0.3</td>
</tr>
</tbody>
</table>

<p>^ = （0.2,03,0.5/</p>

<p>设r=8，0=（红，白，红,红，白,红,白，白），用前向后向概率计算叩4=込|＜9，乂）. 10.3在习题lo.i中，试用维特比算法求最优路径r =（＜,«＜）.</p>

<p>10.4试用前向概率和后向概率推导</p>

<p>N N</p>

<p>Wla）=2S«；0XA（o«+.）A*.（7）. /=i,2,-,r-i</p>

<p>10.5比较维特比算法中变景5的计算和前向算法中变量a的计算的主要区别.</p>

<h6 id="参考文献">参考文献</h6>

<p>[1]    Rabiner L, Juang B. An introduction to hidden markov Models. IEEE ASSP Magazine, January 1986</p>

<p>[2]    Rabiner L. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of IEEE, 1989</p>

<p>[3]    Baum L, et al. A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains. Annals of Mathematical Statistics, 1970,41: 164-171</p>

<p>[4]    Bilmes JA. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models, <a href="http://ssli.ee.washington.edu/~bilmes/mypubs/">http://ssli.ee.washington.edu/~bilmes/mypubs/</a> bilmes1997-em.pdf</p>

<p>[5]    Lari K, Young SJ. Applications of stochastic context-free grammars using the Inside-Outside algorithm, Computer Speech &amp; Language, 1991, 5(3): 237-257</p>

<p>[6]    Ghahramani Z. Learning Dynamic Bayesian Networks. Lecture Notes in Computer Science, Vol. 1387,1997,168-197</p>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/09-em%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">09 EM算法及其推广</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/11-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/">
            <span class="next-text nav-default">11 条件随机场</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
