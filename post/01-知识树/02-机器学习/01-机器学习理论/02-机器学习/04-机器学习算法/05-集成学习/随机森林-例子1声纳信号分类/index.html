<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>随机森林 例子1：声纳信号分类 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="随机森林 例子1：声呐信号分类 项目说明 给你一些声纳信号数据，来区分声纳的类型。 项目数据 链接：https://pan.baidu.com/s/1" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/05-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-%E4%BE%8B%E5%AD%901%E5%A3%B0%E7%BA%B3%E4%BF%A1%E5%8F%B7%E5%88%86%E7%B1%BB/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="随机森林 例子1：声纳信号分类" />
<meta property="og:description" content="随机森林 例子1：声呐信号分类 项目说明 给你一些声纳信号数据，来区分声纳的类型。 项目数据 链接：https://pan.baidu.com/s/1" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/05-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-%E4%BE%8B%E5%AD%901%E5%A3%B0%E7%BA%B3%E4%BF%A1%E5%8F%B7%E5%88%86%E7%B1%BB/" /><meta property="article:published_time" content="2018-08-12T19:51:57&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-12T19:51:57&#43;00:00"/>
<meta itemprop="name" content="随机森林 例子1：声纳信号分类">
<meta itemprop="description" content="随机森林 例子1：声呐信号分类 项目说明 给你一些声纳信号数据，来区分声纳的类型。 项目数据 链接：https://pan.baidu.com/s/1">


<meta itemprop="datePublished" content="2018-08-12T19:51:57&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-12T19:51:57&#43;00:00" />
<meta itemprop="wordCount" content="4889">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="随机森林 例子1：声纳信号分类"/>
<meta name="twitter:description" content="随机森林 例子1：声呐信号分类 项目说明 给你一些声纳信号数据，来区分声纳的类型。 项目数据 链接：https://pan.baidu.com/s/1"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">随机森林 例子1：声纳信号分类</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-12 </span>
        
        <span class="more-meta"> 4889 words </span>
        <span class="more-meta"> 10 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#随机森林-例子1-声呐信号分类">随机森林 例子1：声呐信号分类</a>
<ul>
<li><a href="#项目说明">项目说明</a></li>
<li><a href="#项目数据">项目数据</a></li>
<li><a href="#完整代码">完整代码</a></li>
<li><a href="#几个参数的说明与调整">几个参数的说明与调整</a></li>
<li><a href="#对于一颗树来说-每次split的时候都要重新选择-n-features-个-feature吗">对于一颗树来说，每次split的时候都要重新选择 n_features 个 feature吗？</a></li>
<li><a href="#判断一个string是不是一个数的时候-怎么判断">判断一个string是不是一个数的时候，怎么判断？</a></li>
<li><a href="#list里面的item也是list的时候-怎么拷贝">list里面的item也是list的时候，怎么拷贝？</a></li>
<li><a href="#标签的名字应该怎么取-用-class-value-还是用-labels">标签的名字应该怎么取？用 class_value 还是用 labels？</a></li>
<li><a href="#相关资料">相关资料</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="随机森林-例子1-声呐信号分类">随机森林 例子1：声呐信号分类</h1>

<h2 id="项目说明">项目说明</h2>

<p>给你一些声纳信号数据，来区分声纳的类型。</p>

<h2 id="项目数据">项目数据</h2>

<p>链接：<a href="https://pan.baidu.com/s/1J7SfU1R0eMMq8QztzCzQew">https://pan.baidu.com/s/1J7SfU1R0eMMq8QztzCzQew</a> 密码：kwrg</p>

<h2 id="完整代码">完整代码</h2>

<pre><code class="language-python">from random import seed, randrange, random
import copy
import math


​
# 导入csv文件
def load_data_set(file_name):
    datas = []
    with open(file_name, 'r') as fr:
        for line in fr.readlines():
            if not line:
                continue
            line_arr = []
            for featrue in line.split(','):
                str_f = featrue.strip()  # 移除空格
                try:
                    # 这里不能用str_f.isdigits()来判断是不是数值，因为小数点会被判定为 false
                    n = float(str_f)
                    line_arr.append(n)
                except:
                    if str_f == &quot;R&quot;:  # 分类标签
                        line_arr.append(1)
                    else:
                        line_arr.append(0)
            datas.append(line_arr)
    return datas


​
# 交叉采样 这个采样的方式的名字叫什么？忘记了
# 数据集dataset分成n_flods份，每次都是从dataset中抽取一个dataset/n_folds大小的list，为了用于交叉验证
# 抽出一个list的时候是又放回的还是无放回的？
def cross_validation_split(dataset, n_folds):
    dataset_split = list()
    fold_size = len(dataset) / n_folds
    for i in range(n_folds):
        dataset_copy = copy.deepcopy(dataset)
        fold = list()
        while len(fold) &lt; fold_size:
            index = randrange(len(dataset_copy))
            # 在采样出一个fold的时候，到底使用的是有放回的采样还是无放回的采样？
            fold.append(dataset_copy.pop(index))  # 无放回的方式
            # fold.append(dataset_copy[index])  # 有放回的方式
        dataset_split.append(fold)
    return dataset_split


​
# 尝试根据这一行的这个特征的特征值来划分 dataset
def get_split_result_with_feature_value(feature_index, feature_value, dataset):
    # 根据每个样本对应的这个特征的特征值大于或者小于这个值，来划分为两个列表
    left, right = list(), list()
    for row in dataset:
        if row[feature_index] &lt; feature_value:
            left.append(row)
        else:
            right.append(row)
    return left, right


​
# 为这一次的分割计算基尼指数
def calc_gini_index(groups, labels):  # 个人理解：计算代价，分类越准确，则 gini 越小
    gini = 0.0
    for label in labels:  # labels = [0, 1]
        for group in groups:  # groups = (left, right)
            size = len(group)
            if size == 0:
                # 说明左子树或者右子树没有
                continue
            proportion = [row[-1] for row in group].count(label) / float(size)
            gini += (proportion * (1.0 - proportion))  # 个人理解：计算代价，分类越准确，则 gini 越小
    # 左右两边的数量越一样，说明数据区分度不高，gini系数越大
    return gini


​
# 找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）
def get_current_features_split_result(dataset, n_features):
    # 获得标记的set
    labels = list(set(row[-1] for row in dataset))  # class_values =[0, 1]

    # b_index：最优的分类特征。b_value：分类特征值 。b_score：这里使用的是基尼系数。 b_groups：分类结果 ？
    b_index, b_value, b_score, b_groups = 999, 999, 999, None

    # 开始选择出 n_features 个特征
    ifeatures = list()
    while len(ifeatures) &lt; n_features:
        index = randrange(len(dataset[0]) - 1)
        if index not in ifeatures:
            ifeatures.append(index)
    # 从这些特征中找到一个最优的分法
    for index in ifeatures:  # 在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性
        for row in dataset:
            # 尝试根据这一行的这个特征的特征值来划分dataset
            groups = get_split_result_with_feature_value(index, row[index], dataset)  # groups=(left, right)
            gini = calc_gini_index(groups, labels)
            if gini &lt; b_score:
                b_index, b_value, b_score, b_groups \
                    = index, row[index], gini, groups
                # 将此时这些特征中分类最好的特征的index及其值返回，同时返回分类的左右类
    return {'index': b_index, 'value': b_value, 'groups': b_groups}


​
# 输出group中出现次数较多的标签
def to_terminal(group):
    outcomes = [row[-1] for row in group]
    # 输出 group 中出现次数较多的标签
    # max() 函数中，当 key 参数不为空时，就以 key 的函数对象为判断的标准 max这种用法要总结下。
    return max(set(outcomes), key=outcomes.count)


​
# 为一个结点创建子分割器，递归分类，直到分类结束
# max_depth = 10, min_size = 1,
def split(node, max_depth, min_size, n_features, current_depth):
    left, right = node['groups']
    del (node['groups'])
    # 左边或者右边已经没有了
    if not left or not right:
        # 就不用分了 设定这时候的 node 的左和右是同一种 label
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # 检查是不是超过了最大深度
    if current_depth &gt;= max_depth:
        # 如果超过了，但是还是没有结束，则选取数据中分类标签较多的作为结果，使分类提前结束，防止过拟合
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # 处理左边的
    if len(left) &lt;= min_size:
        # 左边的只有一个样本
        node['left'] = to_terminal(left)
    else:
        # 这个时候 node['left']是一个字典，形式为{'index':b_index, 'value':b_value, 'groups':b_groups}，
        # 也就是说 node是一个多层字典
        node['left'] = get_current_features_split_result(left, n_features)
        # 这个地方不明白了，难道每次递归的时候，n_features也要重新选择出来吗？不是使用的开始随机选出来的那些吗？
        # 这个n_features 的确是要重新选择出来的，我试过把features一开始选择出来，然后后面就用这些feature，但是效果基本维持在60% 上不去了
        split(node['left'], max_depth, min_size, n_features, current_depth + 1)  # 递归，depth+1计算递归层数
    # 处理右边的
    if len(right) &lt;= min_size:
        # 右边的只有一个样本
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_current_features_split_result(right, n_features)
        split(node['right'], max_depth, min_size, n_features, current_depth + 1)


​
# 创建一个决策树
# max_depth       决策树深度不能太深，不然容易导致过拟合 为什么？
# min_size        叶子节点的大小
def build_tree(train, max_depth, min_size, n_features):
    # 进行第一次划分
    root = get_current_features_split_result(train, n_features)
    # 对左右两边的数据 进行递归的调用 按照split里面的算法来看，之前用过的feature仍然是可能被选择的，是这样吗？
    split(root, max_depth, min_size, n_features, current_depth=1)
    return root


​
# 简单投票法判断出该行所属分类
def predict(tree, row):  # 预测模型分类结果
    if row[tree['index']] &lt; tree['value']:
        if isinstance(tree['left'], dict):
            return predict(tree['left'], row)
        else:
            return tree['left']
    else:
        if isinstance(tree['right'], dict):
            return predict(tree['right'], row)
        else:
            return tree['right']


​
# 使用一批树，对一行的结果进行预测
def bagging_predict(trees, row):
    predictions = [predict(tree, row) for tree in trees]
    # 返回结果中出现次数最多的结果，相当于投票
    return max(set(predictions), key=predictions.count)


​
# 可放回的重复采样，创建数据集的随机子样本
def sub_sample(dataset, ratio):
    sample = list()
    n_sample = round(len(dataset) * ratio)  # round() 方法返回浮点数x的四舍五入值。
    while len(sample) &lt; n_sample:
        # ，此则自助采样法。从而保证每棵决策树训练集的差异性
        index = randrange(len(dataset))
        sample.append(dataset[index])
    return sample


​
# 随机森林算法
# train           训练数据集
# test            测试数据集
# max_depth       决策树深度不能太深，不然容易导致过拟合
# min_size        叶子节点的大小
# sample_size     训练数据集的样本比例
# n_trees         决策树的个数
# n_features      选取的特征的个数
def evaluate_rf_effect(train, test, max_depth, min_size, sample_ratio, tree_num, n_features):
    trees = list()
    for i in range(tree_num):
        # 随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性
        sample = sub_sample(train, sample_ratio)
        # 创建一个决策树
        tree = build_tree(sample, max_depth, min_size, n_features)
        trees.append(tree)
    # OK，现在已经得到这些树了，我们使用这些树，对每一行进行预测
    predictions = [bagging_predict(trees, row) for row in test]
    return predictions


​
# 根据预测值和实际值，计算精确度
def calc_accuracy(actual, predicted):
    correct = 0
    for i in range(len(actual)):
        if actual[i] == predicted[i]:
            correct += 1
    return correct / float(len(actual)) * 100.0


​
​
if __name__ == '__main__':
    # 加载数据
    dataset = load_data_set('sonar-all-data.txt')
    # 准备样本数据
    # 将数据集采样为 n_folds 份，其中一个 fold 作为测试集，其余作为训练集，这样可以有 n_folds 套训练和测试样本
    # 遍历整个 folds ，实现交叉验证
    # 数据可以重复重复抽取，每一次 list 的元素是无重复的
    n_folds = 5  # 准备将样本数据分成5份，来进行交叉验证
    folds = cross_validation_split(dataset, n_folds)
    train_sets = []
    test_sets = []
    for fold in folds:
        train_set = list(folds)
        train_set.remove(fold)
        train_set = copy.deepcopy(sum(train_set, []))  # 这也可以，将4个folds合并成一个集合
        test_set = copy.deepcopy(fold)
        train_sets.append(train_set)
        test_sets.append(test_set)

    max_depth = 15  # 调参（自己修改） #决策树深度不能太深，不然容易导致过拟合
    min_size = 1  # 决策树的叶子节点最少的元素数量
    sample_ratio = 0.8  # 做决策树时候的样本的比例
    n_features = int(
        math.sqrt(len(dataset[0]) - 1))  # 需要自己调整，准确性与多样性之间的权衡，为什么n_features 要等于 int(sqrt((dataset[0])-1))？ 有什么说法吗？
    print(n_features)
    for n_trees in [1, 5, 10, 20]:  # 理论上树是越多越好   , 5,10, 20
        scores = list()
        for i in range(n_folds):
            predicted = evaluate_rf_effect(train_sets[i], test_sets[i],
                                           max_depth, min_size, sample_ratio, n_trees, n_features)
            actual = [row[-1] for row in test_sets[i]]
            print(predicted)
            print(actual)
            # 计算随机森林的预测结果的正确率
            accuracy = calc_accuracy(actual, predicted)
            scores.append(accuracy)
        seed(1)
        print('Trees: %d' % n_trees)
        print('Scores: %s' % scores)
        print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))
</code></pre>

<p>输出如下：</p>

<pre><code>7
[1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1]
[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
[1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
[0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1]
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
Trees: 1
Scores: [57.14285714285714, 64.28571428571429, 57.14285714285714, 54.761904761904766, 73.80952380952381]
Mean Accuracy: 61.429%
[0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1]
[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
[0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
[1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]
[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
[1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1]
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
Trees: 5
Scores: [73.80952380952381, 73.80952380952381, 61.904761904761905, 78.57142857142857, 80.95238095238095]
Mean Accuracy: 73.810%
[0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1]
[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
[1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]
[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
[0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
Trees: 10
Scores: [80.95238095238095, 78.57142857142857, 61.904761904761905, 78.57142857142857, 88.09523809523809]
Mean Accuracy: 77.619%
[0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1]
[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1]
[0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]
[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1]
[1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0]
[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]
[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0]
[1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]
[1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]
[1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]
Trees: 20
Scores: [85.71428571428571, 78.57142857142857, 69.04761904761905, 83.33333333333334, 88.09523809523809]
Mean Accuracy: 80.952%
</code></pre>

<p>嗯，还是有几个问题要说一下的：</p>

<h2 id="几个参数的说明与调整">几个参数的说明与调整</h2>

<ul>
<li>max_depth 我现在调到15，调到20的时候反而没有15的时候效果好，<strong>那么实际中到底怎么确定哪个参数好呢？</strong></li>
<li>sample_ratio 我用的是0.8，也就是说每棵树在生成的时候是根据train中的80%的样本生成的。</li>
<li>n_features 这里我用的是 math.sqrt(len(dataset[0]) - 1)) <strong>没明白为什么要用这个？</strong></li>
<li><strong>min_size 会有设置为不为1 的时候吗？</strong></li>
</ul>

<h2 id="对于一颗树来说-每次split的时候都要重新选择-n-features-个-feature吗">对于一颗树来说，每次split的时候都要重新选择 n_features 个 feature吗？</h2>

<p>之前，我以为，随机森林的每棵树在选择特征集的时候，只是在开始的时候选择一些特征，然后后面的split就用这些特征，但是这个代码看下来好像不是这样，而是每次的split，都是从完整特征集里面重新选出n_features个特征。<strong>为什么要这样做呢？</strong></p>

<p>不过，我试了下，如果只是开始的时候选一些特征，然后后面就用这些，那么最后的accuracy基本上就是60%左右，而且上不去了，如果像上面这样的话，就可以随着tree_num的增加一直增加accuracy。<strong>嗯，但是还是不清楚，这样做有什么道理呢？</strong></p>

<h2 id="判断一个string是不是一个数的时候-怎么判断">判断一个string是不是一个数的时候，怎么判断？</h2>

<p>之前程序里用的是 a.isdigits() 来判断 a 里面是不是每个字符都是数字，但是这样对于 0.02 这样的float 会判断为False，因为有小数点。因此这里直接用 try 来进行转换。</p>

<h2 id="list里面的item也是list的时候-怎么拷贝">list里面的item也是list的时候，怎么拷贝？</h2>

<p>这个，用 import copy    b=copy.deepcopy(a) 就可以，其它的比如[:] ，list(a) 什么的都不能将里面的list也进行拷贝。</p>

<h2 id="标签的名字应该怎么取-用-class-value-还是用-labels">标签的名字应该怎么取？用 class_value 还是用 labels？</h2>

<p><strong>这个一直想知道，我这里用的是labels，但是到底应该用什么？</strong></p>

<h2 id="相关资料">相关资料</h2>

<ol>
<li><a href="http://ml.apachecn.org/mlia/ensemble-random-tree-adaboost/#_3">第7章 集成方法 ensemble method</a></li>
</ol>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/05-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/adaboost-%E4%BE%8B%E5%AD%901%E4%BB%8E%E7%96%9D%E6%B0%94%E7%97%85%E7%97%87%E9%A2%84%E6%B5%8B%E7%97%85%E9%A9%AC%E7%9A%84%E6%AD%BB%E4%BA%A1%E7%8E%87/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">AdaBoost 例子1：从疝气病症预测病马的死亡率</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/01-sklearn/01-%E4%BB%8B%E7%BB%8D/">
            <span class="next-text nav-default">01 介绍</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
