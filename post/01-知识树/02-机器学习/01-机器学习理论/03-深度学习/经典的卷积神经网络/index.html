<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>经典的卷积神经网络 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="author: evo comments: true date: 2018-04-25 07:35:25&#43;00:00 layout: post link: http://106.15.37.116/2018/04/25/classic-cnn/ slug: classic-cnn title: 经典的卷积神经网络 wordpress_id: 3833 categories: - 随想与反思 tags: - &amp;lsquo;@todo&amp;rsquo; 相关资料： 七月在线 深度学习 缘由： 对经典的CNN网络进行分析总结，要知道" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="经典的卷积神经网络" />
<meta property="og:description" content="author: evo comments: true date: 2018-04-25 07:35:25&#43;00:00 layout: post link: http://106.15.37.116/2018/04/25/classic-cnn/ slug: classic-cnn title: 经典的卷积神经网络 wordpress_id: 3833 categories: - 随想与反思 tags: - &lsquo;@todo&rsquo; 相关资料： 七月在线 深度学习 缘由： 对经典的CNN网络进行分析总结，要知道" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><meta property="article:published_time" content="2018-07-28T22:28:31&#43;00:00"/>
<meta property="article:modified_time" content="2018-07-28T22:28:31&#43;00:00"/>
<meta itemprop="name" content="经典的卷积神经网络">
<meta itemprop="description" content="author: evo comments: true date: 2018-04-25 07:35:25&#43;00:00 layout: post link: http://106.15.37.116/2018/04/25/classic-cnn/ slug: classic-cnn title: 经典的卷积神经网络 wordpress_id: 3833 categories: - 随想与反思 tags: - &lsquo;@todo&rsquo; 相关资料： 七月在线 深度学习 缘由： 对经典的CNN网络进行分析总结，要知道">


<meta itemprop="datePublished" content="2018-07-28T22:28:31&#43;00:00" />
<meta itemprop="dateModified" content="2018-07-28T22:28:31&#43;00:00" />
<meta itemprop="wordCount" content="1600">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="经典的卷积神经网络"/>
<meta name="twitter:description" content="author: evo comments: true date: 2018-04-25 07:35:25&#43;00:00 layout: post link: http://106.15.37.116/2018/04/25/classic-cnn/ slug: classic-cnn title: 经典的卷积神经网络 wordpress_id: 3833 categories: - 随想与反思 tags: - &lsquo;@todo&rsquo; 相关资料： 七月在线 深度学习 缘由： 对经典的CNN网络进行分析总结，要知道"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">最新</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">最新</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">经典的卷积神经网络</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-07-28 </span>
        
        <span class="more-meta"> 1600 words </span>
        <span class="more-meta"> 4 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#todo">- &lsquo;@todo&rsquo;</a></li>
<li><a href="#相关资料">相关资料：</a></li>
</ul></li>
<li><a href="#缘由">缘由：</a></li>
<li><a href="#卷积神经网络典型cnn">卷积神经网络典型CNN</a></li>
<li><a href="#lenet">Lenet</a></li>
<li><a href="#alexnet">AlexNet</a></li>
<li><a href="#zfnet">ZFNet</a></li>
<li><a href="#vgg">VGG</a></li>
<li><a href="#googlenet">GoogLeNet</a></li>
<li><a href="#resnet-即-deep-residual-learning-network-非常深的残差网络">ResNet   即 Deep Residual Learning network 非常深的残差网络</a></li>
<li><a href="#comment">COMMENT：</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<hr />

<p>author: evo
comments: true
date: 2018-04-25 07:35:25+00:00
layout: post
link: <a href="http://106.15.37.116/2018/04/25/classic-cnn/">http://106.15.37.116/2018/04/25/classic-cnn/</a>
slug: classic-cnn
title: 经典的卷积神经网络
wordpress_id: 3833
categories:
- 随想与反思
tags:</p>

<h2 id="todo">- &lsquo;@todo&rsquo;</h2>

<!-- more -->

<h2 id="相关资料">相关资料：</h2>

<ol>
<li>七月在线 深度学习</li>
</ol>

<hr />

<h1 id="缘由">缘由：</h1>

<p>对经典的CNN网络进行分析总结，<strong>要知道它厉害在哪里？怎么想到要这样做的？有什么理论依据？根据这些网络有什么启发或者还有什么可以创新的？</strong></p>

<p><strong>最好能结合它的代码进行分析，并且，比如可以使用那些网络进行Transfer learning或者有什么新的开创新的东西出现，要迭代进来。</strong></p>

<p><strong>需要拆分的时候，把它拆分开来单独看一下。</strong></p>

<p>#</p>

<h1 id="卷积神经网络典型cnn">卷积神经网络典型CNN</h1>

<ul>
<li><p>LeNet，这是最早用于数字识别的CNN ，沉寂和很长时间。</p></li>

<li><p>AlexNet，2012 ILSVRC比赛远超第2名的CNN，比LeNet更深，用多层小卷积层叠加替换单大卷积层。** 什么是小卷积层？**</p></li>

<li><p>ZF Net，2013 ILSVRC比赛冠军</p></li>

<li><p>GoogLeNet，2014 ILSVRC比赛冠军</p></li>

<li><p>VGGNet，2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像迁移学习问题(比如object detection)上效果很好。到VGG为止，都是层叠的模型。</p></li>

<li><p>ResNet，2015ILSVRC比赛冠军 152层，微软的，深度残差网络  Residual network  ，结构修正(残差学习)以适应深层次CNN训练。它造了一些高速通道，直接把前面的数据拉到后面，让后面的层次看到，它把传统的连乘的形式变成了加法，而连乘的形式复合函数会出现的那种梯度弥散的现象，在Residual network中被减缓了。<strong>想更多的了解下，而且这两年的新的是什么样的？</strong></p></li>
</ul>

<p><strong>这些模型都要仔细看下。</strong></p>

<p>有个问题要注意一下，之所以会有这么利害的模型，这么利害的参数，很大程度上是做实验得到的。比如FC的个数等，并不是有一个公式来计算出是多少。</p>

<h1 id="lenet">Lenet</h1>

<p>Inner Product 就是点积  即全连接。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/4JfEBm0mbc.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/0jdbg8DBaA.png?imageslim" alt="mark" /></p>

<p>还是很经典的</p>

<h1 id="alexnet">AlexNet</h1>

<p>2012 Imagenet 比赛第一，Top5准确度超出第二10%。Top5准确度指的是，你给我5个结果，只要这5个结果有一个中了，我就认为你对了。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/gb00DDI0mD.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/IKeLaJ4bHa.png?imageslim" alt="mark" /></p>

<p>从上面看出：</p>

<ul>
<li><p>pooling的时候的确也是用窗口的。</p></li>

<li><p>而且为了防止信息丢失，他后面的CONV都是stride设为1。</p></li>

<li><p>两个全连接层的目的是为了还原一部分信息，因为前面的池化会丢失一部分信息。</p></li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/e2hE4fkj5j.png?imageslim" alt="mark" /></p>

<h1 id="zfnet">ZFNet</h1>

<p>相当于AlexNet并没有结构上的大变化。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/JCfaD3AF3L.png?imageslim" alt="mark" /></p>

<h1 id="vgg">VGG</h1>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/gLAA86Ii4f.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/immc7hHGkG.png?imageslim" alt="mark" /></p>

<p>VGG是很厉害的，有很多种 VGG-16，VGG-19    <strong>16，19指的是CONV的层数吗？是CONV和FC的层数。input 和maxpool和soft-max不算。</strong></p>

<ul>
<li><p>在那个时候还没有 batch normalization 这样的神器出现，这个batch normalization是google提出的可以帮助训练，保持训练稳定性。<strong>什么是batch normalization？ 为什么提了下说8层的网络用batch normalization还是比较好训练的？</strong></p></li>

<li><p>也没有 ResNet 这样的High-V 出现。</p></li>
</ul>

<p>在这样的情况下训练出了19层，是很难训练的。因为层数越多，复合函数求导越有可能是0，即梯度饱和。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/gHK51IB9d8.png?imageslim" alt="mark" /></p>

<p>上面的这个告诉你每一张图片需要消耗多大的显存，这是一个很重要的问题，比如你跑模型的时候batch太多，经常会报memory error。 <strong>这个地方要着重再看下。</strong></p>

<p>现在VGG在工业界用的很多。有的就是用的VGG，拿过来做一些神经网络的优调，而不是从头开始搭。</p>

<h1 id="googlenet">GoogLeNet</h1>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/D1FD775i3B.png?imageslim" alt="mark" /></p>

<p>它是分了不同的阶段去训练的，</p>

<p>这个网络的参数量非常非常小，相对于他的深度而言，是500万个</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/cJe8HEfkij.png?imageslim" alt="mark" /></p>

<p>他利害的地方在于：他把所有的全连接层全部换成了小卷积，比如说1*1的小卷积。<strong>这个是什么样的卷积？W里面只有1个参数？要自己看下。</strong></p>

<h1 id="resnet-即-deep-residual-learning-network-非常深的残差网络">ResNet   即 Deep Residual Learning network 非常深的残差网络</h1>

<p>微软亚洲研究院提出 ， ILSVRC 2015 冠军，比 VGG 还要深 8 倍，但是比VGG要好训练</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1Eejb11Lg0.png?imageslim" alt="mark" /></p>

<p>中间是一个plain model ，平铺的model，硬搭出来的。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/53Dd1GKJ7l.png?imageslim" alt="mark" /></p>

<p><strong>理解的不是很透彻。再看下相关的。</strong></p>

<p>这个地方提一下，梯度往前传，我一直觉得梯度往前传其实不是很形象，尤其是这个地方的，说梯度有两个传的路径。实际上还不如直接说梯度是怎么计算的，然后导致了前面的w怎么被就改的。这样更加情绪点。</p>

<p>##</p>

<h1 id="comment">COMMENT：</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/06-%E8%81%9A%E7%B1%BB/%E8%81%9A%E7%B1%BB/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">聚类</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/03-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E8%AF%84%E4%BC%B0/">
            <span class="next-text nav-default">特征重要性评估</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
