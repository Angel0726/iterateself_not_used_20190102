<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>DL 卷积神经网络基础 - iterate self</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="ORIGINAL 《解析卷积神经网络》魏秀参 TODO aaa INTRODUCTION aaa 卷积神经网络基础 卷积神经网络（Con 神经网络，区别于神 最主要的特点是卷积 域应用特别是图像相 图像语义分割 物" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E6%9E%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="DL 卷积神经网络基础" />
<meta property="og:description" content="ORIGINAL 《解析卷积神经网络》魏秀参 TODO aaa INTRODUCTION aaa 卷积神经网络基础 卷积神经网络（Con 神经网络，区别于神 最主要的特点是卷积 域应用特别是图像相 图像语义分割 物" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E6%9E%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" /><meta property="article:published_time" content="2018-06-12T22:19:09&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-12T22:19:09&#43;00:00"/>
<meta itemprop="name" content="DL 卷积神经网络基础">
<meta itemprop="description" content="ORIGINAL 《解析卷积神经网络》魏秀参 TODO aaa INTRODUCTION aaa 卷积神经网络基础 卷积神经网络（Con 神经网络，区别于神 最主要的特点是卷积 域应用特别是图像相 图像语义分割 物">


<meta itemprop="datePublished" content="2018-06-12T22:19:09&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-12T22:19:09&#43;00:00" />
<meta itemprop="wordCount" content="5278">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="DL 卷积神经网络基础"/>
<meta name="twitter:description" content="ORIGINAL 《解析卷积神经网络》魏秀参 TODO aaa INTRODUCTION aaa 卷积神经网络基础 卷积神经网络（Con 神经网络，区别于神 最主要的特点是卷积 域应用特别是图像相 图像语义分割 物"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">iterate self</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">about</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">iterate self</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">about</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">DL 卷积神经网络基础</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-12 </span>
        
        <span class="more-meta"> 5278 words </span>
        <span class="more-meta"> 11 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#original">ORIGINAL</a></li>
<li><a href="#todo">TODO</a></li>
<li><a href="#introduction">INTRODUCTION</a></li>
<li><a href="#comment">COMMENT</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="original">ORIGINAL</h1>

<ol>
<li>《解析卷积神经网络》魏秀参</li>
</ol>

<h1 id="todo">TODO</h1>

<ul>
<li>aaa</li>
</ul>

<hr />

<h1 id="introduction">INTRODUCTION</h1>

<ul>
<li>aaa</li>
</ul>

<p>卷积神经网络基础</p>

<p>卷积神经网络（Con 神经网络，区别于神 最主要的特点是卷积 域应用特别是图像相</p>

<p>图像语义分割</p>

<p>物体检测（ot 入，如自然语 数据挖掘（so 网络解决，并</p>

<p>汾类（image classification）、 3像检索（image retrieval）、 ?计算机视觉问题。此外，随着CNN研究的深 language processing）中的文本分类，软件工程 的软件缺陷预测等问题都在尝试利用卷积神经 法甚至其他深度网络模型更优的预测效果。</p>

<p>本章首先回顾卷积神经网络发展历程，接着从抽象层面介绍卷积神经网络的</p>

<p>基本结构，以及卷积神经网络中的两类基本过程：前馈运算（预测和推理）和</p>

<p>反馈运算（训练和学习）。</p>

<p>1.1发展历程
卷积神经网络发展历史中的第一件里程碑事件发生在上世纪60年代左右的 神经科学(neuroscience)中，加拿大神经科学家David H. Hubei和Torsten Wiesel于1959年提出猫的初级视皮层中单个神经元的“感受野” (receptive field)概念，紧接着于1962年发现了猫的视觉中枢里存在感受野、双目视觉和 其他功能结构，标志着神经网络结构首次在大脑视觉系统中被发现。1</p>

<p>图1.1: Torsten Wiesel (左)和David H. Hubei (右)。因其在视觉系统中信息 处理方面的杰出贡献，两人于1981年获得诺贝尔生理学或医学奖。</p>

<p>1980年前后，日本科学家福岛邦彦(Kunihiko Fukushima)在Hubei和 Wiesel工作的基础上，模拟生物视觉系统并提出了一种层级化的多层人工神经 网络，即“神经认知” (neurocognitron) [19],以处理手写字符识别和其他模式 识别任务。神经认知模型在后来也被认为是现今卷积神经网络的前身。在福岛 邦彦的神经认知模型中，两种最重要的组成单元是“S型细胞”(S-cells)和“C 型细胞”(C-cells),两类细胞交替堆叠在一起构成了神经认知网络。其中，S 型细胞用于抽取局部特征(local features)，C型细胞则用于抽象和容错，如图 1+2所示，不难发现这与现今卷积神经网络中的卷积层(convolution layer)和汇 合层(pooling layer)可一一对应。</p>

<p>随后，Yann LeCun等人在1998年提出基于梯度学习的卷积神经网络算</p>

<p>法[54],并将其成功用于手写数字字符识别，在那时的技术条件下就能取得低</p>

<p>1.1.发展历程</p>

<p>Us2^-</p>

<p>Uc3</p>

<p>k，1</p>

<p>图1.2: 1980年福岛邦彦提出</p>

<p>于1%的错误率。因此，LeNet这一卷积神 有的邮政系统，用来识别手写邮政编码进「 是第一个产生实际商业价值的卷积神经网络 展奠定了坚实的基础。鉴于此，Google在: 意将“L”大写，从而向“前辈” LeNet致</p>

<p>w</p>

<p>STet</p>

<p>J发 遊</p>

<p>图 1.3: LeNet-5 结构 “矩形”代表一张特 layer）。</p>

<p>时间来到2012 年 类竞赛四周年之际，</p>

<p>积神经网络。其中，每一个 !全连接层（fully connected</p>

<p>戸，在有计算机视觉界“世界杯”之称的ImageNet图像分</p>

<p>Geoffrey E. Hinton等人凭借卷积神经网络Alex-Net力挫 园牛津大学VGG组等劲旅，且以超过第二名近12%的准 B冠军［52］，霎时间学界业界纷纷惊愕哗然。自此便揭开了 ［机视觉领域逐渐称霸的序幕2，此后每年ImageNet竞赛的</p>

<p>隹生的2012年为计算机视觉领域中的深度学习元年。同时也有人将Hinton</p>

<p>冠军非深度卷积神经网络莫属。直到2015年，在改进了卷积神经网络中的激 活函数(activation function)后，卷积神经网络在ImageNet数据集上的性能 (4.94%)第一次超过了人类预测错误率(5.1%) [34]。近年来，随着神经网络特 别是卷积神经网络相关领域研究人员的增多、技术的日新月异，卷积神经网络 也变得愈宽愈深愈加复杂，从最初的5层、16层，到诸如MSRA提出的152 层Residual Net [36]甚至上千层网络已被广大研究者和工程实践人员司空见惯。</p>

<p>不过有趣的是，图1.4a为Alex-Net网络结构，可以发现在基本结构方面它 与十几年前的LeNet几乎毫无差异。但数十载间，数据和硬件设备(尤其是 GPU)的发展确实翻天覆地，它们实际上才是进一步助力神经网络领域革新的 主引擎。正是如此，才使得深度神经网络不再是“晚会的戏法”和象牙塔里的 研究，真正变成了切实落地可行的工具和应用手段。深度卷积神经网络自2012 年的一炮而红，到现在俨然已成为目前人工智能领域一个举足轻重的研究课题， 甚至可以说深度学习是诸如计算机视觉、自然语言处理等领域主宰性的研究技 术，同时更是工业界各大公司和创业机构着力发展力求占先的技术奇点。</p>

<p>1.2基本结构
总体来说，卷积神经网络是一种层次模型(hierarchical model)，其输人是原 始数据(raw data),如RGB图像、原始音频数据等。卷积神经网络通过卷 积(convolution)操作、汇合(pooling)操作和非线性激活函数(non-linear activation function)映射等一系列操作的层层堆叠，将高层语义信息逐层由原 始数据输人层中抽取出来，逐层抽象，这一过程便是“前馈运算” (feed-forward)。 其中，不同类型操作在卷积神经网络中一般称作“层”：卷积操作对应“卷积 层”，汇合操作对应“汇合层”等等。最终，卷积神经网络的最后一层将其目标 任务(分类、回归等)形式化为目标函数(objective function) 2 3。通过计算预 测值与真实值之间的误差或损失(loss),凭借反向传播算法(back-propagation algorithm [72])将误差或损失由最后一层逐层向前反馈(back-forward)，更新</p>

<p>1.2.基本结构</p>

<p>3 48</p>

<p>(a) AlexNet 结构［52&rdquo;</p>

<p>图1.4: Alex-Net网络结构和Geoffrey E. Hinton。值得一提的，Hinton因其杰 出的研究成就，获得2016年度电气和电子工程师协会（IEEE）和爱丁堡皇家 科学会 DRoyal Society of Edinburgh）联合颁发的 James Clerk Maxwell 奖，以 表彰其在深度学习方面的突出贡献。</p>

<p>每层参数，并在更新 到模型训练的目的。</p>

<p>更通俗地讲，卷积 层作为“基本单元” 砌”，以损失函数的计 一个三维张量（tens：</p>

<p>A后再次前馈，如此往复，直到网络模型收敛，从而达</p>

<p>X只神经网络犹如搭积木的过程（如公式1.1）,将卷积等操作 ”依次“搭”在原始数据（公式1.1中的X1）上，逐层“堆 f计算（公式1.1中的Z）作为过程结束，其中每层数据形式是 Lsor）。具体地D在计算机视觉应用中，卷积神经网络的数据 包空间的图像：H行，W）列，3个通道（分别为R, G, B）, 经过第一层操作可得x2,对应第一层操作中的参数记为^1; 层W的输人，可得x3……直到第L- 1层，此时网络输出</p>

<p>程中,理论上每层操5作层可为单独卷积操作、汇合操作、非</p>

<p>线性映射或其他操作/变換，当然也可以是不同形式操作/变換的组-</p>

<p>最后，整个网络以损失函 (ground truth),则损失函数表</p>

<p>应的真实标记</p>

<p>其中，函数L( + )中的参数即 其参数W是可以为空的，如&rdquo; 数的计算等。实际应用中，对 回归问题为例，常用的12损:</p>

<p>z = Lregression(xL，y) = 2 ||尤【-</p>

<p>交叉墒(cross entropy)损失函$ 其中 Pi = EjESfe (i = 1-归问题还是分类问题，在计算 xL，方可正确计算样本预测的 本书第2.7节。</p>

<p>(xL,y), (1+2)</p>

<p>实上，可以发现对于层中的特定操作，</p>

<p>作、无参的非线性映射以及无参损失函</p>

<p>任务，损失函数的形式也随之改变。以</p>

<p>即可作为卷积网络的目标函数，此时有</p>

<p>若对于分类问题，网络的目标函数常采用</p>

<p>=(classification (x^, y) = — ^2 i yi log(pi)，</p>

<p>)， C 为分类任务类别数。显然，无论回 需要通过合适的操作得到与 y 同维度的 差值。有关不同损失函数的对比请参见</p>

<p>练模型时计算误差还是模型训练完毕后获得样本预测，卷积神经网络的 :feed-forward)运算都较直观。同样以图像分类任务为例，假设网络已训 5,即其中参数^1,&hellip;,^L-1已收敛到某最优解，此时可用此网络进行图 预测。预测过程实际就是一次网络的前馈运算：将测试集图像作为网络 :1送进网络，之后经过第一层操作w1可得x2,依此下去……直至输出 RC。上一节提到，xL是与真实标记同维度的向量。在利用交叉墒损失函 ;后得到的网络中，xL的每一维可表示x1分别隶属C个类别的后验概</p>

<p>1.4.反馈运算</p>

<p>率。如此，可通过下式得到输人图像x1对应的预测标记:</p>

<p>arg max xL . i</p>

<p>1.4反馈运算
同其他许多机器学习模型(支持向量机等)一样,卷积神经网络包括其他所有</p>

<p>深度学习模型都依赖最小化损失函数来学习模型参数，即最小化式1.2中的z。 不过需指出的是，从凸优化理论来看，神经网络模型不仅是非凸(non-convex) 函数而且异常复杂,这便带来优化求解的困难。该情形下,深度学习模型采用 随机梯度下降法(Stochastic Gradient Descent,简记为SGD)和误差反向传播 (error back propogation)进行模型参数更新。有关随机梯度下降法详细内容可 参见附录B。</p>

<p>具体来讲,在卷积神经网络求解时,特别是针对大规模应用问题(如, ILSVRC分类或检测任务)，常采用批处理的随机梯度下降法(mini-batch SGD)。批处理的随机梯度下降法在训练模型阶段随机选取n个样本作为一批 (batch)样本，先通过前馈运算得到预测并计算其误差，后通过梯度下降法更 新参数,梯度从后往前逐层反馈,直至更新到网络的第一层参数,这样的一个 参数更新过程称为一个“批处理过程”(mini-batch)。不同批处理之间按照无放 回抽样遍历所有训练集样本，遍历一次训练样本称为“一轮”(epoch4)。其中, 批处理样本的大小(batch size)不宜设置过小。过小时(如batch size为1，2 等),由于样本采样随机,按照该样本上的误差更新模型参数不一定在全局上最 优(此时仅为局部最优更新),会使得训练过程产生振荡。而批处理大小的上限 则主要取决于硬件资源的限制，如GPU显存大小。一般而言，批处理大小设 为32, 64, 128或256即可。当然在随机梯度下降更新参数时，还有不同的参 数更新策略，具体可参见第11章有关内容。</p>

<p>下面我们来看误差反向传播的详细过程。按照第1.2节的记号，假设某批处</p>

<p>理前馈后得到n个样本上的误差为z，且最后一层L dz</p>

<p>d^L =0 ,</p>

<p>不难发现，实际上每层操作都</p>

<p>数的导数dZi，另一部分是误差</p>

<p>d</p>

<p>关于第 i 层参</p>

<p>关于参数w的导数</p>

<p>dz</p>

<p>dw</p>

<p>n是每次随机梯度下降的 细内容请参见第11.2.2节</p>

<p>关于输入 x4 的导数 误差从最后一层传递</p>

<p>dz</p>

<p>dx4</p>

<p>下面以 第 i +1 值。根据</p>

<p>^ - nd^i, (1-6)</p>

<p>-般随训练轮数(epoch)的增多减小，详</p>

<p>差向前层的反向传播。可将其视作最终</p>

<p>差信号。</p>

<p>更新信号(导数)反向传播至第 i 层时，</p>

<p>昏参数更新时需计算d^和dX^的对应</p>

<p>d vec(x4+4) i+4)T) d(ve«), z d vec(x4+4)</p>

<p>(1+7)</p>

<p>(1+8)</p>

<dl>
<dt>d (vec(x4)T) d (vec(x4+4)T) d (vec(x4)T)</dt>
</dl>

<p>:用向量标记“vec”是由于实际工程实现时张量运算均转化为向量运算。 量运算和求导可参见附录A。前面提到，由于第i + 1层时已计算得到 在第 i 层用于更新该层参数时仅需对其做向量化和转置操作即可得到 I+1)T)，即公式1.7和1.8中等号右端的左项。另一方面，在第i层，由于 直接作用得x4+4,故反向求导时亦可直接得到其偏导数和</p>

<p>1.5.小结</p>

<p>算法1反向传播算法</p>

<p>输人：训练集(N个训练样本及对应标记)(xn, yn), n =1,&hellip;,N;训练轮数 (epoch): T</p>

<p>输出：^ , i = 1,…，L l： for t =1 &hellip;T do 2: while训练集数据未遍历完全do</p>

<p>3： 前馈运算得到每层V，并计算最终误差Z;</p>

<p>4： for i = L&hellip; 1 do</p>

<p>5: (a)用公式1 + 7反向计算第i层误差对该层参数的导数：d(vecd：i)&lsquo;);</p>

<p>6： (b)用公式1 + 8反向计算第i层误差对该层输人数据的导数:</p>

<p><em>dz</em>.</p>

<p>d(vec(xi)T) 7</p>

<p>7:</p>

<p>8:</p>

<p>9:</p>

<p>&copy;用公式1.6更新参数：</p>

<p>end for</p>

<p>end while</p>

<p>10： end for</p>

<p>ll： return ^i.</p>

<p>当然,上述方法是通过手动书写导数并用链式法则计算最终误差对每层不 同参数的梯度,之后仍需通过代码将其实现。可见这一过程不仅繁琐,且容易 出错,特别是对一些复杂操作,其导数很难求得甚至无法显式写出。针对这种 情况，一些深度学习库，如Theano和Tensorflow都采用了符号微分的方法进 行自动求导来训练模型。符号微分可以在编译时就计算导数的数学表示,并进 一步利用符号计算方式进行优化。在实际应用时,用户只需把精力放在模型构 建和前向代码书写上,不用担心复杂的梯度求导过程。不过,在此需指出的是, 读者有必要对上述反向梯度传播过程了解,也要有能力求得正确的导数形式。</p>

<p>本章回顾了卷积神经网络自1959年至今的发展历程;</p>

<p>介绍了卷积神经网络的基本结构,可将其理解 作层的“堆叠”将原始数据表示(raw data rep 人为干预直接映射为高层语义表示(high-level s 并实现向任务目标映射的过程——这也是为何 端” (end-to-end)学习或作为“表示学习” (repr 最重要代表的原因；</p>

<p>learning）中</p>

<p>§ 介绍了卷积神经网络中的两类基本过程：前馈运算和反馈运算。神经网络 模型通过前馈运算对样本进行推理(inference)和预测(prediction),通 过反馈运算将预测误差反向传播逐层更新参数,如此两种运算依次交替迭 代完成模型的训练过程。</p>

<hr />

<h1 id="comment">COMMENT</h1>

    </div>

    
    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">iterateself</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">2018-06-12</span>
  </p>
  
  
</div>

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E6%9E%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E9%83%A8%E4%BB%B6/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">DL 卷积神经网络基本部件</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E6%9E%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%8F%E5%85%B8%E7%BB%93%E6%9E%84/">
            <span class="next-text nav-default">DL 卷积神经网络经典结构</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
