<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>19 近似推断 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="ORIGINAL 《深度学习》Ian Goodfellow TODO aaa INTRODUCTION aaa 第十九章 近似推断 许多概率模型很难训练的原因是很难进行推断。在深度学习中，通常我们有一 系列可见变量和一系列潜变量" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/19-%E8%BF%91%E4%BC%BC%E6%8E%A8%E6%96%AD/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="19 近似推断" />
<meta property="og:description" content="ORIGINAL 《深度学习》Ian Goodfellow TODO aaa INTRODUCTION aaa 第十九章 近似推断 许多概率模型很难训练的原因是很难进行推断。在深度学习中，通常我们有一 系列可见变量和一系列潜变量" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/19-%E8%BF%91%E4%BC%BC%E6%8E%A8%E6%96%AD/" /><meta property="article:published_time" content="2018-06-12T22:18:54&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-12T22:18:54&#43;00:00"/>
<meta itemprop="name" content="19 近似推断">
<meta itemprop="description" content="ORIGINAL 《深度学习》Ian Goodfellow TODO aaa INTRODUCTION aaa 第十九章 近似推断 许多概率模型很难训练的原因是很难进行推断。在深度学习中，通常我们有一 系列可见变量和一系列潜变量">


<meta itemprop="datePublished" content="2018-06-12T22:18:54&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-12T22:18:54&#43;00:00" />
<meta itemprop="wordCount" content="6301">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="19 近似推断"/>
<meta name="twitter:description" content="ORIGINAL 《深度学习》Ian Goodfellow TODO aaa INTRODUCTION aaa 第十九章 近似推断 许多概率模型很难训练的原因是很难进行推断。在深度学习中，通常我们有一 系列可见变量和一系列潜变量"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">iterate self</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">about</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">iterate self</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">about</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">19 近似推断</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-12 </span>
        
        <span class="more-meta"> 6301 words </span>
        <span class="more-meta"> 13 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#original">ORIGINAL</a></li>
<li><a href="#todo">TODO</a></li>
<li><a href="#introduction">INTRODUCTION</a></li>
<li><a href="#comment">COMMENT</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="original">ORIGINAL</h1>

<ol>
<li>《深度学习》Ian Goodfellow</li>
</ol>

<h1 id="todo">TODO</h1>

<ul>
<li>aaa</li>
</ul>

<hr />

<h1 id="introduction">INTRODUCTION</h1>

<ul>
<li>aaa</li>
</ul>

<p>第十九章 近似推断
许多概率模型很难训练的原因是很难进行推断。在深度学习中，通常我们有一</p>

<p>系列可见变量和一系列潜变量^。推断困难通常是指难以计算p(h 14或其期望。</p>

<p>而这样的操作在一些诸如最大似然学习的任务中往往是必需的。</p>

<p>许多仅含一个隐藏层的简单图模型会定义成易于计算 p(h | v) 或其期望的形式</p>

<p>例如受限玻尔兹曼机和概率PCA。不幸的是，大多数具有多层隐藏变量的图模型的</p>

<p>后验分布都很难处理。对于这些模型而言，精确推断算法需要指数量级的运行时间。</p>

<p>即使一些只有单层的模型，如稀疏编码，也存在着这样的问题。</p>

<p>在本章中，我们将会介绍几个用来解决这些难以处理的推断问题的技巧。稍后</p>

<p>在第二十章中，我们还将描述如何将这些技巧应用到训练其他方法难以奏效的概率</p>

<p>模型中，如深度信念网络、深度玻尔兹曼机。</p>

<p>在深度学习中难以处理的推断问题通常源于结构化图模型中潜变量之间的相互 作用。读者可以参考图19.1的几个例子。这些相互作用可能是无向模型的直接相互 作用，也可能是有向模型中同一个可见变量的共同祖先之间的 “相消解释&rdquo; 作用。</p>

<p>图 19.1: 深度学习中难以处理的推断问题通常是由于结构化图模型中潜变量的相互作用。这些</p>

<p>相互作用产生于一个潜变量与另一个潜变量或者当V-结构的子节点可观察时与更长的激活路径 相连。(左)一个隐藏单元存在连接的半受限玻尔兹曼机(semi-restricted Boltzmann Machine) (Osindero and Hinton, 2008)。由于存在大量潜变量的团，潜变量的直接连接使得后验分布难以处</p>

<p>理。 (中) 一个深度玻尔兹曼机，被分层从而使得不存在层内连接，由于层之间的连接其后验分布仍</p>

<p>然难以处理。 (右) 当可见变量可观察时这个有向模型的潜变量之间存在相互作用，因为每两个潜</p>

<p>变量都是共父。即使拥有上图中的某一种结构，一些概率模型依然能够获得易于处理的关于潜变</p>

<p>量的后验分布。如果我们选择条件概率分布来引入相对于图结构描述的额外的独立性这种情况也</p>

<p>是可能出现的。举个例子，概率PCA的图结构如右图所示，然而由于其条件分布的特殊性质(带</p>

<p>有相互正交基向量的线性高斯条件分布)依然能够进行简单的推断。</p>

<p>19.1 把推断视作优化问题
精确推断问题可以描述为一个优化问题，有许多方法正是由此解决了推断的困</p>

<p>难。通过近似这样一个潜在的优化问题，我们往往可以推导出近似推断算法。</p>

<p>为了构造这样一个优化问题，假设我们有一个包含可见变量W和潜变量的概 率模型。我们希望计算观察数据的对数概率logp(T;0)。有时候如果边缘化消去h的 操作很费时，我们会难以计算logp(% 0)。作为替代，我们可以计算一个logp(W0) 的下界L(v, 0,q)。这个下界被称为证据下界(evidence lower bound, ELBO )。这个 下界的另一个常用名称是负变分自由能(variational free energy )。具体地，这个证 据下界是这样定义的：</p>

<p>L(v,0,q)=logP(v;0)-DKL(q(h|v)lP(h|v;0)),    (19.1)</p>

<p>其中 q 是关于 h 的一个任意概率分布。</p>

<p>因为logp(v)和L(v, 0,q)之间的距离是由KL散度来衡量的，且KL散度总是</p>

<p>非负的，我们可以发现L总是小于等于所求的对数概率。当且仅当分布q完全相等 于 P(h | v) 时取到等号。</p>

<p>令人吃惊的是，对于某些分布q，计算L可以变得相当简单。通过简单的代数 运算我们可以把L重写成一个更加简单的形式：</p>

<p>L(v,0,q) =logP(v;0)</p>

<p>DKL(q(h | v)| P(h | v; 0))</p>

<p>(19.2)</p>

<p>=logP(v;0)</p>

<p>log q(h | v)</p>

<p>Uogp(h| v)</p>

<p>(19.3)</p>

<p>=logP(v;0)</p>

<p>log q(h | v)</p>

<p>Eh-q log p(h，v;0)</p>

<p>(19.4)</p>

<p>p(v;0)</p>

<p>=logP(v;0)</p>

<p>-Eh^q[log q(h | v) - logp(h, v; 0) + logp(v; 0)]</p>

<p>(19.5)</p>

<p>=-Eh^q[log q(h | v) - logp(h, v; 0)].</p>

<p>(19.6)</p>

<p>这也给出了证据下界的标准定义：</p>

<p>L(v, 0,q) = Eh^q[logp(h, v)] + H(q).    (19.7)</p>

<p>对于一个选择的合适分布 q 来说， L 是容易计算的。对任意分布 q 的选择来说 L提供了似然函数的一个下界。越好地近似p(h | v)的分布q(h | v)，得到的下界就 越紧，换言之，就是与 log P(v) 更加接近。当 q(h | v) = P(h | v) 时，这个近似是完 美的，也意味着 L(v, 0, q) = log p(v;0)。</p>

<p>因此我们可以将推断问题看作是找一个分布q使得L最大的过程。精确推断能 够在包含分布P(h| v)的函数族中搜索一个函数，完美地最大化L。在本章中，我们 将会讲到如何通过近似优化寻找分布 q 的方法来推导出不同形式的近似推断。我们 可以通过限定分布 q 的形式或者使用并不彻底的优化方法来使得优化的过程更加高 效(却更粗略)，但是优化的结果是不完美的，不求彻底地最大化L，而只要显著地 提升 L。</p>

<p>无论我们选择什么样的分布 q， L 始终是一个下界。我们可以通过选择一个更简 单或更复杂的计算过程来得到对应的更松或更紧的下界。通过一个不彻底的优化过 程或者将分布 q 做很强的限定(并且使用一个彻底的优化过程)我们可以获得一个 很差的分布q，但是降低了计算开销。</p>

<p>19.2 期望最大化
我们介绍的第一个最大化下界L的算法是期望最大化(expectation maximization, EM)算法。在潜变量模型中，这是一个非常常见的训练算法。在这里我们描 述 Neal and Hinton (1999) 所提出的 EM 算法。与大多数我们在本章中介绍的其他 算法不同的是， EM 并不是一个近似推断算法，而是一种能够学到近似后验的算法。</p>

<p>EM算法由交替迭代，直到收敛的两步运算组成：</p>

<p>•    E步(expectation step ):令0(0)表示在这一步开始时的参数值。对任何我们 想要训练的(对所有的或者小批量数据均成立)索引为i的训练样本i；(i)，令 q(h(i) I V) = p(h(i) I v(i)； 0(。))。通过这个定义，我们认为q在当前参数0(。)下 定义。如果我们改变0，那么p(h | v;0)将会相应地变化，但是q(h | V)还是 不变并且等于p(h| v;0(0))。</p>

<p>•    M步(maximization step ):使用选择的优化算法完全地或者部分地关于0最 大化</p>

<p>L(V(i),0,q).    (19.8)</p>

<p>i</p>

<p>这可以被看作通过坐标上升算法来最大化L。在第一步中，我们更新分布q来 最大化L，而在另一步中，我们更新0来最大化L。</p>

<p>基于潜变量模型的随机梯度上升可以被看作是一个 EM 算法的特例，其中 M 步包括了单次梯度操作。 EM 算法的其他变种可以实现多次梯度操作。对一些模型 族来说，M步甚至可以直接推出解析解，不同于其他方法，在给定当前q的情况下 直接求出最优解。</p>

<p>尽管E步采用的是精确推断，我们仍然可以将EM算法视作是某种程度上的近 似推断。具体地说，M步假设一个分布q可以被所有的0值分享。当M步越来越 远离 E 步中的 0(。) 时，这将会导致 L 和真实的 logp(V) 之间出现差距。幸运的是 在进入下一个循环时， E 步把这种差距又降到了 0。</p>

<p>EM 算法还包含一些不同的见解。首先，它包含了学习过程的一个基本框架，就 是我们通过更新模型参数来提高整个数据集的似然，其中缺失变量的值是通过后验 分布来估计的。这种特定的性质并非 EM 算法独有的。例如，使用梯度下降来最大化 对数似然函数的方法也有相同的性质。计算对数似然函数的梯度需要对隐藏单元的</p>

<p>后验分布求期望。 EM 算法另一个关键的性质是当我们移动到另一个 0 时候，我们 仍然可以使用旧的分布q。在传统机器学习中，这种特有的性质在推导大M步更新 时候得到了广泛的应用。在深度学习中，大多数模型太过于复杂以致于在最优大M 步更新中很难得到一个简单的解。所以 EM 算法的第二个特质，更多为其所独有 较少被使用。</p>

<p>19.3 最大后验推断和稀疏编码
我们通常使用推断(inference)这个术语来指代给定一些其他变量的情况下计 算某些变量概率分布的过程。当训练带有潜变量的概率模型时，我们通常关注于计 算p(h | v)。另一种可选的推断形式是计算一个缺失变量的最可能值来代替在所有可 能值的完整分布上的推断。在潜变量模型中，这意味着计算</p>

<p>h, = arg max P(h | v).    (19.9)</p>

<p>h</p>

<p>这被称作最大后验(Maximum A Posteriori。推断，简称MAP推断。</p>

<p>MAP 推断并不被视作是一种近似推断，它只是精确地计算了最有可能的一个 然而，如果我们希望设计一个最大化L(v,h,q)的学习过程，那么把MAP推断</p>

<p>视作是输出一个q值的学习过程是很有帮助的。在这种情况下，我们可以将MAP推 断视作是近似推断，因为它并不能提供一个最优的 q。</p>

<p>我们回过头来看看第19.1节中所描述的精确推断，它指的是关于一个在无限制 的概率分布族中的分布 q 使用精确的优化算法来最大化</p>

<p>L(v, 0,q) = Eh^q[logp(h, v)] + H(q).    (19.10)</p>

<p>我们通过限定分布 q 属于某个分布族，能够使得 MAP 推断成为一种形式的近似推 断。具体地说，我们令分布q满足一个Dirac分布：</p>

<p>q(h | v) = S(h — ^).    (19.11)</p>

<p>这也意味着现在我们可以通过M来完全控制分布q。将L中不随M变化的项丢弃， 我们只需解决一个优化问题：</p>

<p>= arg max logp(h=&ldquo;, v),    (19.12)</p>

<p>这等价于 MAP 推断问题</p>

<p>h, = arg max P(h | v).    (19.13)</p>

<p>h
因此我们能够证明一种类似于 EM 算法的学习算法，其中我们轮流迭代两步 一步是用MAP推断估计出h<em>，另一步是更新0来增大logp(h</em>, v)。从EM算法角 度看，这也是对 L 的一种形式的坐标上升，交替迭代时通过推断来优化关于 q 的 L 以及通过参数更新来优化关于0的L。作为一个整体，这个算法的正确性可以得到 保证，因为 L 是 logP(v) 的下界。在 MAP 推断中，这个保证是无效的，因为 Dirac 分布的微分熵趋近于负无穷，使得这个界会无限地松。然而，人为加人一些M的噪 声会使得这个界又有了意义。</p>

<p>MAP 推断作为特征提取器以及一种学习机制被广泛地应用在了深度学习中。它</p>

<p>主要用于稀疏编码模型中。</p>

<p>我们回过头来看第13.4节中的稀疏编码，稀疏编码是一种在隐藏单元上加上了 诱导稀疏性的先验知识的线性因子模型。一个常用的选择是可分解的 Laplace 先验 表示为</p>

<p>P(hi) = | exp(-A|hi|).    (19.14)</p>

<p>可见的节点是由一个线性变化加上噪声生成的：</p>

<p>P(v | h) = N(v; Wh+b,卢-iI).    (19.15)</p>

<p>分布P(h | v)难以计算，甚至难以表达。每一对hi，hj变量都是v的母节点。 这也意味着当 v 可被观察时，图模型包含了一条连接 hi 和 hj 的活跃路径。因此 P(h| v) 中所有的隐藏单元都包含在了一个巨大的团中。如果是高斯模型，那么这些 相互作用关系可以通过协方差矩阵来高效地建模。然而稀疏型先验使得这些相互作 用关系并不服从高斯分布。</p>

<p>分布 P(x| h) 的难处理性导致了对数似然及其梯度也很难得到。因此我们不能 使用精确的最大似然估计来进行学习。取而代之的是，我们通过 MAP 推断以及最 大化由以h为中心的Dirac分布所定义而成的ELBO来学习模型参数。</p>

<p>如果我们将训练集中所有的向量h拼成矩阵H，并将所有的向量v拼起来组成 矩阵V，那么稀疏编码问题意味着最小化</p>

<p>J (H, W) =    |Hi,j | +    (V - ffWT)j    (19.16)</p>

<p>i,j i,j i,j</p>

<p>为了避免如极端小的 H 和极端大的 W 这样的病态的解，大多数稀疏编码的应用包</p>

<p>含了权重衰减或者对 H 列范数的限制。</p>

<p>我们可以通过交替迭代，分别关于H和研最小化J的方式来最小化J。且两</p>

<p>个子问题都是凸的。事实上，关于 W 的最小化问题就是一个线性回归问题。然而关</p>

<p>于这两个变量同时最小化 J 的问题通常并不是凸的。</p>

<p>关于 H 的最小化问题需要某些特别设计的算法，例如特征符号搜索方法 (Lee</p>

<p>et al., 2007)。</p>

<p>19.4 变分推断和变分学习
我们已经说明过了为什么证据下界L(v,0,q)是logp(v;0)的一个下界、如何将 推断看作是关于分布q最大化L的过程以及如何将学习看作是关于参数0最大化L 的过程。我们也讲到了 EM算法在给定了分布q的条件下能够进行大学习步骤，而 基于 MAP 推断的学习算法则是学习一个 p(h| V) 的点估计而非推断整个完整的分 布。在这里我们介绍一些变分学习中更加通用的算法。</p>

<p>变分学习的核心思想就是在一个关于q的有约束的分布族上最大化L。选择这 个分布族时应该考虑到计算 Eqlogp(h,V) 的难易度。一个典型的方法就是添加分布 q 如何分解的假设。</p>

<p>一种常用的变分学习的方法是加人一些限制使得q是一个因子分布：</p>

<p>q(h | V) =    q(hi | V).    (19.17)</p>

<p>i</p>

<p>这被称为均值场(mean-field)方法。更一般地说，我们可以通过选择分布q的 形式来选择任何图模型的结构，通过选择变量之间相互作用的多少来灵活地决定 近似程度的大小。这种完全通用的图模型方法被称为结构化变分推断(structured variational inference)(Saul and Jordan, 1996)。</p>

<p>变分方法的优点是我们不需要为分布 q 设定一个特定的参数化形式。我们设定 它如何分解，之后通过解决优化问题来找出在这些分解限制下最优的概率分布。对 离散型潜变量来说，这意味着我们使用传统的优化技巧来优化描述分布 q 的有限个 变量。对连续型潜变量来说，这意味着我们使用一个被称为变分法的数学分支工具 来解决函数空间上的优化问题。然后决定哪一个函数来表示分布q。变分法是“变分 学习&rdquo;或者‘‘变分推断&rdquo;这些名字的来因，尽管当潜变量是离散时变分法并没有用武 之地。当遇到连续型潜变量时，变分法不需要过多地人工选择模型，是一种很有用 的工具。我们只需要设定分布 q 如何分解，而不需要去猜测一个特定的能够精确近 似原后验分布的分布 q。</p>

<p>因为 L(v, 0, q)被定义成 logp(v;0) - DKL(q(h | v)||p(h | v;0))，我们可以认为 关于q最大化L的问题等价于(关于q)最小化DKL(q(h | v)|p(h | v))。在这种 情况下，我们要用q来拟合p。然而，与以前方法不同，我们使用KL散度的相 反方向来拟合一个近似。当我们使用最大似然估计来用模型拟合数据时，我们最小 化DKL(Pdata||Pmodel)。如图3.6所示，这意味着最大似然鼓励模型在每一个数据达 到高概率的地方达到高概率，而基于优化的推断则鼓励了 q 在每一个真实后验分 布概率低的地方概率较小。这两种基于 KL 散度的方法都有各自的优点与缺点。选 择哪一种方法取决于在具体每一个应用中哪一种性质更受偏好。在基于优化的推断 问题中，从计算角度考虑，我们选择使用DKL(q(h| v)||p(h| v))。具体地说，计算 DKL(q(h I v)|p(h | v))涉及到了计算分布q下的期望。所以通过将分布q设计得较 为简单，我们可以简化求所需要的期望的计算过程。KL散度的相反方向需要计算真 实后验分布下的期望。因为真实后验分布的形式是由模型的选择决定的，所以我们 不能设计出一种能够精确计算DKL(P(h I v)|q(h | v))的开销较小的方法。</p>

<p>19.4.1 离散型潜变量
关于离散型潜变量的变分推断相对来说比较直接。我们定义一个分布q，通常 分布 q 的每个因子都由一些离散状态的可查询表格定义。在最简单的情况中， h 是二值的并且我们做了均值场假定，分布q可以根据每一个hi分解。在这种情况 下，我们可以用一个向量来参数化分布q，h的每一个元素都代表一个概率，即 q(hi = 1 | v) = hi。</p>

<p>在确定了如何表示分布 q 以后，我们只需要优化它的参数。在离散型潜变量模 型中，这是一个标准的优化问题。基本上分布q的选择可以通过任何优化算法解决， 比如梯度下降算法。</p>

<p>因为它在许多学习算法的内循环中出现，所以这个优化问题必须可以很快求解。 为了追求速度，我们通常使用特殊设计的优化算法。这些算法通常能够在极少的循 环内解决一些小而简单的问题。一个常见的选择是使用不动点方程，换句话说，就 是解关于hi的方程</p>

<p>d</p>

<p>L = 0.    (19.18)</p>

<p>我们反复地更新h不同的元素直到满足收敛准则。</p>

<p>为了具体化这些描述，我们接下来会讲如何将变分推断应用到二值稀疏编码 (binary sparse coding)模型(这里我们所描述的模型是Henniges et al. (2010)提出 的，但是我们采用了传统、通用的均值场方法，而原文作者采用了一种特殊设计的 算法)中。数学推导过程非常详细，为希望完全了解我们描述过的变分推断和变分 学习高级概念描述的读者所准备。而对于并不计划推导或者实现变分学习算法的读 者来说，可以放心跳过，直接阅读下一节，这并不会遗漏新的高级概念。建议那些 从事二值稀疏编码研究的读者可以重新看一下第3.10节中描述的一些经常在概率模 型中出现的有用的函数性质。我们在推导过程中随意地使用了这些性质，并没有特 别强调它们。</p>

<p>在二值稀疏编码模型中，输人v e Rn，是由模型通过添加高斯噪声到m个或 有或无的不同成分的和而生成的。每一个成分可以是开或者关的，对应着隐藏单 元 he {0,l}m：</p>

<p>p(hi = 1) = ^(bi),</p>

<p>(19.19)</p>

<p>P(v | h) = N(v; Wh,卢-i),    (19.20)</p>

<p>其中b是一个可以学习的偏置集合，W是一个可以学习的权值矩阵，^是一个可以 学习的对角精度矩阵。</p>

<p>使用最大似然来训练这样一个模型需要对参数进行求导。我们考虑对其中一</p>

<p>个偏置进行求导的过程：</p>

<p>dbilog p(v)</p>

<p>_j；p(v)</p>

<p>p(v)</p>

<p>羞 hp(h, v)</p>

<p>p(v)</p>

<h1 id="comment">COMMENT</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E6%9E%90%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%8F%E5%85%B8%E7%BB%93%E6%9E%84/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">DL 卷积神经网络经典结构</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/03-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/15-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/">
            <span class="next-text nav-default">15 表示学习</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
