<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>梯度下降和拟牛顿 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="梯度下降和拟牛顿 相关资料： 七月在线 机器学习 缘由： 对梯度下降进行总结 (\nabla) 的符号是 nabla # 预备题目 已知二次函数的一个点函数值和导数值，以及另外一个点的" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/01-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E6%8B%9F%E7%89%9B%E9%A1%BF/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="梯度下降和拟牛顿" />
<meta property="og:description" content="梯度下降和拟牛顿 相关资料： 七月在线 机器学习 缘由： 对梯度下降进行总结 (\nabla) 的符号是 nabla # 预备题目 已知二次函数的一个点函数值和导数值，以及另外一个点的" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/01-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E6%8B%9F%E7%89%9B%E9%A1%BF/" /><meta property="article:published_time" content="2018-08-01T18:30:48&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-01T18:30:48&#43;00:00"/>
<meta itemprop="name" content="梯度下降和拟牛顿">
<meta itemprop="description" content="梯度下降和拟牛顿 相关资料： 七月在线 机器学习 缘由： 对梯度下降进行总结 (\nabla) 的符号是 nabla # 预备题目 已知二次函数的一个点函数值和导数值，以及另外一个点的">


<meta itemprop="datePublished" content="2018-08-01T18:30:48&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-01T18:30:48&#43;00:00" />
<meta itemprop="wordCount" content="3290">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="梯度下降和拟牛顿"/>
<meta name="twitter:description" content="梯度下降和拟牛顿 相关资料： 七月在线 机器学习 缘由： 对梯度下降进行总结 (\nabla) 的符号是 nabla # 预备题目 已知二次函数的一个点函数值和导数值，以及另外一个点的"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/recent/">
        <li class="mobile-menu-item">最新</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/recent/">最新</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">梯度下降和拟牛顿</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-01 </span>
        
        <span class="more-meta"> 3290 words </span>
        <span class="more-meta"> 7 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#梯度下降和拟牛顿">梯度下降和拟牛顿</a>
<ul>
<li><a href="#相关资料">相关资料：</a></li>
</ul></li>
<li><a href="#缘由">缘由：</a></li>
<li><a href="#预备题目">预备题目</a></li>
<li><a href="#从线性回归引起两个问题">从线性回归引起两个问题</a></li>
<li><a href="#进行梯度下降的学习率的实验">进行梯度下降的学习率的实验：</a>
<ul>
<li><a href="#实验1-固定学习率的梯度下降">实验1：固定学习率的梯度下降</a></li>
<li><a href="#实验2-固定学习率的梯度下降">实验2：固定学习率的梯度下降</a></li>
<li><a href="#固定学习率实验的代码">固定学习率实验的代码</a></li>
</ul></li>
<li><a href="#那么怎么调整学习率好呢">那么怎么调整学习率好呢？</a>
<ul>
<li>
<ul>
<li><a href="#梯度下降的运行过程阐述">梯度下降的运行过程阐述</a></li>
</ul></li>
<li><a href="#视角转换一下">视角转换一下</a></li>
<li><a href="#看作是关于-alpha-的函数之后-就可以求导了">看作是关于(\alpha )的函数之后，就可以求导了</a></li>
<li><a href="#分析这个学习率函数的导数">分析这个学习率函数的导数</a></li>
<li><a href="#使用线性搜索的方法-line-search">使用线性搜索的方法 Line Search</a>
<ul>
<li><a href="#二分线性搜索-bisection-line-search">二分线性搜索(Bisection Line Search)</a></li>
<li><a href="#回溯线性搜索-backing-line-search">回溯线性搜索(Backing Line Search)</a></li>
<li><a href="#回溯与二分线性搜索的异同">回溯与二分线性搜索的异同</a></li>
</ul></li>
</ul></li>
<li><a href="#使用回溯线性搜索">使用回溯线性搜索</a>
<ul>
<li><a href="#回溯线性搜索代码">回溯线性搜索代码</a></li>
<li><a href="#实验-回溯线性搜索寻找学习率">实验：回溯线性搜索寻找学习率</a></li>
<li><a href="#回溯线性搜索的思考-插值法">回溯线性搜索的思考：插值法</a>
<ul>
<li><a href="#二次插值法求极值">二次插值法求极值</a></li>
</ul></li>
</ul></li>
<li><a href="#二次插值法">二次插值法</a>
<ul>
<li><a href="#代码如下">代码如下：</a></li>
<li><a href="#实验-二次插值线性搜索寻找学习率">实验：二次插值线性搜索寻找学习率</a></li>
<li><a href="#总结与思考">总结与思考</a></li>
</ul></li>
<li><a href="#搜索方向">搜索方向</a>
<ul>
<li>
<ul>
<li><a href="#附-平方根算法">附：平方根算法</a></li>
<li><a href="#实验-搜索方向的探索">实验：搜索方向的探索</a></li>
<li><a href="#分析上述结果的原因">分析上述结果的原因</a></li>
<li><a href="#牛顿法">牛顿法</a></li>
<li><a href="#牛顿法的特点">牛顿法的特点</a></li>
<li><a href="#拟牛顿的思路">拟牛顿的思路</a></li>
<li><a href="#bfgs">BFGS</a></li>
<li><a href="#l-bfgs">L-BFGS</a></li>
</ul></li>
</ul></li>
<li><a href="#comment">COMMENT：</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="梯度下降和拟牛顿">梯度下降和拟牛顿</h1>

<h2 id="相关资料">相关资料：</h2>

<ol>
<li>七月在线 机器学习</li>
</ol>

<hr />

<h1 id="缘由">缘由：</h1>

<p>对梯度下降进行总结</p>

<p>(\nabla) 的符号是 nabla</p>

<p>#</p>

<h1 id="预备题目">预备题目</h1>

<p>已知二次函数的一个点函数值和导数值，以及另外一个点的函数值，如果确定该函数的
解析式？即：二次函数f(x)，已知(f(a))，(f’(a))，(f(b))，求f(x)</p>

<p>特殊的，若a=0，题目变成：对于二次函数f(x)，已知(f(0))，(f’(0))，(f(a))，求f(x)</p>

<p>实际上是可以求的，求出来是这样的：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/ICE7jag41K.png?imageslim" alt="mark" /></p>

<h1 id="从线性回归引起两个问题">从线性回归引起两个问题</h1>

<p>经过回归部分的学习，基本上对线性回归有认识了。那么下面就有几个问题了：</p>

<p>学习率的问题：</p>

<ul>
<li><p>学习率α如何确定？</p></li>

<li><p>使用固定学习率还是变化学习率</p></li>

<li><p>学习率设置多大比较好？</p></li>
</ul>

<p>下降方向的问题：</p>

<ul>
<li><p>处理梯度方向，其他方向是否可以？</p></li>

<li><p>如果可以的话，可行的方向和梯度方向有何关系？</p></li>
</ul>

<h1 id="进行梯度下降的学习率的实验">进行梯度下降的学习率的实验：</h1>

<h2 id="实验1-固定学习率的梯度下降">实验1：固定学习率的梯度下降</h2>

<p>(y=x^2) ，初值取x=1.5，学习率使用0.01</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/h1G2i3dCe4.png?imageslim" alt="mark" /></p>

<ul>
<li>效果还不错</li>
<li>经过200次迭代，x=0.0258543；</li>
<li>经过1000次迭代，x=2.52445×10^-9</li>
</ul>

<h2 id="实验2-固定学习率的梯度下降">实验2：固定学习率的梯度下降</h2>

<p>(y=x^4) ，初值取x=1.5，学习率使用0.01</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/heC63Kidga.png?imageslim" alt="mark" /></p>

<ul>
<li><p>分析：效果不理想</p></li>

<li><p>经过200次迭代，x=0.24436；</p></li>

<li><p>经过1000次迭代，x=0.111275</p></li>
</ul>

<p>为什么次数变高之后下降就不理想了呢？因为对于(y=x^4) 来说，降到一定程度的时候，虽然x还是很大，但是y的值已经很小了，比如x=0.1，但是y已经是0.0001 ，这个时候 y 很难往下降了。</p>

<p><strong>所以如果使用固定学习率做梯度下降的时候，有些时候很难下降了，达不到要求。</strong></p>

<h2 id="固定学习率实验的代码">固定学习率实验的代码</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/6ddJHDG7a9.png?imageslim" alt="mark" /></p>

<h1 id="那么怎么调整学习率好呢">那么怎么调整学习率好呢？</h1>

<p>分析“学习率α”在f(x)中的意义</p>

<p>调整学习率：</p>

<ul>
<li><p>在斜率(方向导数)大的地方，使用小学习率</p></li>

<li><p>在斜率(方向导数)小的地方，使用大学习率 比如在(y=x^4) 中，x比较小的时候，y已经很小了，这个时候能不能用更大的学习率，使它降得更快呢？</p></li>
</ul>

<p>如何构造学习率α</p>

<h3 id="梯度下降的运行过程阐述">梯度下降的运行过程阐述</h3>

<p>从点 (x<em>k=a)，沿着负梯度方向，移动到 (x</em>{k+1} =b)，这个时候有：</p>

<p>[b=a-\alpha \bigtriangledown F(a)\Rightarrow f(a)\geq g(b)]</p>

<p>也就是说从 (x_0) 为出发点，每次沿着当前函数梯度反方向移动一定距离(\alpha k)，可以得到序列：</p>

<p>[x_0,x_1,\cdots ,x_n]</p>

<p>此时对应的各点函数值序列之间的关系为：</p>

<p>[f(x_0)\geq f(x_1)\geq \cdots \geq f(x_n)]</p>

<p>当n达到一定值时，函数f(x)收敛到局部最小值</p>

<h2 id="视角转换一下">视角转换一下</h2>

<p>记当前点为(x_k) ，当前搜索方向为(d_k) (如：负梯度方向)，因为学习率(\alpha )是待考察的对象，因此，将下列函数 (f(x_k+\alpha d_k))看做是关于(\alpha )的函数(h(\alpha))。<strong>是的，可以的。</strong></p>

<p>[h(\alpha)=f(x_k+\alpha d_k),\;\alpha &gt;0]</p>

<p>那么，当(\alpha=0)时，(h(0)=f(x_k))。而对(\alpha)求偏导可以得到：(\bigtriangledown h(\alpha)=\bigtriangledown f(x_k+\alpha d_k)^Td_k)</p>

<h2 id="看作是关于-alpha-的函数之后-就可以求导了">看作是关于(\alpha )的函数之后，就可以求导了</h2>

<p>因为梯度下降是寻找f(x)的最小值，那么，在(x_k) 和(d_k) 给定的前提下，即寻找函数 (f(x_k +\alpha d_k ))的最小值。即：</p>

<p>[\alpha =arg \,\underset{\alpha &gt;0}{min}h(\alpha)=arg\,\underset{\alpha&gt;0}{min}f(x_k+\alpha d_k)]</p>

<p>进一步，如果 (h(\alpha))可导，局部最小值处的(\alpha)满足：</p>

<p>[h&rsquo;(\alpha)=\bigtriangledown f(x_k+\alpha d_k)^Td_k=0]</p>

<p>注意这个(\alpha)的函数并不一定是凸函数，我们也不需要找到一个最好的(\alpha)，只需要找到一个还不错的(\alpha)就行 。</p>

<h2 id="分析这个学习率函数的导数">分析这个学习率函数的导数</h2>

<p>将 (\alpha =0) 带入：<strong>还有些没明白</strong></p>

<p>[h&rsquo;(0)=\bigtriangledown f(x_k+0*d_k)^Td_k=\bigtriangledown f(x_k)^Td_k]</p>

<p>下降方向(d_k)可以选负梯度方向 (d_k=-\bigtriangledown f(x_k))，或者选中与负梯度夹角小于90°的某个方向（谋面会继续阐述搜索方向问题）。因此：(h&rsquo;(0)&lt;0)。而我们是想找到一个等于0的。</p>

<p>因此，如果能够找到足够大的 (\alpha) 使得 (h&rsquo;(\hat{\alpha})&gt;0) 。则必存在某个 (\alpha^<em>) ，使得(h&rsquo;(\alpha^</em>)=0)。而这个 (\alpha^*) 即为要寻找的学习率。</p>

<p>那么怎么找这个使(h&rsquo;(\alpha^*)=0)的点呢？</p>

<h2 id="使用线性搜索的方法-line-search">使用线性搜索的方法 Line Search</h2>

<h3 id="二分线性搜索-bisection-line-search">二分线性搜索(Bisection Line Search)</h3>

<p>这个使最简单的处理方法</p>

<p>不断将区间 ([\alpha_1,\alpha_2]) 分成两半，选择端点异号的一侧，直到区间足够小或者找到当前最优学习率。</p>

<h3 id="回溯线性搜索-backing-line-search">回溯线性搜索(Backing Line Search)</h3>

<p>基于Armijo准则计算搜素方向上的最大步长，其基本思想是沿着搜索方向移动一个较大的步长估计值，然后以迭代形式不断缩减步长，直到该步长使得函数值 (f(x_k+\alpha d_k)) 相对与当前函数值(f(x_k ))的减小程度大于预设的期望值(即满足Armijo准则)为止。</p>

<p>[f(x_k+\alpha d_k)\leq f(x_k)+c_1\alpha \bigtriangledown f(x_k)^Td_k\; c_1\in (0,1)]</p>

<h3 id="回溯与二分线性搜索的异同">回溯与二分线性搜索的异同</h3>

<p>二分线性搜索的目标是求得满足 (h&rsquo;(\alpha)\approx 0) 的最优步长近似值，而回溯线性搜索放松了对步长的约束，只要步长能使函数值有足够大的变化即可。</p>

<p>二分线性搜索可以减少下降次数，但在计算最优步长上花费了不少代价。而回溯线性搜索找到一个差不多的步长即可。</p>

<h1 id="使用回溯线性搜索">使用回溯线性搜索</h1>

<h2 id="回溯线性搜索代码">回溯线性搜索代码</h2>

<ul>
<li><p>x为当前值</p></li>

<li><p>d为x处的导数</p></li>

<li><p>a为输入学习率</p></li>

<li><p>返回调整后的学习率</p></li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/19afcdC8lg.png?imageslim" alt="mark" /></p>

<h2 id="实验-回溯线性搜索寻找学习率">实验：回溯线性搜索寻找学习率</h2>

<p>y=x^4 ，初值取x=1.5，回溯线性方法</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1cmHh5CfAm.png?imageslim" alt="mark" /></p>

<p>效果还不错</p>

<ul>
<li><p>经过12次迭代，x=0.00010872；</p></li>

<li><p>经过100次迭代，x=3.64905×10^-7</p></li>
</ul>

<p>试比较固定学习率时:</p>

<ul>
<li><p>经过200次迭代，x=0.24436；</p></li>

<li><p>经过1000次迭代，x=0.111275</p></li>
</ul>

<h2 id="回溯线性搜索的思考-插值法">回溯线性搜索的思考：插值法</h2>

<p>采用多项式插值法(Interpolation) 拟合简单函数，然后根据该简单函数估计函数的极值点，这样选择合适步长的效率会高很多。</p>

<p>现在拥有的数据为：(x_k) 处的函数值(f(x_k ))及其导数(f’(x_k )) ，再加上第一次尝试的步长(\alpha_0) 。如果(\alpha_0)满足条件，显然算法退出；若(\alpha_0)不满足条件，则根据上述信息可以构造一个二次近似函数：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1DcL5e7dee.png?imageslim" alt="mark" /></p>

<h3 id="二次插值法求极值">二次插值法求极值</h3>

<p>显然，导数为0的最优值为：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/54f62H6KJF.png?imageslim" alt="mark" /></p>

<p>若 (\alpha_1) 满足Armijo准则，则输出该学习率；否则，继续迭代。</p>

<h1 id="二次插值法">二次插值法</h1>

<h2 id="代码如下">代码如下：</h2>

<ul>
<li><p>x为当前值</p></li>

<li><p>d为x处的导数</p></li>

<li><p>a为输入学习率</p></li>

<li><p>返回调整后的学习率</p></li>
</ul>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/f5C14CGBHi.png?imageslim" alt="mark" /></p>

<h2 id="实验-二次插值线性搜索寻找学习率">实验：二次插值线性搜索寻找学习率</h2>

<p>y=x^4 ，初值取x=1.5，二次插值线性搜索方法</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/50kJ9c1El7.png?imageslim" alt="mark" /></p>

<p>效果还不错</p>

<ul>
<li><p> 经过12次迭代，x=0.0000282229 ；</p></li>

<li><p> 经过100次迭代，x=3.61217×10^-7</p></li>
</ul>

<p>试比较回溯线性搜索时:</p>

<ul>
<li><p> 经过12次迭代，x=0.00010872 ；</p></li>

<li><p> 经过1000次迭代，x=3.649×10^-7</p></li>
</ul>

<h2 id="总结与思考">总结与思考</h2>

<p>通过使用线性搜索的方式，能够比较好的解决学习率问题，一般的说，回溯线性搜索和二次插值线性搜索能够基本满足实践中的需要。</p>

<p>那么：</p>

<ul>
<li><p>可否在搜索过程中，随着信息的增多，使用三次或者更高次的函数曲线，从而得到更快的学习率收敛速度？</p></li>

<li><p>为避免高次产生的震荡，可否使用三次 Hermite 多项式，在端点保证函数值和一阶导都相等，从而构造更光顺的简单低次函数？</p></li>
</ul>

<h1 id="搜索方向">搜索方向</h1>

<p>若搜索方向不是严格梯度方向，是否可以？</p>

<p>思考：因为函数 二阶导数反应了函数的凸凹性；二阶导越大，一阶导的变化越大。在搜索中，可否用二阶导做些“修正”？如：二者相除？</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/CdAKccJ2eB.png?imageslim" alt="mark" /></p>

<h3 id="附-平方根算法">附：平方根算法</h3>

<p>在任意点x 0 处Taylor展开</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Leb66CIieE.png?imageslim" alt="mark" /></p>

<p>一般若干次(5、6次)迭代即可获得比较好的近似值</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/LK7mKG4AE3.png?imageslim" alt="mark" /></p>

<h3 id="实验-搜索方向的探索">实验：搜索方向的探索</h3>

<p>y=x^4 ，初值取x=1.5，负梯度除以二阶导</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/58cEjcFb96.png?imageslim" alt="mark" /></p>

<p>效果出奇的好！</p>

<ul>
<li><p> 经过12次迭代，x=0.00770735；</p></li>

<li><p> 经过100次迭代，x=3.68948×10 -18</p></li>
</ul>

<p>试比较二次插值线性搜索时:</p>

<ul>
<li><p> 经过12次迭代，x=0.0000282229；</p></li>

<li><p> 经过1000次迭代，x=3.61217×10 -7</p></li>
</ul>

<h3 id="分析上述结果的原因">分析上述结果的原因</h3>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/8idA3dl01l.png?imageslim" alt="mark" /></p>

<h3 id="牛顿法">牛顿法</h3>

<p>上述迭代公式，即牛顿法</p>

<p>该方法可以直接推广到多维：用方向导数代替一阶导，用Hessian矩阵代替二阶导</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/2F4h8aIa22.png?imageslim" alt="mark" /></p>

<h3 id="牛顿法的特点">牛顿法的特点</h3>

<ul>
<li><p>经典牛顿法虽然具有二次收敛性，但是要求初始点需要尽量靠近极小点，否则有可能不收敛。</p></li>

<li><p> 计算过程中需要计算目标函数的二阶偏导数，难度较大。</p></li>

<li><p> 目标函数的Hessian矩阵无法保持正定，会导致算法产生的方向不能保证是f在x_k 处的下降方向，从而令牛顿法失效；</p></li>

<li><p> 如果Hessian矩阵奇异，牛顿方向可能根本是不存在的。</p></li>
</ul>

<p>二阶导非正定的情况(一维则为负数)</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/a9FJLBfHE2.png?imageslim" alt="mark" /></p>

<p>修正牛顿方向</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/G7ijm0g8I1.png?imageslim" alt="mark" /></p>

<h3 id="拟牛顿的思路">拟牛顿的思路</h3>

<p>求Hessian矩阵的逆影响算法效率，同时，搜索方向只要和负梯度的夹角小于90°即可，因此，可以用近似矩阵代替Hessian矩阵，只要满足该矩阵正定、容易求逆，或者可以通过若干步递推公式计算得到。</p>

<ul>
<li><p>BFGS / LBFGS</p></li>

<li><p>Broyden – Fletcher – Goldfarb - Shanno</p></li>
</ul>

<h3 id="bfgs">BFGS</h3>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/1JF1fd0KmE.png?imageslim" alt="mark" /></p>

<h3 id="l-bfgs">L-BFGS</h3>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/80H8kDJifl.png?imageslim" alt="mark" /></p>

<h1 id="comment">COMMENT：</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/01-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">梯度下降法</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/01-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E6%A0%87%E9%87%8F%E5%90%91%E9%87%8F%E7%9F%A9%E9%98%B5%E5%92%8C%E5%BC%A0%E9%87%8F/">
            <span class="next-text nav-default">标量、向量、矩阵和张量</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
