<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>信息论-熵 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="信息论-熵 TODO 信息增益和基尼系数要不要单独拿出来？互信息要不要单独拆开来？**** 看看还有什么需要补充的？ 并且，信息论里面还有什么需要掌握的？" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/01-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E7%86%B5/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="信息论-熵" />
<meta property="og:description" content="信息论-熵 TODO 信息增益和基尼系数要不要单独拿出来？互信息要不要单独拆开来？**** 看看还有什么需要补充的？ 并且，信息论里面还有什么需要掌握的？" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/01-%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E4%BF%A1%E6%81%AF%E8%AE%BA-%E7%86%B5/" /><meta property="article:published_time" content="2018-08-01T18:08:04&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-01T18:08:04&#43;00:00"/>
<meta itemprop="name" content="信息论-熵">
<meta itemprop="description" content="信息论-熵 TODO 信息增益和基尼系数要不要单独拿出来？互信息要不要单独拆开来？**** 看看还有什么需要补充的？ 并且，信息论里面还有什么需要掌握的？">


<meta itemprop="datePublished" content="2018-08-01T18:08:04&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-01T18:08:04&#43;00:00" />
<meta itemprop="wordCount" content="3274">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="信息论-熵"/>
<meta name="twitter:description" content="信息论-熵 TODO 信息增益和基尼系数要不要单独拿出来？互信息要不要单独拆开来？**** 看看还有什么需要补充的？ 并且，信息论里面还有什么需要掌握的？"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">信息论-熵</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-01 </span>
        
        <span class="more-meta"> 3274 words </span>
        <span class="more-meta"> 7 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#信息论-熵">信息论-熵</a></li>
<li><a href="#motive">MOTIVE</a></li>
<li><a href="#缘由">缘由：</a></li>
<li><a href="#信息量和熵">信息量和熵</a>
<ul>
<li><a href="#什么是信息量">什么是信息量</a></li>
<li><a href="#什么是熵">什么是熵？</a></li>
<li><a href="#ok-我们来研究下熵里面的这个函数-f-x-xlnx">OK，我们来研究下熵里面的这个函数 f(x)=xlnx</a></li>
<li><a href="#实际上我们可以把这个函数画出来看下">实际上我们可以把这个函数画出来看下</a></li>
</ul></li>
<li><a href="#ok-我们开始对熵进行学习">OK，我们开始对熵进行学习</a>
<ul>
<li><a href="#对熵的理解">对熵的理解</a></li>
<li><a href="#两点分布的熵">两点分布的熵</a></li>
<li><a href="#三点分布的熵">三点分布的熵</a></li>
</ul></li>
<li><a href="#组合数的关系">组合数的关系</a>
<ul>
<li><a href="#推到">推到</a></li>
<li><a href="#自封闭系统的运动总是倒向均匀分布">自封闭系统的运动总是倒向均匀分布</a></li>
<li><a href="#下面我们继续思考-根据函数形式判断概率分布">下面我们继续思考：根据函数形式判断概率分布</a></li>
<li><a href="#之前我们知道-均匀分布熵是最大的-那么给定条件呢">之前我们知道，均匀分布熵是最大的，那么给定条件呢？</a></li>
</ul></li>
<li><a href="#联合熵和条件熵">联合熵和条件熵</a>
<ul>
<li><a href="#定义">定义</a></li>
<li><a href="#推导一下条件熵的定义式">推导一下条件熵的定义式</a></li>
<li><a href="#ok-我们现在可以提出相对熵的概念">OK 我们现在可以提出相对熵的概念：</a></li>
<li><a href="#在实践中用谁相对谁呢">在实践中用谁相对谁呢？</a></li>
<li><a href="#两个-kl-散度的区别">两个 KL 散度的区别</a></li>
</ul></li>
<li><a href="#互信息">互信息</a>
<ul>
<li><a href="#互信息得定义">互信息得定义</a></li>
<li><a href="#计算条件熵的定义式-h-x-i-x-y">计算条件熵的定义式：(H(X)-I(X,Y))</a></li>
<li><a href="#一道思考题-天平与假币">一道思考题：天平与假币</a></li>
</ul></li>
<li><a href="#信息增益">信息增益：</a>
<ul>
<li><a href="#基本记号">基本记号：</a></li>
<li><a href="#信息增益的计算方法">信息增益的计算方法</a></li>
<li><a href="#经验条件熵h-d-a">经验条件熵H(D|A)</a></li>
<li><a href="#其他目标">其他目标</a></li>
<li><a href="#关于基尼指数的讨论-一家之言">关于基尼指数的讨论(一家之言)</a></li>
<li><a href="#ref">REF</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="信息论-熵">信息论-熵</h1>

<p>TODO</p>

<ul>
<li><strong>信息增益和基尼系数要不要单独拿出来？</strong>互信息要不要单独拆开来？****</li>
<li><strong>看看还有什么需要补充的？</strong></li>
<li><strong>并且，信息论里面还有什么需要掌握的？</strong></li>
<li><strong>机器学习书中的KL散度的东西需要添加下。</strong></li>
<li><strong>而且感觉这一篇太大了，要不要拆分下。</strong></li>
</ul>

<h1 id="motive">MOTIVE</h1>

<ul>
<li>a</li>
</ul>

<hr />

<h1 id="缘由">缘由：</h1>

<p>实际上在机器学习中，经常用到熵，比如最大熵模型，以及决策树剪枝的时候，因此把熵单独拿出来，好好总结下。不然总是这里一点那里一点。</p>

<h1 id="信息量和熵">信息量和熵</h1>

<h2 id="什么是信息量">什么是信息量</h2>

<p>比如说，讲课，讲到的知识越不熟悉，说明信息量越大，如果是都知道的，那么熵就是0，就没有信息量。<strong>听到这个地方，我就感觉，一个信息的信息量不仅仅与信息本身有关，还与接受信息的人有关，因为对于不同的人来说，信息量是不同的，然后又想到了量子力学中观测者对于结果的干扰。。看下有没有相似性。</strong></p>

<p>现在我们把看一下刚才的 (log_2p) 是什么？</p>

<p>比如说我们想定义一个东西的信息量，就是一个事件发生的概率越小的话，那么信息量就越大，那么如何定义信息量呢？</p>

<p>首先我们对要定义的这个东西有两个原则：</p>

<ul>
<li><p>某事件发生的概率小，则该事件的信息量大。</p></li>

<li><p>如果两个事件X和Y独立，即 p(xy)=p(x)p(y) ，假定X和Y的信息量分别为h(X)和h(Y)，则二者同时发生的信息量应该为 h(XY)=h(X)+h(Y)。</p></li>
</ul>

<p>上面的相加比较符合我们对信息的感觉。</p>

<p>那么怎么弄成加起来的形式呢？最简单的形式就是取对数，由于概率是0~1，因此对数后是负的，因此我们这么定义事件 X 发生的信息量： (h(x)=-log_2x)</p>

<p>可见，如果x的概率越小，它如果发生了，那么它的信息量是很大的。</p>

<p>那么我们能不能定义信息量的期望呢？比如事件X的信息量的期望如何计算呢？</p>

<p>OK，这时候我们就介绍一下熵：</p>

<h2 id="什么是熵">什么是熵？</h2>

<p>什么是熵呢？对随机事件的信息量求期望，就得到了熵的定义：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/EdFbl0GDdd.png?imageslim" alt="mark" /></p>

<p>可见，熵是来定义这个随机事件的信息量的强弱大小的。<strong>嗯，不错。</strong></p>

<p>有两点需要注意：</p>

<ol>
<li>这个地方的 p(x) 是一个函数，不是一个数。</li>
<li>在经典熵的定义，底数是 2，单位是 bit   。但是实际上有时候也用以 e 、10为底的，为什么可以改变这个底数呢？因为我们不关心这个信息量的值具体是什么，我们关心的是当一个事件的信息量取最大的时候，他的 p 应该取什么，即可以通过取什么参数来使信息量达到最大。在本例中，为分析方便使用底数 e，此时单位是 nat (奈特)。</li>
</ol>

<h2 id="ok-我们来研究下熵里面的这个函数-f-x-xlnx">OK，我们来研究下熵里面的这个函数 f(x)=xlnx</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/AhCc5DcHBa.png?imageslim" alt="mark" /></p>

<p>由于在 x=0 的时候，lnx 是没有定义的，但 (\underset{lim}{x\rightarrow 0}f(x)=0) ，因此为了使这个函数连续，因此我们强制定义 (f(0)=0)。</p>

<h2 id="实际上我们可以把这个函数画出来看下">实际上我们可以把这个函数画出来看下</h2>

<p>代码如下：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/87IB393edc.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Kd7Da8I23l.png?imageslim" alt="mark" /></p>

<h1 id="ok-我们开始对熵进行学习">OK，我们开始对熵进行学习</h1>

<h2 id="对熵的理解">对熵的理解</h2>

<p>由于 (p(x)) 是0~1的，因此 (ln p(x))就是小于0的，那么熵就是大于等于0 的，而且由于 (p(x)) 实际上是一个分布函数，那么 (H(X)) 就可以看成一个函数的函数，也就是一个函数到一个值的映射。**在讲到这里的时候提到了泛函和变分推导，泛函是什么？变分推导是什么？ **</p>

<p>熵是随机变量不确定性的度量：</p>

<ul>
<li>随机变量的不确定性越大，熵值越大。</li>
<li>若随机变量退化成定值，即若某一个事件以概率1发生。那么这件事的熵就是0，就是极小值。</li>
</ul>

<p>而这种不确定性度量的本质就是信息熵的期望。<strong>？是这样吗？</strong></p>

<p>均匀分布是 ”最不确定“ 的分布。<strong>厉害了</strong></p>

<p>上面提到了熵的最小值是0，那么它有上界吗？上界是多少？</p>

<p>比如说，对于一个两点分布而言：</p>

<h2 id="两点分布的熵">两点分布的熵</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/m1h5HEF49E.png?imageslim" alt="mark" /></p>

<p>那么把两点分布变成三点分布呢？</p>

<h2 id="三点分布的熵">三点分布的熵</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/0cGaE92gcE.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/8F3JGF2Bm3.png?imageslim" alt="mark" /></p>

<p>第一幅图的高度是熵，后面两幅都是旋转后的，可见，p1=p2=1/3的时候，三点的概率分布最大。<strong>这幅图与dirichlet分布有关吗？</strong></p>

<p>可见，n 个的概率都相等的时候，熵最大，所以熵的最大值就是： (log|X|)。</p>

<p>也即：</p>

<p>[0\leq H(X)\leq log|X|]</p>

<p>OK，下面我们再从另外一个角度来探讨熵是什么？</p>

<h1 id="组合数的关系">组合数的关系</h1>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/8lLIiKFKKb.png?imageslim" alt="mark" /></p>

<h2 id="推到">推到</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/D7jH1jemb2.png?imageslim" alt="mark" /></p>

<p>其中 (ln N!\rightarrow N(lnN-1)) 的推导如下：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/bA83gebG5G.png?imageslim" alt="mark" /></p>

<p>对上面的推导进行解释：</p>

<p>最后一步是因为ni/N 是频率，由大数定理，频率等于概率，因此当N无穷大的额时候，</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/EIlFI62k48.png?imageslim" alt="mark" />就是熵。</p>

<p>所以当N无穷大到无穷的时候：</p>

<h2 id="自封闭系统的运动总是倒向均匀分布">自封闭系统的运动总是倒向均匀分布</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/K657Jl8k9C.png?imageslim" alt="mark" /></p>

<h2 id="下面我们继续思考-根据函数形式判断概率分布">下面我们继续思考：根据函数形式判断概率分布</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/IhkC6g6ege.png?imageslim" alt="mark" /></p>

<p>也就是说，正太分布的概率密度函数的对数形式是一个二次函数形式的。</p>

<p>那么反过来，如果一个分布的对数可以写成一个二次形式，那么这个分布必然是一个正态分布。</p>

<p>比如我们再看一下Gamma分布：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/AcB78cg8dB.png?imageslim" alt="mark" /></p>

<p>这个地方提一下：二项分布的共轭分布就是Gamma分布，从2到n就是多形式分布，从Gamme分布到N就是directlit分布。</p>

<h2 id="之前我们知道-均匀分布熵是最大的-那么给定条件呢">之前我们知道，均匀分布熵是最大的，那么给定条件呢？</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/kDAdf8hG3H.png?imageslim" alt="mark" /></p>

<p>利害，这怎么求？</p>

<p>过程如下：</p>

<p>首先，我们先建立一个目标函数：我们不知道 p(x) 是什么样子的，我们想要做的就是将 H(x) 取极大，然后求出这个时候的 p(x)。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/mDK7f1c8ae.png?imageslim" alt="mark" /></p>

<p>使用方差公式化简约束条件：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/dHC61mCD1e.png?imageslim" alt="mark" /></p>

<p>也就是说，我们现在知道了 x 的期望和 x^2 的期望。那么显然，这是一个带约束的极值问题，可以用Lagrange乘子法：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/fCBja0CHGc.png?imageslim" alt="mark" /></p>

<p>求偏导得：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/3FaCF9cC22.png?imageslim" alt="mark" /></p>

<p>可见 p(x) 的对数形式是一个关于x得平方的。根据我们刚才的引理，这样的分布一定是一个正态分布。** 利害的。在看一下 之前相关的引理。**</p>

<p>OK，上面都是一个随机变量的问题，现在准备看下多个随机变量之间的关系：</p>

<h1 id="联合熵和条件熵">联合熵和条件熵</h1>

<h2 id="定义">定义</h2>

<p>联合熵 H(X,Y)：</p>

<p>两个随机变量 X，Y 的联合分布，可以形成联合熵  Joint Entropy。注意：H(X,Y) 与 H(XY) 这两种写法是不加以区别的。</p>

<p>条件熵 H(X|Y)=H(X,Y) – H(Y)</p>

<p>即 (X,Y) 发生所包含的熵，减去 Y 单独发生包含的熵就是在 Y 发生的前提下，X 发生 “新” 带来的熵。</p>

<h2 id="推导一下条件熵的定义式">推导一下条件熵的定义式</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/KkHLg1ke09.png?imageslim" alt="mark" /></p>

<p>解释一下：上面的对于 (p(y)) 写成 (\sum_{x}^{ }p(x,y))的形式，是因为边缘分布等于各自的联合分布的和。</p>

<p>OK，上面已经得到了条件熵的定义式，继续推导：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/0jGElHdF5g.png?imageslim" alt="mark" /></p>

<p>解释一下：导数第二部的括号中的意思是：x取某一个值的时候p(y)的条件分布的熵。因此可以写成最后的形式。</p>

<h2 id="ok-我们现在可以提出相对熵的概念">OK 我们现在可以提出相对熵的概念：</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/GCd7ifBc35.png?imageslim" alt="mark" /></p>

<p>显然，上面的式子中的求和可以看作是在p(x)的分布下求这个式子的期望</p>

<p>我们对上面的相对熵的式子进行理解：如果某一个真实分布p(x)，我们没有用真实分布来求信息熵，我们用q(x)来求信息熵，所以最后是这个样子(\sum p(x)ln(q(x))) 而这个与我们实际的(\sum p(x)ln(p(x))) 二者之间的差就是上面说的相对熵。利害。</p>

<p>相对熵还是很重要的：相对熵可以度量两个随机变量的 “距离” 。</p>

<p>由凸函数中的 Jensen 不等式可以得：D(p||q)≥0、 D(q||p) ≥0 。</p>

<p>在 “贝叶斯网络” 、 “变分推导” 等章节会再次遇到相对熵。<strong>遇到之后回来补充下</strong></p>

<p>相对熵比较麻烦的地方就是，这两个不是对称的定义，一般的，D(p||q) ≠D(q||p)，既然不是对称的情况，哪我们在实践中用谁相对谁呢？</p>

<h2 id="在实践中用谁相对谁呢">在实践中用谁相对谁呢？</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/kD7k5Efegl.png?imageslim" alt="mark" /></p>

<p>注意，P是存在的未知的一个东西，Q是假定的算出的简单的一个东西。所以两个KL散度还是不大一样的。</p>

<h2 id="两个-kl-散度的区别">两个 KL 散度的区别</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Km4eHCKA2L.png?imageslim" alt="mark" /></p>

<p>如果KL=0，当且仅当p=q才可以。</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/JgdDDGAK21.png?imageslim" alt="mark" /></p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/JkL7l4BiLc.png?imageslim" alt="mark" /></p>

<p>OK，我们开始介绍互信息</p>

<h1 id="互信息">互信息</h1>

<h2 id="互信息得定义">互信息得定义</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/6D2ff2ecjd.png?imageslim" alt="mark" /></p>

<p>如果 X，Y 是独立的，那么 p(x,y) 就等于p(x)p(y)，因此相等的时候互信息就是0，因此互信息表示的是二者之间的相关性的一种度量。</p>

<p>同样，我们也可以推出来条件熵：</p>

<h2 id="计算条件熵的定义式-h-x-i-x-y">计算条件熵的定义式：(H(X)-I(X,Y))</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/33926I9dHE.png?imageslim" alt="mark" /></p>

<p>根据条件熵的定义式，可以得到</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/64ebJag228.png?imageslim" alt="mark" /></p>

<p>所以条件熵可以写成这两种形式：</p>

<ul>
<li><p>H(X|Y) = H(X,Y) - H(Y)   条件熵定义</p></li>

<li><p>H(X|Y) = H(X) - I(X,Y)   根据互信息定义展开得到</p></li>
</ul>

<p>同样，可以衍生出：</p>

<ul>
<li>对偶式

<ul>
<li>H(Y|X)= H(X,Y) - H(X)</li>
<li>H(Y|X)= H(Y) - I(X,Y)</li>
</ul></li>
<li>I(X,Y)= H(X) + H(Y) - H(X,Y)</li>
<li>H(X|Y) ≤H(X) ， H(Y|X) ≤H(Y)</li>
</ul>

<p>最后一个不等式 H(X|Y) ≤H(X) ， H(Y|X) ≤H(Y)  意味着：随着给定的条件越来越多，不确定性越来越少，最终走向确定性。</p>

<p>Venn 图可以说明上述的这些公式：</p>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/hb67lckbel.png?imageslim" alt="mark" /></p>

<h2 id="一道思考题-天平与假币">一道思考题：天平与假币</h2>

<p>有13枚硬币，其中有1枚是假币，但不知道是重还是轻。现给定一架没有砝码的天平，问至少需要多少次称量才能找到这枚假币？</p>

<p>答：3次。如何称量？如何证明？</p>

<h1 id="信息增益">信息增益：</h1>

<p>概念：当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。</p>

<p>信息增益表示得知特征A的信息而使得类X的信息的不确定性减少的程度。</p>

<p>定义：特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：g(D,A)=H(D) – H(D|A)    显然，这即为训练数据集D和特征A的互信息。</p>

<h2 id="基本记号">基本记号：</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/h98cF0kC92.png?imageslim" alt="mark" /></p>

<h2 id="信息增益的计算方法">信息增益的计算方法</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/kcjm3IL1jJ.png?imageslim" alt="mark" /></p>

<h2 id="经验条件熵h-d-a">经验条件熵H(D|A)</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/7L5J8FF1HI.png?imageslim" alt="mark" /></p>

<h2 id="其他目标">其他目标</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/DmDlA7G1gL.png?imageslim" alt="mark" /></p>

<h2 id="关于基尼指数的讨论-一家之言">关于基尼指数的讨论(一家之言)</h2>

<p><img src="http://pacdb2bfr.bkt.clouddn.com/blog/image/180728/Jhd7gh39Hh.png?imageslim" alt="mark" /></p>

<h2 id="ref">REF</h2>

<ul>
<li>七月在线 机器学习</li>
<li>《机器学习》周志华</li>
</ul>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/02-%E4%B8%AA%E4%BA%BA%E8%BF%AD%E4%BB%A3/02-%E4%B8%AA%E4%BA%BA/03-%E8%A1%8C%E4%B8%BA%E4%B9%A0%E6%83%AF/%E5%86%A5%E6%83%B3/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">冥想</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/01-%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B/01-%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/01-c&#43;&#43;/c&#43;&#43;-gtest/">
            <span class="next-text nav-default">C&#43;&#43; gtest</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
