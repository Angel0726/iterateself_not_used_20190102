<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>RL 多臂老虎机问题 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="author: evo comments: true date: 2018-05-16 16:19:21&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/5872/ slug: &amp;lsquo;5872&amp;rsquo; title: RL 多臂老虎机问题 wordpress_id: 5872 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="RL 多臂老虎机问题" />
<meta property="og:description" content="author: evo comments: true date: 2018-05-16 16:19:21&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/5872/ slug: &lsquo;5872&rsquo; title: RL 多臂老虎机问题 wordpress_id: 5872 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98/" /><meta property="article:published_time" content="2018-08-21T18:16:22&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-21T18:16:22&#43;00:00"/>
<meta itemprop="name" content="RL 多臂老虎机问题">
<meta itemprop="description" content="author: evo comments: true date: 2018-05-16 16:19:21&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/5872/ slug: &lsquo;5872&rsquo; title: RL 多臂老虎机问题 wordpress_id: 5872 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所">


<meta itemprop="datePublished" content="2018-08-21T18:16:22&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-21T18:16:22&#43;00:00" />
<meta itemprop="wordCount" content="2434">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RL 多臂老虎机问题"/>
<meta name="twitter:description" content="author: evo comments: true date: 2018-05-16 16:19:21&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/5872/ slug: &lsquo;5872&rsquo; title: RL 多臂老虎机问题 wordpress_id: 5872 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">最新</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">最新</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">RL 多臂老虎机问题</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-21 </span>
        
        <span class="more-meta"> 2434 words </span>
        <span class="more-meta"> 5 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#reinforcement-learning">- Reinforcement Learning</a></li>
<li><a href="#相关资料">相关资料</a></li>
<li><a href="#需要补充的">需要补充的</a></li>
</ul></li>
<li><a href="#introduction">INTRODUCTION</a>
<ul>
<li><a href="#数学符号的含义">数学符号的含义</a></li>
<li><a href="#多臂老o虎o机问题">多臂老O虎O机问题</a>
<ul>
<li><a href="#如何判断算法的好坏">如何判断算法的好坏</a></li>
</ul></li>
<li><a href="#解决方法">解决方法</a>
<ul>
<li><a href="#行动-价值方法-action-value-method">行动-价值方法 (action-value method)</a></li>
<li><a href="#增值实现-incremental-implementation">增值实现(incremental implementation)</a></li>
<li><a href="#带权值步长的增值实现-incremental-implementation-with-weighted-step-size">带权值步长的增值实现(incremental implementation with weighted step size)</a></li>
<li><a href="#优化初始值-optimistic-initial-values">优化初始值(Optimistic initial values)</a></li>
<li><a href="#置信上界选择算法-upper-confidence-bound-action-selection">置信上界选择算法 (Upper-Confidence-Bound action selection)</a></li>
</ul></li>
<li><a href="#参照">参照</a></li>
</ul></li>
<li><a href="#comment">COMMENT</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<hr />

<p>author: evo
comments: true
date: 2018-05-16 16:19:21+00:00
layout: post
link: <a href="http://106.15.37.116/2018/05/17/5872/">http://106.15.37.116/2018/05/17/5872/</a>
slug: &lsquo;5872&rsquo;
title: RL 多臂老虎机问题
wordpress_id: 5872
categories:
- 人工智能学习
tags:
- NOT_ADD</p>

<h2 id="reinforcement-learning">- Reinforcement Learning</h2>

<!-- more -->

<p>[mathjax]</p>

<p><strong>注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所列的原文。</strong></p>

<h2 id="相关资料">相关资料</h2>

<ol>
<li></li>
</ol>

<p><a href="http://www.cnblogs.com/steven-yang/p/6476034.html">强化学习读书笔记 - 02 - 多臂老O虎O机问题</a></p>

<h2 id="需要补充的">需要补充的</h2>

<ul>
<li>aaa</li>
</ul>

<hr />

<h1 id="introduction">INTRODUCTION</h1>

<ul>
<li>aaa</li>
</ul>

<h2 id="数学符号的含义">数学符号的含义</h2>

<ul>
<li><p>通用
(a) - 行动(action)。
(A_t) - 第t次的行动(select action)。通常指求解的问题。</p></li>

<li><p>在老O虎O机问题中
(q_*(a)) - 行动 a 的真实奖赏(true value)。这个是（实际中）不可知的。期望计算的结果收敛(converge)与它。
(N_t(a)) - 在第t次之前，行动a被选择的次数。
(R_t) - 第t步的实际奖赏(actual reward)。
(Q_t(a)) - 行动 a 在第t次前（不包括第t次）的实际平均奖赏。
[
Q<em>t(a) = \frac{\sum</em>{i=1}^{t-1} R<em>i \times 1</em>{A_i=a} }{N_t(a)}
]
(H_t(a)) - 对于行动a的学习到的倾向。
(\epsilon) - 在ε-贪婪策略中，采用随机行动的概率([0, 1))。</p></li>
</ul>

<h2 id="多臂老o虎o机问题">多臂老O虎O机问题</h2>

<p>一般的老O虎O机只有一个臂（杆）。你塞10个硬币，拉一下杆，老O虎O机可能会吐出来一两个硬币，或者100个硬币。
多臂老O虎O机有多个杆（象征着多个行动(action)，每个杆有自己特有的吐钱逻辑）。
注意：每个杆的吐钱概率可能是固定的(stationary)，也可能是不固定的(non-stationary)。不固定在现实中更常见。
多臂老O虎O机问题就是在许多次尝试后，<strong>找到一个有效收益的策略</strong>。
多臂老O虎O机问题是统计学、工程、心理学的一个经典问题。不要小看了这个问题，很多权威都研究过。
在强化学习方面，我们通过这个问题，可以了解强化学习的基本概念和算法的思路。其中包括：</p>

<ul>
<li><p>探索(exploration)和采用(exploitation)的权衡</p></li>

<li><p>贪婪(greedy)</p></li>

<li><p>收敛(convergence)</p></li>

<li><p>参数初始化的重要性</p></li>

<li><p>梯度递减(gradient descent)
（注：梯度递增和梯度递减的意思一样，只是看问题的方向不一样。）
等等。</p></li>
</ul>

<h3 id="如何判断算法的好坏">如何判断算法的好坏</h3>

<p>在讨论算法前，我们先考虑判断算法好坏的标准。</p>

<ul>
<li><p>建立模型
建立一个10臂老O虎O机。
每个臂的真实行动价值(q_<em>(a), a = 1, \dots, 10)是一个符合（平均值=0, 方差=1）的正态分布。
每个臂的每次行动的估值(R<em>t(a))是一个符合（平均值=(q</em></em>(a)), 方差=1）的正态分布。</p></li>

<li><p>测试标准</p>

<ul>
<li><p>平均奖赏 - 奖赏越多越好</p></li>

<li><p>最优行动 - 和实际最优行动一致的比率越大越好</p></li>
</ul></li>
</ul>

<h2 id="解决方法">解决方法</h2>

<h3 id="行动-价值方法-action-value-method">行动-价值方法 (action-value method)</h3>

<p>在决定第t次的行动(A_t)时，使用下面的算法。
[
A_t = \underset{a}{argmax} Q<em>t(a) <br />
where <br />
1</em>{A_i=a} =
\begin{cases}
1, if A_i = a <br />
0, otherwise
\end{cases}
]</p>

<ul>
<li>贪婪方法(greedy method)
总是选择当前取得最大收益的行动。
特点：<strong>最大化采用(exploitation)。</strong>
算法的过程如下：</li>
</ul>

<blockquote>初始： \(Q_0(a), a = 1, \cdots, 10\) 都为0；
每个杆（action）都拉一下。 \(Q_0(a), a = 1, \cdots, 10\) 有了新值。
根据当前平均收益最大的杆，当做本次选择的杆。</blockquote>

<ul>
<li>ε - 贪婪方法(ε-greedy method)
ε - 读作epsilon。有弹性的意思。
一般情况下选择当前取得最大收益的行动，但是有一定比例ε的行动是随机选择的。
特点：<strong>增强了探索(exploration)性。</strong>
算法的过程如下：</li>
</ul>

<blockquote>初始： \(Q_0(a), a = 1, \cdots, 10\) 都为0；
每个杆（action）都拉一下。 \(Q_0(a), a = 1, \cdots, 10\) 有了新值。
根据当前平均收益最大的杆，当做本次选择的杆。
同时根据\(ε\)的值，随机选择一个杆。（比如：\(ε=0.1\)，每十次随机选择一个杆）</blockquote>

<h3 id="增值实现-incremental-implementation">增值实现(incremental implementation)</h3>

<p>如何计算(Q_t)。
<strong>算法</strong>
[
\begin{array} <br />
Q<em>t
&amp; = \frac{1}{t-1} \sum</em>{i=1}^{t-1} R_i \text{ : this method need memory all } R<em>i. <br />
&amp; = Q</em>{t-1} + \frac{1}{t-1} \left [ R<em>{t-1} - Q</em>{t-1} \right ] \text{this method is better, just need memory last } R<em>{t-1}, Q</em>{t-1}.
\end{array}
]</p>

<h3 id="带权值步长的增值实现-incremental-implementation-with-weighted-step-size">带权值步长的增值实现(incremental implementation with weighted step size)</h3>

<p>一个替代算法。用步长(\alpha) 代替$ \frac{1}{t-1}$。
<strong>算法</strong>
[
Q<em>t = Q</em>{t-1} + \alpha \left [ R<em>{t-1} - Q</em>{t-1} \right ]
]</p>

<p><strong>解释</strong>
这个算法利于解决非稳定(non-stationary)问题。
非稳定(non-stationary)问题意味着(q_*(a))是会发生变化的。因此，最近几次的奖赏更具代表性。
(\alpha)越大，意味着最近奖赏的权重越大。
这里也可以看到梯度计算的影子。</p>

<h3 id="优化初始值-optimistic-initial-values">优化初始值(Optimistic initial values)</h3>

<p>优化初始值(Q_1(a))，如果赋值越大，会鼓励探索。
初始值为0时，ε - 贪婪方法(ε=0.1) 好于 ε - 贪婪方法(ε=0.01) 好于 贪婪方法。
看来冒一定风险还是有好处的。
初始值为5的贪婪方法 好于 ε - 贪婪方法(ε=0.1)。
有钱人更容易成功。</p>

<h3 id="置信上界选择算法-upper-confidence-bound-action-selection">置信上界选择算法 (Upper-Confidence-Bound action selection)</h3>

<p>可理解为求每个行动的<strong>最大可信值</strong>，选择<strong>最大可信值</strong>最大的行动。</p>

<p><strong>算法</strong>
[
A_t = \underset{a}{argmax} \left [ Q_t(a) + c \sqrt{\frac{\log{t} }{N_t(a)} } \right ] <br />
where <br />
c \text{ : &gt; 0, controls the degree of exploration. bigger c means more exploration.} <br />
\text{if } N_t(a) = 0 \text{, then a is considered to be a maximizing action.}
]</p>

<p><strong>算法理解</strong>
这个算法在计算：<strong>第t次的行动应该是什么？</strong></p>

<blockquote>我们没有说：“第t次的**最优**行动应该是什么？”。为什么不说**最优**呢？
因为，强化学习的目的是总体最优，不是局部最优，因此“第t次的**最优**行动”不是强化学习最求的目标。</blockquote>

<p>(c)是一个可调的参数。我们在理解中不用太关心它，当它是(1)好了。</p>

<blockquote>在机器学习中，算法一般都有几个参数可以调节。不同环境中，调节参数最优化可以很大的提高算法的质量。
\(Q_t(a)\) - 行动a当前的奖赏。
\(t\) - 第t次。
\(\log{t}\) - 求t的指数。随着t变大，\(\log{t}\)变大的速度变慢。
\(N_t(a)\) - 行动a被选择的次数。
\(\left [ \sqrt{\frac{\log{t} }{N_t(a)} } \right ]\) - 由于\(\frac{\log{t} }{N_t(a)} < 1 \text{， when x > 7 }\)， 求平方根，反而是起了一个放缓、放大的作用。
在没有奖赏的情况下：\(Q_t(a)\) 不变。\(\log{t}\)比\(N_t(a)\)变化的慢，因此总结果会变小。</blockquote>

<ul>
<li>梯度老O虎O机算法 (Gradient Bandit Algorithms)
之前的算法，主要是通过发生的事件，根据<strong>行动的估计奖赏</strong>，来决定选择哪个行动。
梯度算法是：通过发生的事件，根据<strong>行动的倾向</strong>(H_t(a))，来决定选择哪个行动。
（个人没看出有什么不同）。
[
Pr{A_t = a} = \pi_t(a) = softmax(H_t(a)) = \frac{e^{H<em>t(a)} }{ \sum</em>{i=1}^k e^{H_t(a)} } <br />
A_t = \underset{a}{argmax} (\pi_t(a)) <br />
\pi_t(a) \text{ for the probability of taking action a at time t.}
]</li>
</ul>

<blockquote>softmax是一个激活函数。通常用于输出的概率计算，就是现在看到的例子。</blockquote>

<p>[
H_1(a) = 0, \forall a <br />
\text{After action } A_t \text{ and receiving the reward } R<em>t, <br />
H</em>{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1- \pi_t(A<em>t)) \text{, and} <br />
H</em>{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) , \forall a \ne A_t <br />
\bar{R}_t = \frac{\sum R_t}{t} <br />
where <br />
\alpha \text{ - step size parameter.} <br />
\bar{R}_t \text{ - the average of all the rewards up through and including time t.}
]</p>

<h2 id="参照">参照</h2>

<hr />

<h1 id="comment">COMMENT</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-on-policy-%E9%A2%84%E6%B5%8B%E7%9A%84%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">RL on-policy 预测的近似方法</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">
            <span class="next-text nav-default">RL 强化学习总结</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
