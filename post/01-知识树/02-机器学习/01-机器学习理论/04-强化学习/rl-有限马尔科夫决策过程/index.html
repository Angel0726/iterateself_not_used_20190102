<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>RL 有限马尔科夫决策过程 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="author: evo comments: true date: 2018-05-16 16:26:26&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b/ slug: rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b title: RL 有限马尔科夫决策过程 wordpress_id: 5878 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推荐直接看" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="RL 有限马尔科夫决策过程" />
<meta property="og:description" content="author: evo comments: true date: 2018-05-16 16:26:26&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b/ slug: rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b title: RL 有限马尔科夫决策过程 wordpress_id: 5878 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推荐直接看" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" /><meta property="article:published_time" content="2018-08-21T18:16:22&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-21T18:16:22&#43;00:00"/>
<meta itemprop="name" content="RL 有限马尔科夫决策过程">
<meta itemprop="description" content="author: evo comments: true date: 2018-05-16 16:26:26&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b/ slug: rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b title: RL 有限马尔科夫决策过程 wordpress_id: 5878 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推荐直接看">


<meta itemprop="datePublished" content="2018-08-21T18:16:22&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-21T18:16:22&#43;00:00" />
<meta itemprop="wordCount" content="2185">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RL 有限马尔科夫决策过程"/>
<meta name="twitter:description" content="author: evo comments: true date: 2018-05-16 16:26:26&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b/ slug: rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b title: RL 有限马尔科夫决策过程 wordpress_id: 5878 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推荐直接看"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">RL 有限马尔科夫决策过程</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-21 </span>
        
        <span class="more-meta"> 2185 words </span>
        <span class="more-meta"> 5 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#reinforcement-learning">- Reinforcement Learning</a></li>
</ul></li>
<li><a href="#original">ORIGINAL</a></li>
<li><a href="#强化学习读书笔记-03-有限马尔科夫决策过程-http-www-cnblogs-com-steven-yang-p-6480666-html"><a href="http://www.cnblogs.com/steven-yang/p/6480666.html">强化学习读书笔记 - 03 - 有限马尔科夫决策过程</a></a></li>
<li><a href="#todo">TODO</a></li>
<li><a href="#introduction">INTRODUCTION</a>
<ul>
<li><a href="#代理-环境接口-the-agent-environment-interface">代理-环境接口(The agent-environment interface)</a></li>
<li><a href="#情节性任务-episodic-tasks-和连续任务-continuing-tasks">情节性任务(Episodic Tasks)和连续任务(Continuing Tasks)</a></li>
<li><a href="#马尔科夫属性-the-markov-property">马尔科夫属性(The Markov property)</a></li>
<li><a href="#马尔科夫决策过程-数学模型">马尔科夫决策过程 - 数学模型</a></li>
<li><a href="#状态-state-行动-action-奖赏-reward-视图">状态(state)-行动(action)-奖赏(reward)视图</a></li>
<li><a href="#目标-goal-奖赏-reward-视图">目标(goal)-奖赏(reward)视图</a></li>
<li><a href="#决策过程视图">决策过程视图</a>
<ul>
<li><a href="#相应的数学定义和公式">相应的数学定义和公式</a></li>
</ul></li>
<li><a href="#策略视图">策略视图</a>
<ul>
<li><a href="#价值方法-value-functions">价值方法(Value Functions)</a></li>
<li><a href="#最优价值方法-optimal-value-functions">最优价值方法(Optimal Value Functions)</a></li>
</ul></li>
</ul></li>
<li><a href="#comment">COMMENT</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<hr />

<p>author: evo
comments: true
date: 2018-05-16 16:26:26+00:00
layout: post
link: <a href="http://106.15.37.116/2018/05/17/rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b/">http://106.15.37.116/2018/05/17/rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b/</a>
slug: rl-%e6%9c%89%e9%99%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96%e8%bf%87%e7%a8%8b
title: RL 有限马尔科夫决策过程
wordpress_id: 5878
categories:
- 人工智能学习
tags:
- NOT_ADD</p>

<h2 id="reinforcement-learning">- Reinforcement Learning</h2>

<!-- more -->

<p>[mathjax]</p>

<p><strong>注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所列的原文。</strong></p>

<h1 id="original">ORIGINAL</h1>

<ol>
<li></li>
</ol>

<h1 id="强化学习读书笔记-03-有限马尔科夫决策过程-http-www-cnblogs-com-steven-yang-p-6480666-html"><a href="http://www.cnblogs.com/steven-yang/p/6480666.html">强化学习读书笔记 - 03 - 有限马尔科夫决策过程</a></h1>

<h1 id="todo">TODO</h1>

<ul>
<li>aaa</li>
</ul>

<hr />

<h1 id="introduction">INTRODUCTION</h1>

<ul>
<li>aaa</li>
</ul>

<h2 id="代理-环境接口-the-agent-environment-interface">代理-环境接口(The agent-environment interface)</h2>

<p>代理(agent) - 学习者或者决策者</p>

<p>环境(environment) - 代理外部的一切，代理与之交互。</p>

<h2 id="情节性任务-episodic-tasks-和连续任务-continuing-tasks">情节性任务(Episodic Tasks)和连续任务(Continuing Tasks)</h2>

<p>情节性任务(Episodic Tasks)，所有的任务可以被可以分解成一系列情节。逻辑上，可以看作为有限步骤的任务。</p>

<p>连续任务(Continuing Tasks) ，所有的任务不能分解。可以看作为无限步骤任务。</p>

<h2 id="马尔科夫属性-the-markov-property">马尔科夫属性(The Markov property)</h2>

<p>state - 马尔科夫属性，表示当前环境的状态。</p>

<p>举个例子：一个国际象棋的state可能包含：棋盘上所有棋子的位置，上一步的玩家，上一步的走法。</p>

<p>看看下面的公式：</p>

<p>这个公式在计算下一步（状态是(s&rsquo;)、奖赏是(r)）的概率。</p>

<p>并说明这个概率是由至今为止所有的状态(S<em>)，行动(A</em>)和奖赏(R*)决定的。</p>

<p>[
Pr{s<em>{t+1} = s&rsquo;, R</em>{t+1} = r | S_0, A_0, R_1, S_1, A_1, \dots, R_t, S_t, A_t } <br />
]</p>

<p>如果，我们有马尔科夫属性state，有了现在环境的所有状态，那么上面的公式可以简化为：</p>

<p>这个公式的含义是下一步（状态是(s&rsquo;)、奖赏是(r)）的概率是<strong>由马尔科夫属性(s)和行动(a)决定的</strong>。</p>

<p>[
p(s&rsquo;, r | s, a) = Pr {S<em>{t+1} = s&rsquo;, R</em>{t+1} = r | S_t = s, A_t = a }
]</p>

<h2 id="马尔科夫决策过程-数学模型">马尔科夫决策过程 - 数学模型</h2>

<p>马尔科夫决策过程是一个强化学习问题的数学描述模型。</p>

<p>这个数学模型可以从几个视图来学习。</p>

<ul>
<li><p>状态(state)-行动(action)-奖赏(reward)视图</p></li>

<li><p>目标(goal)-奖赏(reward)视图</p></li>

<li><p>决策过程视图</p></li>

<li><p>策略(policy)视图</p></li>
</ul>

<h2 id="状态-state-行动-action-奖赏-reward-视图">状态(state)-行动(action)-奖赏(reward)视图</h2>

<p>Markov Decision Processes - TermsMarkov Decision Processes - Termsstates  (state)state1s&rsquo; (state)state-&gt;state1 r  (reward) a  (action)</p>

<p>这是一个马尔科夫抉择过程的基本视图。</p>

<p>描述agent在状态(s)下，选择了行动(a)，状态变为(s&rsquo;)，获得了奖赏(r)。</p>

<p>这个很容易理解，说明奖赏是行动引起状态转变后得到的。</p>

<p>举个特殊例子：天上掉馅饼的过程：行动是等待；新状态是获得馅饼。</p>

<h2 id="目标-goal-奖赏-reward-视图">目标(goal)-奖赏(reward)视图</h2>

<p>Markov Decision Processes - GoalMarkov Decision Processes - GoalS2&hellip;S_tS_tS2-&gt;S_tS_t_1S_t+1S_t_2&hellip;S_t_1-&gt;S_t_2 R_t+2A_t+1S0S0S1S1S0-&gt;S1R1A0S1-&gt;S2R2A1S_t-&gt;S_t_1 R_t+1A_t</p>

<p>奖赏假设(reward hypothesis) - 目标就是：最大化长期累计奖赏的期望值。</p>

<p>注：不是立即得到的奖赏。</p>

<p>回报(G_t)：</p>

<p>[
G<em>t \doteq \sum</em>{k=0}^{\infty} \gamma^k R_{t+k+1} <br />
where <br />
\gamma \text{ - is a parameter, discount rate, } 0 \leqslant \gamma \leqslant 1
]</p>

<p>(\gamma)折扣率决定了未来奖赏的当前价值：</p>

<p>在k步之后的一个奖赏，如果换算成当前奖赏，需要乘以它的(\gamma^{k-1})倍。</p>

<p><strong>情节性任务(episodic tasks)的回报计算</strong></p>

<p>[
G<em>t \doteq \sum</em>{k=0}^{T-t-1} \gamma^k R_{t+k+1} \quad (T = \infty \text{ or } \gamma = 1 \text{ (but not both)}) <br />
where <br />
T \ne \infty \text{ - case of episodic tasks} <br />
T = \infty \text{ - case of continuing tasks}
]</p>

<h2 id="决策过程视图">决策过程视图</h2>

<p>Reinforcement Learning - Markov Decision ProcessesReinforcement Learning - Markov Decision Processesssas-&gt;a a  a_2s-&gt;a_2 a_2  s_1s&rsquo;s_2s_2&rsquo;ra-&gt;rp(s&rsquo;|s,a)r_3a-&gt;r_3p(s_2&rsquo;|s,a)r_4a_2-&gt;r_4 r-&gt;s_1p(s&rsquo;,r|s,a),rr-&gt;s_1p(s&rsquo;,r&rsquo;|s,a),r&rsquo;r&rsquo;r_3-&gt;s_2 r_4-&gt;s_1</p>

<p>上图说明了：</p>

<ol>
<li><p>状态(s)下，采取行动(a)，转变成新状态(s&rsquo;)，是由概率(p(s&rsquo; | s, a))决定的。</p></li>

<li><p>状态(s)下，采取行动(a)，转变成新状态(s&rsquo;)，获得的奖赏(r)，是由概率(p(s&rsquo;, r | s, a))决定的。</p></li>

<li><p>引起状态(s)到状态(s&rsquo;)的转变行动，不一定是唯一的。</p></li>
</ol>

<h3 id="相应的数学定义和公式">相应的数学定义和公式</h3>

<p>在状态(s)下，执行行动(a)，转变为状态(s&rsquo;)并获得奖赏(r)的可能性：</p>

<p>[
p(s&rsquo;, r | s, a) \doteq Pr {S<em>{t+1} = s&rsquo;, R</em>{t+1} = r | S_t = s, A_t = a }
]</p>

<p>在状态(s)下，执行行动(a)的期望奖赏：</p>

<p>[
r(s,a) \doteq \mathbb{E}[R_{t+1} | S_t = s, A<em>t = a] = \sum</em>{r \in \mathcal{R} } r \sum_{s&rsquo; \in \mathcal{S} } p(s&rsquo;, r|s,a)
]</p>

<p>在状态(s)下，执行行动(a)，转变为状态(s&rsquo;)的可能性：</p>

<p>[
p(s&rsquo; | s,a) \doteq Pr {S_{t+1} = s&rsquo; | S_t=s, A<em>t=a } = \sum</em>{r \in \mathcal{R} } p(s&rsquo;,r | s,a)
]</p>

<p>在状态(s)下，执行行动(a)，转变为状态(s&rsquo;)的期望奖赏：</p>

<p>[
r(s,a,s&rsquo;) \doteq \mathbb{E}[R_{t+1} | S_t = s, A<em>t = a, S</em>{t+1} = s&rsquo;] = \frac{\sum_{r \in \mathcal{R} } r  p(s&rsquo;,r|s,a)}{p(s&rsquo;|s,a)}
]</p>

<h2 id="策略视图">策略视图</h2>

<p>强化学习的目标是找到（可以获得长期最优回报）的最佳策略。</p>

<p>(\pi) - 策略(policy)。</p>

<p>(\pi) - 策略(policy)。强化学习的目标：<strong>找到最优策略</strong>。</p>

<p>策略规定了状态(s)时，应该选择的行动(a)。</p>

<p>[
\pi = [\pi(s_1), \cdots, \pi(s_n)]
]</p>

<p>(\pi(s)) - 策略(\pi)在状态(s)下，选择的行动。</p>

<p>(\pi_*) - 最优策略(optimal policy)。</p>

<p>(\pi(a | s)) - <strong>随机策略</strong>(\pi)在状态(s)下，选择的行动(a)的概率。</p>

<h3 id="价值方法-value-functions">价值方法(Value Functions)</h3>

<p><strong>使用策略(\pi)，状态价值方法 - state-value function</strong></p>

<p>[
v_{\pi}(s) \doteq \mathbb{E}[G_t | S<em>t = s] = \mathbb{E}</em>{\pi} \left [ \sum<em>{k=0}^{\infty} \gamma^k R</em>{t+k+1} | S<em>t = s \right ] <br />
where <br />
\pi \text{ - polity} <br />
\mathbb{E}</em>{\pi}[\cdot] \text{ - the expected value of a value follows policy } \pi
]</p>

<p><strong>使用策略(\pi)，行动价值方法 - action-value function</strong></p>

<p>[
q_{\pi}(s,a) \doteq \mathbb{E}[G_t | S_t = s, A<em>t = a] = \mathbb{E}</em>{\pi} \left [ \sum<em>{k=0}^{\infty} \gamma^k R</em>{t+k+1} | S_t = s, A_t = a \right ] <br />
]</p>

<p><strong>使用策略(\pi)，迭代状态价值方法 - iterative state-value function</strong></p>

<p>a.k.a Bellman equation for (v_{\pi})</p>

<p>[
\begin{align}
v_{\pi}(s)
    &amp; \doteq \mathbb{E}[G_t | S<em>t = s] <br />
    &amp; = \mathbb{E}</em>{\pi} \left [ \sum<em>{k=0}^{\infty} \gamma^k R</em>{t+k+1} | S<em>t = s \right ] <br />
    &amp; = \mathbb{E}</em>{\pi} \left [ R<em>{t+1} + \gamma\sum</em>{k=0}^{\infty} \gamma^k R_{t+k+2} | S<em>t = s \right ] <br />
    &amp; = \sum</em>{a} \pi(a|s) \sum<em>{s&rsquo;} \sum</em>{r} p(s&rsquo;,r|s,a) \left [ r + \gamma\mathbb{E}<em>{\pi} \left [ \sum</em>{k=0}^{\infty} \gamma^k R<em>{t+k+2} | S</em>{t+1} = s&rsquo; \right ] \right ] <br />
    &amp; = \sum<em>{a} \pi(a|s) \sum</em>{s&rsquo;,r} p(s&rsquo;,r|s,a) \left [ r + \gamma v_{\pi}(s&rsquo;) \right ], \ \forall s \in \mathcal{S}
\end{align}
]</p>

<h3 id="最优价值方法-optimal-value-functions">最优价值方法(Optimal Value Functions)</h3>

<p><strong>最优状态价值方法 - optimal state-value function</strong></p>

<p>[
v<em>*(s) \doteq \underset{\pi}{max} \ v</em>{\pi}(s), \forall s \in \mathcal{S}
]</p>

<p><strong>最优行动价值方法 - optimal action-value function</strong></p>

<p>[
q<em>*(s, a) \doteq \underset{\pi}{max} \ q</em>{\pi}(s, a), \ \forall s \in \mathcal{S} \ and \ a \in \mathcal{A}(s)
]</p>

<p>最优的行为价值等于最优的状态价值下的最大期望：</p>

<p>[
q<em>*(s,a) = \mathbb{E}[R</em>{t+1} + \gamma v<em>* (S</em>{t+1}) \ | \ S_t = s, A_t = a]
]</p>

<p><strong>最优状态价值迭代方法 - interval optimal state-value function</strong></p>

<p>[
\begin{align}
v<em>*(s)
    &amp; = \underset{a \in \mathcal{A}(s)}{max} \ q</em>{\pi<em>*}(s, a) <br />
    &amp; = \underset{a}{max} \ \mathbb{E}</em>{\pi<em>} [G_t \ | \ S_t=s, A<em>t=a] <br />
    &amp; = \underset{a}{max} \ \mathbb{E}</em>{\pi</em>} \left [ \sum<em>{k=0}^{\infty} \gamma^k R</em>{t+k+1} \ | \ S_t=s, A<em>t=a \right ] <br />
    &amp; = \underset{a}{max} \ \mathbb{E}</em>{\pi<em>} \left [ R<em>{t+1} + \gamma\sum</em>{k=0}^{\infty} \gamma^k R_{t+k+2} \ | \ S_t=s, A<em>t=a \right ] <br />
    &amp; = \underset{a}{max} \ \mathbb{E}[R</em>{t+1} + \gamma v_</em>(S_{t+1}) \ | \ S_t=s, A<em>t=a ] <br />
    &amp; = \underset{a \in \mathcal{A}(s)}{max} \sum</em>{s&rsquo;,r} p(s&rsquo;,r|s,a)[r + \gamma v_*(s&rsquo;)]  <br />
\end{align}
]</p>

<hr />

<h1 id="comment">COMMENT</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">RL 数学符号</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/06-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/tl-%E6%B7%B1%E5%BA%A6%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">
            <span class="next-text nav-default">TL 深度迁移学习</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
