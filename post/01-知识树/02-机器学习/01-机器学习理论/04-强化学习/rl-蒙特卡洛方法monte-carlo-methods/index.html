<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>RL 蒙特卡洛方法(Monte Carlo Methods) - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="author: evo comments: true date: 2018-05-16 16:31:19&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods/ slug: rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods title: RL 蒙特卡洛方法(Monte Carlo Methods) wordpress_id: 5884 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95monte-carlo-methods/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="RL 蒙特卡洛方法(Monte Carlo Methods)" />
<meta property="og:description" content="author: evo comments: true date: 2018-05-16 16:31:19&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods/ slug: rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods title: RL 蒙特卡洛方法(Monte Carlo Methods) wordpress_id: 5884 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95monte-carlo-methods/" /><meta property="article:published_time" content="2018-06-11T08:14:52&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-11T08:14:52&#43;00:00"/>
<meta itemprop="name" content="RL 蒙特卡洛方法(Monte Carlo Methods)">
<meta itemprop="description" content="author: evo comments: true date: 2018-05-16 16:31:19&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods/ slug: rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods title: RL 蒙特卡洛方法(Monte Carlo Methods) wordpress_id: 5884 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推">


<meta itemprop="datePublished" content="2018-06-11T08:14:52&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-11T08:14:52&#43;00:00" />
<meta itemprop="wordCount" content="3012">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RL 蒙特卡洛方法(Monte Carlo Methods)"/>
<meta name="twitter:description" content="author: evo comments: true date: 2018-05-16 16:31:19&#43;00:00 layout: post link: http://106.15.37.116/2018/05/17/rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods/ slug: rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods title: RL 蒙特卡洛方法(Monte Carlo Methods) wordpress_id: 5884 categories: - 人工智能学习 tags: - NOT_ADD - Reinforcement Learning [mathjax] 注：非原创，只是按照自己的思路做了整合，修改。推"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">RL 蒙特卡洛方法(Monte Carlo Methods)</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-11 </span>
        
        <span class="more-meta"> 3012 words </span>
        <span class="more-meta"> 7 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#reinforcement-learning">- Reinforcement Learning</a></li>
</ul></li>
<li><a href="#original">ORIGINAL</a></li>
<li><a href="#强化学习读书笔记-05-蒙特卡洛方法-monte-carlo-methods-http-www-cnblogs-com-steven-yang-p-6507015-html"><a href="http://www.cnblogs.com/steven-yang/p/6507015.html">强化学习读书笔记 - 05 - 蒙特卡洛方法(Monte Carlo Methods)</a></a></li>
<li><a href="#todo">TODO</a></li>
<li><a href="#introduction">INTRODUCTION</a>
<ul>
<li><a href="#蒙特卡洛方法简话">蒙特卡洛方法简话</a></li>
<li><a href="#蒙特卡洛方法的基本思路">蒙特卡洛方法的基本思路</a></li>
<li><a href="#蒙特卡洛方法的使用条件">蒙特卡洛方法的使用条件</a></li>
<li><a href="#蒙特卡洛方法在强化学习中的用例">蒙特卡洛方法在强化学习中的用例</a></li>
<li><a href="#蒙特卡洛方法在强化学习中的基本思路">蒙特卡洛方法在强化学习中的基本思路</a></li>
<li><a href="#一些概念">一些概念</a></li>
<li><a href="#蒙特卡洛-起始点-exploring-starts-方法">蒙特卡洛（起始点（Exploring Starts））方法</a></li>
<li><a href="#on-policy-first-visit-蒙特卡洛方法-for-epsilon-soft-policies">On-policy first visit 蒙特卡洛方法（for (\epsilon)-soft policies）</a></li>
<li><a href="#off-policy-every-visit-蒙特卡洛方法">Off-policy every-visit 蒙特卡洛方法</a></li>
<li><a href="#总结">总结</a>
<ul>
<li><a href="#蒙特卡洛方法和动态规划的区别">蒙特卡洛方法和动态规划的区别</a></li>
<li><a href="#蒙特卡洛方法的优势">蒙特卡洛方法的优势</a></li>
<li><a href="#蒙特卡洛方法的劣势">　蒙特卡洛方法的劣势</a></li>
</ul></li>
<li><a href="#参照">参照</a></li>
</ul></li>
<li><a href="#comment">COMMENT</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<hr />

<p>author: evo
comments: true
date: 2018-05-16 16:31:19+00:00
layout: post
link: <a href="http://106.15.37.116/2018/05/17/rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods/">http://106.15.37.116/2018/05/17/rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods/</a>
slug: rl-%e8%92%99%e7%89%b9%e5%8d%a1%e6%b4%9b%e6%96%b9%e6%b3%95monte-carlo-methods
title: RL 蒙特卡洛方法(Monte Carlo Methods)
wordpress_id: 5884
categories:
- 人工智能学习
tags:
- NOT_ADD</p>

<h2 id="reinforcement-learning">- Reinforcement Learning</h2>

<!-- more -->

<p>[mathjax]</p>

<p><strong>注：非原创，只是按照自己的思路做了整合，修改。推荐直接看 ORIGINAL 中所列的原文。</strong></p>

<h1 id="original">ORIGINAL</h1>

<ol>
<li></li>
</ol>

<h1 id="强化学习读书笔记-05-蒙特卡洛方法-monte-carlo-methods-http-www-cnblogs-com-steven-yang-p-6507015-html"><a href="http://www.cnblogs.com/steven-yang/p/6507015.html">强化学习读书笔记 - 05 - 蒙特卡洛方法(Monte Carlo Methods)</a></h1>

<h1 id="todo">TODO</h1>

<ul>
<li>aaa</li>
</ul>

<hr />

<h1 id="introduction">INTRODUCTION</h1>

<ul>
<li>aaa</li>
</ul>

<p>学习笔记：</p>

<p><a href="http://incompleteideas.net/sutton/book/bookdraft2017june19.pdf/">Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto c 2014, 2015, 2016</a></p>

<p>数学符号看不懂的，先看看这里：</p>

<ul>
<li><a href="http://www.cnblogs.com/steven-yang/p/6481772.html">强化学习读书笔记 - 00 - 术语和数学符号</a></li>
</ul>

<h2 id="蒙特卡洛方法简话">蒙特卡洛方法简话</h2>

<p><strong>蒙特卡洛</strong>是一个赌城的名字。冯·诺依曼给这方法起了这个名字，增加其神秘性。</p>

<p>蒙特卡洛方法是一个计算方法，被广泛的用于许多领域，用于求值。</p>

<p>相对于确定性的算法，蒙特卡洛方法是基于抽样数据来计算结果。</p>

<h2 id="蒙特卡洛方法的基本思路">蒙特卡洛方法的基本思路</h2>

<p>蒙特卡洛方法的整体思路是：<strong>模拟 -&gt; 抽样 -&gt; 估值</strong>。</p>

<p><strong>示例：</strong></p>

<p>比如：如何求(\pi)的值。一个使用蒙特卡洛方法的经典例子如下：</p>

<p>我们知道一个直径为1的圆的面积为(\pi)。</p>

<p>把这个圆放到一个边长为2的正方形（面积为4）中，圆的面积和正方形的面积比是：(\frac{\pi}{4})。</p>

<p>如果可以测量出这个比值(c)，那么(\pi=c \times 4)。</p>

<p>如何测量比值(c)呢？用飞镖去扎这个正方形。扎了许多次后，用圆内含的小孔数除以正方形含的小孔数可以近似的计算比值(c)。</p>

<p><strong>说明：</strong></p>

<p>模拟 - 用飞镖去扎这个正方形为一次模拟。</p>

<p>抽样 - 数圆内含的小孔数和正方形含的小孔数。</p>

<p>估值 - 比值(c) = 圆内含的小孔数 / 正方形含的小孔数</p>

<h2 id="蒙特卡洛方法的使用条件">蒙特卡洛方法的使用条件</h2>

<ul>
<li>环境是可模拟的<br /></li>
</ul>

<p>在实际的应用中，模拟容易实现。相对的，了解环境的完整知识反而比较困难。</p>

<p>由于环境可模拟，我们就可以抽样。</p>

<ul>
<li>只适合情节性任务(episodic tasks)<br /></li>
</ul>

<p>因为，需要抽样完成的结果，只适合有限步骤的情节性任务。</p>

<h2 id="蒙特卡洛方法在强化学习中的用例">蒙特卡洛方法在强化学习中的用例</h2>

<p>只要满足蒙特卡洛方法的使用条件，就可以使用蒙特卡洛方法。</p>

<p>比如：游戏类都适合：完全信息博弈游戏，像围棋、国际象棋。非完全信息博弈游戏：21点、麻将等等。</p>

<h2 id="蒙特卡洛方法在强化学习中的基本思路">蒙特卡洛方法在强化学习中的基本思路</h2>

<p>蒙特卡洛方法的整体思路是：<strong>模拟 -&gt; 抽样 -&gt; 估值</strong>。</p>

<p>如何应用到强化学习中呢？</p>

<p>强化学习的目的是得到最优策略。</p>

<p>得到最优策略的一个方法是求(v<em>{pi}(s), \ q</em>{pi}{s, a})。 - 这就是一个<strong>求值问题</strong>。</p>

<p>结合通用策略迭代(GPI)的思想。</p>

<p>下面是蒙特卡洛方法的一个迭代过程：</p>

<ol>
<li><p>策略评估迭代</p></li>

<li><p>探索 - 选择一个状态(s, a)。</p></li>

<li><p>模拟 - 使用当前策略(\pi)，进行一次模拟，从当前状态(s, a)到结束，随机产生一段情节(episode)。</p></li>

<li><p>抽样 - 获得这段情节上的每个状态(s, a)的回报(G(s, a))，记录(G(s, a))到集合(Returns(s, a))。</p></li>

<li><p>估值 - q(s, a) = Returns(s, a)的平均值。</p></li>
</ol>

<p>（因为状态(s, a)可能会被多次选择，所以状态(s, a)有一组回报值。）</p>

<ol>
<li>策略优化 - 使用新的行动价值(q(s, a))优化策略(\pi(s))。</li>
</ol>

<p><strong>解释</strong></p>

<ul>
<li>上述的策略评估迭代步骤，一般会针对所有的状态-行动，或者一个起始((s_0, a_0))下的所有状态-行动。<br /></li>
</ul>

<p>这也说明<strong>持续探索（continual exploration）是蒙特卡洛方法的主题</strong>。</p>

<ul>
<li>模拟过程 - 会模拟到结束。是前进式的，随机选择下一个行动，一直前进到结束为止。<br /></li>
</ul>

<p>因此可以看出蒙特卡洛方法需要大量的迭代，才能正确的找到最优策略。</p>

<ul>
<li>策略评估是计算行动价值((q(s, a)))。<br /></li>
</ul>

<p>(也可以是状态价值，则(\pi(s))为状态(s)到其下一个最大价值状态(s‘)的任意行动。)</p>

<p>计算方法：</p>

<p>[
q(s, a) = average(Returns(s, a))
]</p>

<h2 id="一些概念">一些概念</h2>

<ul>
<li>Exploring Starts 假设 - 指有一个探索起点的环境。<br /></li>
</ul>

<p>比如：围棋的当前状态就是一个探索起点。自动驾驶的汽车也许是一个没有起点的例子。</p>

<ul>
<li><p>first-visit - 在一段情节中，一个状态只会出现一次，或者只需计算第一次的价值。</p></li>

<li><p>every-visit - 在一段情节中，一个状态可能会被访问多次，需要计算每一次的价值。</p></li>

<li><p>on-policy method - 评估和优化的策略和模拟的策略是同一个。</p></li>

<li><p>off-policy method - 评估和优化的策略和模拟的策略是不同的两个。</p></li>
</ul>

<p>有时候，模拟数据来源于其它处，比如：已有的数据，或者人工模拟等等。</p>

<ul>
<li><p>target policy - 目标策略。off policy method中，需要优化的策略。</p></li>

<li><p>behavior policy - 行为策略。off policy method中，模拟数据来源的策略。</p></li>
</ul>

<p>根据上面的不同情境，在强化学习中，提供了不同的蒙特卡洛方法。</p>

<ul>
<li><p>蒙特卡洛（起始点（Exploring Starts））方法</p></li>

<li><p>On-policy first visit 蒙特卡洛方法（for (\epsilon)-soft policies）</p></li>

<li><p>Off-policy every-visit 蒙特卡洛方法</p></li>
</ul>

<h2 id="蒙特卡洛-起始点-exploring-starts-方法">蒙特卡洛（起始点（Exploring Starts））方法</h2>

<blockquote>

> 
> Initialize, for all \(s \in \mathcal{S}, \ a \in \mathcal{A}(s)\):  

 \(Q(s,a) \gets\) arbitrary  

 \(\pi(s) \gets\) arbitrary  

 \(Returns(s, a) \gets\) empty list
> 
> 

> 
> Repeat forever:  

 Choose \(S_0 \in \mathcal{S}\) and \(A_0 \in \mathcal{A}(S_0)\) s.t. all pairs have probability > 0  

 Generate an episode starting from \(S_0, A_0\), following \(\pi\)  

 For each pair \(s,a\) appearing in the episode:  

   $G \gets $ return following the first occurrence of s,a  

   Append \(G\) to \(Returns(s, a)\)  

   \(Q(s, a) \gets average(Returns(s, a))\)  

 For each s in the episode:  

   \(\pi(s) \gets \underset{a}{argmax} Q(s,a)\)
> 
> 
</blockquote>

<h2 id="on-policy-first-visit-蒙特卡洛方法-for-epsilon-soft-policies">On-policy first visit 蒙特卡洛方法（for (\epsilon)-soft policies）</h2>

<blockquote>

> 
> Initialize, for all \(s \in \mathcal{S}, \ a \in \mathcal{A}(s)\):  

  \(Q(s,a) \gets\) arbitrary  

  \(\pi(a|s) \gets\) an arbitrary \(\epsilon\)-soft policy  

  \(Returns(s, a) \gets\) empty list
> 
> 

> 
> Repeat forever:  

  (a) Generate an episode using \(\pi\)  

  (b) For each pair \(s,a\) appearing in the episode:  

   $G \gets $ return following the first occurrence of s,a  

   Append \(G\) to \(Returns(s, a)\)  

   \(Q(s, a) \gets average(Returns(s, a))\)  

  (c) For each s in the episode:  

   \(A^* \gets \underset{a}{argmax} \ Q(s,a)\)  

   For all \(a \in \mathcal{A}(s)\):  

    if \(a = A^*\)  

     \(\pi(a|s) \gets 1 - \epsilon + \frac{\epsilon}{|\mathcal{A}(s)|}\)  

    if \(a \ne A^*\)  

     \(\pi(a|s) \gets \frac{\epsilon}{|\mathcal{A}(s)|}\)
> 
> 
</blockquote>

<h2 id="off-policy-every-visit-蒙特卡洛方法">Off-policy every-visit 蒙特卡洛方法</h2>

<blockquote>

> 
> Initialize, for all \(s \in \mathcal{S}, \ a \in \mathcal{A}(s)\):  

  \(Q(s,a) \gets\) arbitrary  

  \(C(s,a) \gets\) 0  

  \(\mu(a|s) \gets\) an arbitrary soft behavior policy  

  \(\pi(a|s) \gets\) a deterministic policy that is greedy with respect to Q
> 
> 

> 
> Repeat forever:  

  Generate an episode using \(\mu\):  

   \(S_0,A_0,R_1,\cdots,S_{T-1},A_{T-1},R_T,S_T\)  

  \(G \gets 0\)  

  \(W \gets 1\)  

  For t = T - 1 downto 0:  

   \(G \gets \gamma G + R_{t+1}\)  

   \(C(S_t, A_t) \gets C(S_t, A_t) + W\)  

   \(Q(S_t, A_t) \gets Q(S_t, A_t) + \frac{W}{C(S_t, A_t)} |G - Q(S_t, A_t)|\)  

   \(\pi(S_t) \gets \underset{a}{argmax} \ Q(S_t, a)\) (with ties broken consistently)  

   If \(A_t \ne \pi(S_t)\) then ExitForLoop  

   \(W \gets W \frac{1}{\mu(A_t|S_t)}\)
> 
> 
</blockquote>

<h2 id="总结">总结</h2>

<h3 id="蒙特卡洛方法和动态规划的区别">蒙特卡洛方法和动态规划的区别</h3>

<ol>
<li>动态规划是基于模型的，而蒙特卡洛方法是无模型的。</li>
</ol>

<blockquote>

> 
> 注：基于模型(model-base)还是无模型(model-free)是看(状态或者行动)价值(\(G, v(s), q(s,a)\))是如何得到的？  

如果是已知的、根据已知的数据计算出来的，就是基于模型的。  

如果是取样得到的、试验得到的，就是无模型的。
> 
> 
</blockquote>

<ol>
<li>动态规划的计算的，而蒙特卡洛方法的计算是取样性的(sampling)。</li>
</ol>

<blockquote>

> 
> 注：引导性的(bootstrapping)还是取样性的(sampling)是看(状态或者行动)价值(\(G, v(s), q(s,a)\))是如何计算的？  

如果是根据其它的价值计算的，就是引导性的。  

如果是通过在实际环境中模拟的、取样的，就是取样性的。  

引导性和取样性并不是对立的。可以是取样的，并且是引导的。  

如果价值是根据其它的价值计算的，但是有部分值（比如：奖赏）是取样得到的，就是无模型、取样的、引导性的。
> 
> 
</blockquote>

<p><strong>解释：</strong></p>

<p>上面两个区别，可以从计算状态价值(v<em>{\pi}(s), q</em>{\pi}(s, a))的过程来看：</p>

<p>动态规划是从初始状态开始，一次计算一步可能发生的所有状态价值，然后迭代计算下一步的所有状态价值。这就是引导性。</p>

<p>蒙特卡洛方法是从初始状态开始，通过在实际环境中模拟，得到一段情节（从头到结束）。</p>

<p>比如，如果结束是失败了，这段情节上的状态节点，本次价值都为0,；如果成功了，本次价值都为1。</p>

<p>下面的比喻（虽然不太恰当，但是比较形象）</p>

<p>想象一棵树，动态规划是先算第一层的所有节点价值，然后算第二层的所有节点价值。</p>

<p>蒙特卡洛方法，随便找一个从根到叶子的路径。根据叶子的值，计算路径上每个节点价值。</p>

<p>可以看出蒙特卡洛方法比较方便。</p>

<h3 id="蒙特卡洛方法的优势">蒙特卡洛方法的优势</h3>

<ul>
<li>蒙特卡洛方法可以从交互中直接学习优化的策略，而不需要一个环境的动态模型。<br /></li>
</ul>

<p>环境的动态模型 - 似乎表示环境的状态变化是可以完全推导的。表明了解环境的所有知识。</p>

<p>说白了，就是可以计算(v(s), q(s, a))这意味着必须了解所有状态变化的可能性。</p>

<p>蒙特卡洛方法只需要一些（可能是大量的）取样就可以。</p>

<ul>
<li><p>蒙特卡洛方法可以用于模拟（样本）模型。</p></li>

<li><p>蒙特卡洛方法可以只考虑一个小的状态子集。</p></li>

<li><p>蒙特卡洛方法的每个状态价值计算是独立的。不会影响其他的状态价值。</p></li>
</ul>

<h3 id="蒙特卡洛方法的劣势">　蒙特卡洛方法的劣势</h3>

<ul>
<li><p>需要大量的探索（模拟）。</p></li>

<li><p>基于概率的，不是确定性的。</p></li>
</ul>

<h2 id="参照">参照</h2>

<hr />

<h1 id="comment">COMMENT</h1>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">RL 动态规划</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E8%A7%84%E5%88%92%E5%BC%8F%E6%96%B9%E6%B3%95%E5%92%8C%E5%AD%A6%E4%B9%A0%E5%BC%8F%E6%96%B9%E6%B3%95/">
            <span class="next-text nav-default">RL 规划式方法和学习式方法</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
