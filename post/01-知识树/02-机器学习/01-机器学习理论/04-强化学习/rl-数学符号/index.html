<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>RL 数学符号 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="RL 数学符号 基本概念 Agent - 本体。学习者、决策者。 Environment - 环境。本体外部的一切。 (s) - 状态(state)。一个表示环境的数据。 (S, \mathcal{S}) - 所有状态集合。环境中" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="RL 数学符号" />
<meta property="og:description" content="RL 数学符号 基本概念 Agent - 本体。学习者、决策者。 Environment - 环境。本体外部的一切。 (s) - 状态(state)。一个表示环境的数据。 (S, \mathcal{S}) - 所有状态集合。环境中" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7/" /><meta property="article:published_time" content="2018-08-21T18:16:22&#43;00:00"/>
<meta property="article:modified_time" content="2018-08-21T18:16:22&#43;00:00"/>
<meta itemprop="name" content="RL 数学符号">
<meta itemprop="description" content="RL 数学符号 基本概念 Agent - 本体。学习者、决策者。 Environment - 环境。本体外部的一切。 (s) - 状态(state)。一个表示环境的数据。 (S, \mathcal{S}) - 所有状态集合。环境中">


<meta itemprop="datePublished" content="2018-08-21T18:16:22&#43;00:00" />
<meta itemprop="dateModified" content="2018-08-21T18:16:22&#43;00:00" />
<meta itemprop="wordCount" content="2585">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RL 数学符号"/>
<meta name="twitter:description" content="RL 数学符号 基本概念 Agent - 本体。学习者、决策者。 Environment - 环境。本体外部的一切。 (s) - 状态(state)。一个表示环境的数据。 (S, \mathcal{S}) - 所有状态集合。环境中"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">RL 数学符号</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-08-21 </span>
        
        <span class="more-meta"> 2585 words </span>
        <span class="more-meta"> 6 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#rl-数学符号">RL 数学符号</a>
<ul>
<li><a href="#基本概念">基本概念</a></li>
<li><a href="#策略">策略</a></li>
<li><a href="#近似计算">近似计算</a></li>
<li><a href="#老o虎o机问题">老O虎O机问题</a></li>
<li><a href="#通用数学符号">通用数学符号</a></li>
<li><a href="#术语">术语</a></li>
<li><a href="#参照">参照</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h1 id="rl-数学符号">RL 数学符号</h1>

<h2 id="基本概念">基本概念</h2>

<p>Agent - 本体。学习者、决策者。</p>

<p>Environment - 环境。本体外部的一切。</p>

<p>(s) - 状态(state)。一个表示环境的数据。</p>

<p>(S, \mathcal{S}) - 所有状态集合。环境中所有的可能状态。</p>

<p>(a) - 行动(action)。本体可以做的动作。</p>

<p>(A, \mathcal{A}) - 所有行动集合。本体可以做的所有动作。</p>

<p>(A(s), \mathcal{A}(s)) - 状态(s)的行动集合。本体在状态(s)下，可以做的所有动作。</p>

<p>(r) - 奖赏(reward)。本体在一个行动后，获得的奖赏。</p>

<p>(\mathcal{R}) - 所有奖赏集合。本体可以获得的所有奖赏。</p>

<p>(S_t) - 第t步的状态(state)。(t) from 0</p>

<p>(A_t) - 第t步的行动(select action)。(t) from 0</p>

<p>(R_t) - 第t步的奖赏(reward)。(t) from 1</p>

<p>(G_t) - 第t步的长期回报(return)。(t) from 0。 <strong>强化学习的目标1：追求最大回报</strong></p>

<p>[
G<em>t \doteq \sum</em>{k=0}^{\infty} \gamma^k R_{t+k+1} <br />
where <br />
k \text{ - the sequence number of an action.} <br />
\gamma \text{ - discount rate,} \ 0 \leqslant \gamma \leqslant 1
]</p>

<p>可以看出，当(\gamma=0)时，只考虑当前的奖赏。当(\gamma=1)时，未来的奖赏没有损失。</p>

<p>(G_t^{(n)}) - 第t步的n步回报(n-step return)。。一个回报的近似算法。</p>

<p>[
G<em>t^{(n)} \doteq \sum</em>{k=0}^{n} \gamma^k R_{t+k+1} <br />
where <br />
k \text{ - the sequence number of an action.} <br />
\gamma \text{ - discount rate,} \ 0 \leqslant \gamma \leqslant 1
]</p>

<p>(G_t^{\lambda}) - 第t步的(\lambda)回报((\lambda)-return)。一个回报的近似算法。可以说是(G_t^{(n)})的优化。</p>

<p>[
\text{Continuing tasks: } <br />
G<em>t^{\lambda} \doteq (1 - \lambda) \sum</em>{n=1}^{\infty} \lambda^{n-1}G_t^{(n)} <br />
\text{Episodic tasks: } <br />
G<em>t^{\lambda} \doteq (1 - \lambda) \sum</em>{n=1}^{T-t-1} \lambda^{n-1}G_t^{(n)} + \lambda^{T-t-1}G<em>t <br />
where <br />
\lambda \in [0, 1] <br />
(1 - \lambda) \sum</em>{n=1}^{\infty}\lambda^{n-1} = 1 <br />
(1 - \lambda) \sum_{n=1}^{T-t-1} \lambda^{n-1} + \lambda^{T-t-1} = 1 <br />
\text{if } \lambda = 0, \text{become to 1-step TD algorithm}<br />
\text{if } \lambda = 1, \text{become to Monte Carlo algorithm} <br />
]</p>

<h2 id="策略">策略</h2>

<p>(\pi) - 策略(policy)。<strong>强化学习的目标2：找到最优策略</strong>。</p>

<p>策略规定了状态(s)时，应该选择的行动(a)。</p>

<p>[
\pi = [\pi(s_1), \cdots, \pi(s_n)]
]</p>

<p>(\pi(s)) - 策略(\pi)在状态(s)下，选择的行动。</p>

<p>(\pi_*) - 最优策略(optimal policy)。</p>

<p>(\pi(a | s)) - <strong>随机策略</strong>(\pi)在状态(s)下，选择的行动(a)的概率。</p>

<p>(r(s, a)) - 在状态(s)下，选择行动(a)的奖赏。</p>

<p>(r(s, a， s&rsquo;)) - 在状态(s)下，选择行动(a)，变成(状态(s‘))的奖赏。</p>

<p>(p(s′, r | s, a)) - (状态(s)、行动(a))的前提下，变成(状态(s‘)、奖赏(r))的概率。</p>

<p>(p(s′ | s, a)) - (状态(s)、行动(a))的前提下，变成(状态(s‘))的概率。</p>

<p>(v_{\pi}(s)) - 状态价值。使用策略(\pi)，（状态(s)的）长期奖赏(G_t)。</p>

<p>(q_{\pi}(s, a)) - 行动价值。使用策略(\pi)，（状态(s)，行动(a)的）长期奖赏(G_t)。</p>

<p>(v_{*}(s)) - 最佳状态价值。</p>

<p>(q_{*}(s, a)) - 最佳行动价值。</p>

<p>(V(s)) - (v_{\pi}(s))的集合。</p>

<p>(Q(s, a)) - (q_{\pi}(s, a))的集合。</p>

<p>[
\text{For continuing tasks: } <br />
G<em>t \doteq \sum</em>{k=0}^{\infty} \gamma^k R_{t+k+1} <br />
\text{For episodic tasks: } <br />
G<em>t \doteq \sum</em>{k=0}^{T-t-1} \gamma^k R<em>{t+k+1} <br />
v</em>{\pi}(s) \doteq \mathbb{E}_{\pi} [G_t | S<em>t=s] = \mathbb{E}</em>{\pi} \left [ \sum<em>{k=0}^{\infty} \gamma^k R</em>{t+k+1}|S<em>t = s \right ] <br />
q</em>{\pi}(s,a) \doteq \mathbb{E}_{\pi} [G_t | S_t=s,A<em>t=a] = \mathbb{E}</em>{\pi} \left [ \sum<em>{k=0}^{\infty} \gamma^k
R</em>{t+k+1}|S_t = s, A<em>t=a \right ] <br />
v</em>{\pi}(s) = \max<em>{a \in \mathcal{A} } q</em>{\pi}(s,a) <br />
\pi(s) = \underset{a}{argmax} \ v<em>{\pi}(s&rsquo; | s, a) <br />
\pi(s) \text{ is the action which can get the next state which has the max value.} <br />
\pi(s) = \underset{a}{argmax} \ q</em>{\pi}(s, a) <br />
\pi(s) \text{ is the action which can get the max action value from the current state.} <br />
]</p>

<p>由上面的公式可以看出：(\pi(s))可以由(v<em>{\pi}(s))或者(q</em>{\pi}(s,a))决定。</p>

<p>[
\text{Reinforcement Learning} \doteq \pi<em>* <br />
\quad \updownarrow <br />
\pi</em>* \doteq { \pi(s) }, \ s \in \mathcal{S} <br />
\quad \updownarrow <br />
\begin{cases}
\pi(s) = \underset{a}{argmax} \ v<em>{\pi}(s&rsquo; | s, a), \ s&rsquo; \in S(s), \quad \text{or} <br />
\pi(s) = \underset{a}{argmax} \ q</em>{\pi}(s, a) <br />
\end{cases} <br />
\quad \updownarrow <br />
\begin{cases}
v<em>*(s), \quad \text{or} <br />
q</em>*(s, a) <br />
\end{cases} <br />
\quad \updownarrow <br />
\text{approximation cases:} <br />
\begin{cases}
\hat{v}(s, \theta) \doteq \theta^T \phi(s), \quad \text{state value function} <br />
\hat{q}(s, a, \theta) \doteq \theta^T \phi(s, a), \quad \text{action value function} <br />
\end{cases} <br />
where <br />
\theta \text{ - value function&rsquo;s weight vector} <br />
]</p>

<p><strong>强化学习的目标3：找到最优价值函数(v<em>*(s))或者(q</em>*(s,a))</strong>。</p>

<h2 id="近似计算">近似计算</h2>

<p><strong>强化学习的目标4：找到最优近似价值函数(\hat{v}(S_t, \theta_t))或者(\hat{q}(S_t, A_t, \theta_t))</strong>。</p>

<p><strong>强化学习的目标5：找到求解(\theta)</strong>。</p>

<p>(\rho_t^k) - importance sampling ratio for time t to time k - 1。</p>

<p>(\mathcal{J}(s)) - 状态(s)被访问的步骤序号。</p>

<p>(\theta) - 近似价值函数的权重向量。</p>

<p>(\phi(s)) - 近似价值函数的特征函数。是一个将状态(s)转化成计算向量的方法。这个结果和(\theta)组成近似价值函数。</p>

<p>(\hat{v}(S_t, \theta_t)) - 近似状态价值函数。</p>

<p>[
\hat{v} \doteq \theta^T \phi(s)
]</p>

<p>(\hat{q}(S_t, A_t, \theta_t)) - 近似行动价值函数。</p>

<p>[
\hat{q} \doteq \theta^T \phi(s,a)
]</p>

<p>(e_t) - 第t步资格迹向量(eligibility trace rate)。可以理解为近似价值函数微分的优化值。</p>

<p>[
e_0 \doteq 0 <br />
e_t \doteq \nabla \hat{v}(S_t, \theta<em>t) + \gamma \lambda e</em>{t-1} <br />
\theta_t \doteq \theta_t + \alpha \delta_t e_t
]</p>

<p>(\alpha) - 学习步长。(\alpha \in (0, 1))</p>

<p>(\gamma) - 未来回报的折扣率(discount rate)。(\gamma \in [0, 1])</p>

<p>(\lambda) - (\lambda)-return中的比例参数。(\lambda \in [0, 1])</p>

<p>h（horizon）- 水平线h表示on-line当时可以模拟的数据步骤。(t &lt; h \le T)</p>

<h2 id="老o虎o机问题">老O虎O机问题</h2>

<p>(q_*(a)) - 行动 a 的真实奖赏(true value)。这个是（实际中）不可知的。期望计算的结果收敛(converge)与它。</p>

<p>(N_t(a)) - 在第t步之前，行动a被选择的次数。</p>

<p>(Q_t(a)) - 行动 a 在第t步前（不包括第t步）的实际平均奖赏。</p>

<p>[
Q<em>t(a) = \frac{\sum</em>{i=1}^{t-1} R<em>i \times 1</em>{A_i=a} }{N_t(a)}
]</p>

<p>(H_t(a)) - 对于行动a的学习到的倾向(reference)。</p>

<p>(\epsilon) - 在ε-贪婪策略中，采用随机行动的概率([0, 1))。</p>

<h2 id="通用数学符号">通用数学符号</h2>

<p>(\doteq) - 定义上的等价关系。</p>

<p>(\mathbb{E}[X]) - (X)的期望值。</p>

<p>(Pr{X = x}) - 变量(X)值为(x)的概率。</p>

<p>(v \mapsto g) - v渐近g。</p>

<p>(v \approx g) - v约等于g。</p>

<p>(\mathbb{R}) - 实数集合。</p>

<p>(\mathbb{R}^n) - n个元素的实数向量。</p>

<p>(\underset{a \in \mathcal{A} }{max} \ F(a)) - 在所有的行动中，求最大值(F(a))。</p>

<p>(\underset{c}{argmax} \ F&copy;) - 求当F&copy;为最大值时，参数(c)的值。</p>

<h2 id="术语">术语</h2>

<p>episodic tasks - 情节性任务。指（强化学习的问题）会在有限步骤下结束。</p>

<p>continuing tasks - 连续性任务。指（强化学习的问题）有无限步骤。</p>

<p>episode - 情节。指从起始状态（或者当前状态）到结束的所有步骤。</p>

<p>tabular method - 列表方法。指使用了数组或者表格存储每个状态（或者状态-行动）的信息（比如：其价值）。</p>

<p>planning method - 计划性方法。需要一个模型，在模型里，可以获得状态价值。比如： 动态规划。</p>

<p>learning method - 学习性方法。不需要模型，通过模拟（或者体验），来计算状态价值。比如：蒙特卡洛方法，时序差分方法。</p>

<p>on-policy method - on-policy方法。评估的策略和优化的策略是同一个。</p>

<p>off-policy method - off-policy方法。评估的策略和优化的策略不是同一个。意味着优化策略使用来自外部的样本数据。</p>

<p>target policy - 目标策略。off-policy方法中需要优化的策略。</p>

<p>behavior policy - 行为策略(\mu)。off-policy方法中提供样本数据的策略。</p>

<p>importance sampling - 行为策略(\mu)的样本数据。</p>

<p>importance sampling rate - 由于目标策略(\pi)和行为策略(\mu)不同，导致样本数据在使用上的加权值。</p>

<p>ordinary importance sampling - 无偏见的计算策略价值的方法。</p>

<p>weighted importance sampling - 有偏见的计算策略价值的方法。</p>

<p>MSE(mean square error) - 平均平方误差。</p>

<p>MDP(markov decision process) - 马尔科夫决策过程</p>

<p>The forward view - We decide how to update each state by looking forward to future rewards and states.</p>

<p>例如：</p>

<p>[
G<em>t^{(n)} \doteq R</em>{t+1} + \gamma R<em>{t+2} + \dots + \gamma^{n-1} R</em>{t+n} + \gamma^n \hat{v}(S<em>{t+n}, \theta</em>{t+n-1}) , \ 0 \le t \le T-n <br />
]</p>

<p>The backward or mechanistic view - Each update depends on the current TD error combined with eligibility traces of past events.</p>

<p>例如：</p>

<p>[
e_0 \doteq 0 <br />
e_t \doteq \nabla \hat{v}(S_t, \theta<em>t) + \gamma \lambda e</em>{t-1} <br />
]</p>

<h2 id="参照">参照</h2>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">RL 强化学习总结</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl-%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/">
            <span class="next-text nav-default">RL 有限马尔科夫决策过程</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
