<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>06 模仿学习 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="模仿学习 亦称“学徒学习” (apprenticeship learning), “示范学习” (learning from demonstration), “观 察学习” (learning by watching);与机器学习早 期的“示教学习”有直接 联系，参见1.5节. 在强" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/06-%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="06 模仿学习" />
<meta property="og:description" content="模仿学习 亦称“学徒学习” (apprenticeship learning), “示范学习” (learning from demonstration), “观 察学习” (learning by watching);与机器学习早 期的“示教学习”有直接 联系，参见1.5节. 在强" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/01-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/04-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/06-%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/" /><meta property="article:published_time" content="2018-07-05T21:33:08&#43;00:00"/>
<meta property="article:modified_time" content="2018-07-05T21:33:08&#43;00:00"/>
<meta itemprop="name" content="06 模仿学习">
<meta itemprop="description" content="模仿学习 亦称“学徒学习” (apprenticeship learning), “示范学习” (learning from demonstration), “观 察学习” (learning by watching);与机器学习早 期的“示教学习”有直接 联系，参见1.5节. 在强">


<meta itemprop="datePublished" content="2018-07-05T21:33:08&#43;00:00" />
<meta itemprop="dateModified" content="2018-07-05T21:33:08&#43;00:00" />
<meta itemprop="wordCount" content="2467">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="06 模仿学习"/>
<meta name="twitter:description" content="模仿学习 亦称“学徒学习” (apprenticeship learning), “示范学习” (learning from demonstration), “观 察学习” (learning by watching);与机器学习早 期的“示教学习”有直接 联系，参见1.5节. 在强"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">06 模仿学习</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-07-05 </span>
        
        <span class="more-meta"> 2467 words </span>
        <span class="more-meta"> 5 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#comment">COMMENT</a></li>
<li><a href="#ref">REF</a></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<p>模仿学习</p>

<p>亦称“学徒学习”</p>

<p>(apprenticeship learning), “示范学习” (learning from demonstration), “观</p>

<p>察学习” (learning by watching);与机器学习早 期的“示教学习”有直接 联系，参见1.5节.</p>

<p>在强化学习的经典任务设置中，机器所能获得的反馈信息仅有多步决策后 的累积奖赏，但在现实任务中，往往能得到人类专家的决策过程范例，例如在种 瓜任务上能得到农业专家的种植过程范例.从这样的范例中学习，称为“模仿 学习” (imitation learning).</p>

<p>16.6.1直接模仿学习</p>

<p>强化学习任务中多步决策的搜索空间巨大，基于累积奖赏来学习很多步之 前的合适决策非常困难，而直接模仿人类专家的“状态-动作对”可显著缓解这 一困难，我们称其为“直接模仿学习”.</p>

<p>假定我们获得了一批人类专家的决策轨迹数据｛Tl,T2,&hellip;5Tm｝5每条轨迹 包含状态和动作序列</p>

<p>Ti =    s2&gt; a2? • • • 5 4+1〉，</p>

<p>其中％为第S条轨迹中的转移次数.</p>

<p>有了这样的数据，就相当于告诉机器在什么状态下应选择什么动作，于是 可利用监督学习来学得符合人类专家决策轨迹数据的策略.</p>

<p>我们可将所有轨迹上的所有“状态-动作对”抽取出来，构造出一个新的数 据集合</p>

<p>D = ｛（Sl5 ai）5 （s2, «2）, . • •,    ）
即把状态作为特征，动作作为标记；然后，对这个新构造出的数据集合I）使用 分类（对于离散动作）或回归（对于连续动作）算法即可学得策略模型.学得的这 个策略模型可作为机器进行强化学习的初始策略，再通过强化学习方法基于环 境反馈进行改进，从而获得更好的策略.</p>

<p>16.6.2逆强化学习</p>

<p>在很多任务中，设计奖赏函数往往相当困难，从人类专家提供的范例数据 中反推出奖赏函数有助于解决该问题，这就是逆强化学习（inverse reinforcement learning） [Abbeel and Ng, 2004].</p>

<p>在逆强化学习中，我们知道状态空间X、动作空间式并且与直接模仿学 习类似,有一个决策轨迹数据集逆强化学习的基本思想是：欲 使机器做出与范例一致的行为，等价于在某个奖赏函数的环境中求解最优策略, 该最优策略所产生的轨迹与范例数据一致.换言之，我们要寻找某种奖赏函数 使得范例数据是最优的，然后即可使用这个奖赏函数来训练强化学习策略.</p>

<p>不妨假设奖赏函数能表达为状态特征的线性函数，即= wT®.于是， 策略7T的累积奖赏可写为</p>

<p>&rsquo;+OO    &lsquo;</p>

<p>=E</p>

<p>■+oo    ■
y | 7r</p>

<p>=wtE</p>

<p>&rdquo;+OO</p>

<p>_t=0</p>

<p>7T</p>

<p>(16.37)</p>

<p>即状态向量加权和的期望与系数w的内积.</p>

<p>将状态向量的期望E 简写为注意到获得，需求取期 望.我们可使用蒙特卡罗方法通过采样来近似期望，而范例轨迹数据集恰可看 作最优策略的一个采样，于是，可将每条范例轨迹上的状态加权求和再平均，记 为，.对于最优奖赏函数E(x) = w^Tx和任意其他策略产生的^r，有</p>

<p>w<em>Tac</em> - w*T^ =    &gt; 0 .    (16.38)
若能对所有策略计算出即可解出</p>

<p>w* = arg max minwT(»* — x^)    (16.39)</p>

<p>w    冗</p>

<p>s.t. ||w||    1</p>

<p>显然，我们难以获得所有策略,一个较好的办法是从随机策略开始，迭代地 求解更好的奖赏函数，基于奖赏函数获得更好的策略，直至最终获得最符合范 例轨迹数据集的奖赏函数和策略，如图16.15算法所示.注意在求解更好的奖 赏函数时，需将式(16.39)中对所有策略求最小改为对之前学得的策略求最小.</p>

<p>输入：环境均</p>

<p>状态空间X;</p>

<p>动作空间次</p>

<p>范例轨迹数据集D = {ti,T2, &hellip; , Tm}.</p>

<p>过程：</p>

<p>1:念* =从范例轨迹中算出状态加权和的均值向量；</p>

<p>2： 7T =隨机策略；</p>

<p>3: for 4 = 1,2,&hellip; do</p>

<p>4：    =从7T的采样轨迹算出状态加权和的均值向量；</p>

<p>5： 求解 w* = argmaxw min<em>=1 wT(念</em> — xf) s.t. ||w||    1;</p>

<p>6：    % =在环境〈X, A, R{x) = w*Ta?)中求癖最优策略；</p>

<p>7： end for</p>

<p>输出：奖赏函数R(x) = w*Tx与策略TT</p>

<p>图16.15迭代式逆强化学习算法</p>

<hr />

<h1 id="comment">COMMENT</h1>

<p>强化学习专门书籍中最著名的是［Sutton and Barto, 1998］. ［Gosavi, 2003］ 从优化的角度来讨论强化学习，［Whiteson, 2010］则侧重于介绍基于演化算法 搜索的强化学习方法.［Mausam and Kolobov, 2012］从马尔可夫决策过程的视 角介绍强化学习，［Sigaud and Buffet, 2010］覆盖了很多内容，包括本章未介绍 的部分可观察马尔可夫决策过程(Partially Observable MDP,简称POMDP)、 策略梯度法等.基于值函数近似的强化学习可参阅［Busoniu et al., 2010］.</p>

<p>欧洲强化学习研讨会(EWRL)是专门性的强化学习系列研讨会，多学科强 化学习与决策会议(RLDM)则是从2013年开始的新会议.</p>

<p>［Kaelbling et al., 1996］是一个较早的强化学习综述，［Kober et al., 2013; Deisenroth et al., 2013］则综述了强化学习在机器人领域的应用.</p>

<p>“后悔”(regret)是指在 不确定性条件下的决策与 确定性条件下的决策所获 得的奖赏间的差别.</p>

<p>Samuel跳棋工作参见</p>

<p>p.22.</p>

<p>［Kuleshov and Precup, 2000］和［Vermorel and Mohri, 2005］介绍了 多种 尺-摇臂赌博机算法并进行了比较.多摇臂赌博机模型在统计学领域有大量研 究［Berry and Fristedt, 1985］,近年来在“在线学习” (online learning)、“对 抗学习 ” (adversarial learning)等方面有广泛应用，［Bubeck and Cesa-Bianchi, 2012］对其“悔界”(regret bound)分析方面的结果进行了综述.</p>

<p>时序差分(TD)学习最早是A. Samuel在他著名的跳棋工作中提出, ［Sutton, 1988］提出了 TD(A)算法，由于［Tesauro, 1995］基于 TD(A)研制的 TD-Gammon程序在西洋双陆棋上达到人类世界冠军水平而使TD学习备受 关注.Q-学习算法是［Watkins and Dayan, 1992］提出，Sarsa则是在Q-学习算 法基础上的改进［Rummery and Niranjan, 1994］. TD学习近年来仍有改进和 推广，例如广义TD学习［Ueno et al., 2011］＞使用资格迹(eligibility traces)的 TD 学习［Geist and Scherrer, 2014］等.［Dann et al., 2014］对 TD 学习中的策略 评估方法进行了比较.</p>

<p>模仿学习被认为是强化学习提速的重要手段［Lin, 1992; Price and Boutili-er, 2003］,在机器人领域被广泛使用［Argali et al., 2009］. ［Abbeel and Ng, 2004; Langford and Zadrozny, 2005］提出了逆强化学习方法.</p>

<p>在运筹学与控制论领域，强化学习方面的研究被称为“近似动态规 划” (approximate dynamic programming),可参阅［Bertsekas，2012］.</p>

<h1 id="ref">REF</h1>

<ol>
<li>《机器学习》周志华</li>
</ol>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/01-%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B/04-%E7%AE%97%E6%B3%95/%E5%A4%A7%E7%BA%B2/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">大纲</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/01-%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B/04-%E7%AE%97%E6%B3%95/%E5%89%91%E6%8C%87offer%E9%87%8C%E9%9D%A2%E7%9A%84/%E7%AE%97%E6%B3%95%E6%B1%82-1&#43;2&#43;3&#43;...&#43;n/">
            <span class="next-text nav-default">算法：求 1&#43;2&#43;3&#43;...&#43;n</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
