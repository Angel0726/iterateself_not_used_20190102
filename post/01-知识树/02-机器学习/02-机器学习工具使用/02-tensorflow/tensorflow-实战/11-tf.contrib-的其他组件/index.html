<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>11 TF.Contrib 的其他组件 - iterate self</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="TF.Contrib的其他组件 TF.Contrib是TensorFlow里很重要的一个部分，很大一部分开源社区的贡献都被集中 在这里，特别是一" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/02-tensorflow/tensorflow-%E5%AE%9E%E6%88%98/11-tf.contrib-%E7%9A%84%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="11 TF.Contrib 的其他组件" />
<meta property="og:description" content="TF.Contrib的其他组件 TF.Contrib是TensorFlow里很重要的一个部分，很大一部分开源社区的贡献都被集中 在这里，特别是一" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/02-tensorflow/tensorflow-%E5%AE%9E%E6%88%98/11-tf.contrib-%E7%9A%84%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6/" /><meta property="article:published_time" content="2018-06-26T20:29:18&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-26T20:29:18&#43;00:00"/>
<meta itemprop="name" content="11 TF.Contrib 的其他组件">
<meta itemprop="description" content="TF.Contrib的其他组件 TF.Contrib是TensorFlow里很重要的一个部分，很大一部分开源社区的贡献都被集中 在这里，特别是一">


<meta itemprop="datePublished" content="2018-06-26T20:29:18&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-26T20:29:18&#43;00:00" />
<meta itemprop="wordCount" content="8820">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="11 TF.Contrib 的其他组件"/>
<meta name="twitter:description" content="TF.Contrib的其他组件 TF.Contrib是TensorFlow里很重要的一个部分，很大一部分开源社区的贡献都被集中 在这里，特别是一"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">iterate self</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">about</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">iterate self</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">about</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">11 TF.Contrib 的其他组件</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-26 </span>
        
        <span class="more-meta"> 8820 words </span>
        <span class="more-meta"> 18 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#tf-contrib的其他组件">TF.Contrib的其他组件</a>
<ul>
<li>
<ul>
<li><a href="#11-1统计分布">11.1统计分布</a></li>
<li><a href="#11-2-layer-模块">11.2 Layer 模块</a>
<ul>
<li><a href="#11-2-1机器学习层">11.2.1机器学习层</a></li>
<li><a href="#11-2-2损失函数">11.2.2损失函数</a></li>
<li><a href="#11-2-3-特征列-feature-column">11.2.3 特征列 Feature Column</a></li>
<li><a href="#11-2-4-embeddings">11.2.4 Embeddings</a></li>
</ul></li>
<li><a href="#11-3性能分析器tf-prof">11.3性能分析器tf prof</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h3 id="tf-contrib的其他组件">TF.Contrib的其他组件</h3>

<p>TF.Contrib是TensorFlow里很重要的一个部分，很大一部分开源社区的贡献都被集中 在这里，特别是一些比较新的功能，由于都是一些刚贡献的功能，谷歌会将这些代码暂时 放在这里，由谷歌内部及外部的用户一起测试，根据反馈意见改进性能和改善API的友 好度，等它们的API都比较稳定时，会被移到TensorFlow的核心模块。</p>

<p>这个模块里提供了机器学习需要的大部分功能，包括统计分布、机器学习层、优化函 数、指标，等等。本章将简单介绍其中的一些功能让大家了解TensorFlow的涵盖范围和 感受到社区积极地参与和贡献度。注意这部分功能在未来会不断变动和改进，如果是写生 产代码的话，请以最新的官方教程和API指南作为最权威的参考。</p>

<h5 id="11-1统计分布">11.1统计分布</h5>

<p>在 TF.Contrib.distributions 模块里有许多的统计分布，例如 Bernoulli、Beta、Binomial、 Gamma、Exponential、Normal、Poisson、Uniform,等等。这些统计分布大多数都是统计 研究和应用中经常用到的，也是各种统计及机器学习模型的基石，许多的概率模型和图形 模型（例如Bayesian模型）都非常依赖这些统计分布。</p>

<p>每个不同的统计分布有着不同的特征和函数,但是它们都是从同样的子类Distribution</p>

<p>扩展开来的，Distribution是建立和组织随机变量和统计分布的一个最基础的类，它有着 许多有用的属性及类函数，例如用is_continuous表明这个随机变量分布是不是连续的， 用allow_nan_stats表示这个分布是否接受nan的数据，用sample()从这个分布里取样，用 prob()计算随机变量密度函数，用cdf()求累职分布函数，以及用entropy()、mean()、std()、 varianceO得到统计分布的平均值和方差之类的特征。如果想贡献自己的统计分布类，需 要实现一些对应以上的方程，例如_11^11()、_std(^n_variance()，也需要实现_is_continuous 之类表明这个变量分布的属性。</p>

<p>我们接下来以实现好的Gamma分布为例来说明这个模块的大概使用方法。首先，从 contrib.distributions 里导入 Gamma 分布，然后初始化 alpha 和 beta 的 tf.constant,这些 constant被用于建立Gamma分布，我们可以通过batch_shape().eval()得到每个样本的形状， 这里例子的样本形状shape 1是(5，)，我们也可以使用get_batch_shape()得到样本形状，但 是是以tf.TensorShape的类出现的，这个例子里shape2是tf.TensorShape(5),两种方法各 有所长，需要依据具体应用的需求来使用。</p>

<p>from tensorflow.contrib.distributions import Gamma</p>

<p>import tensorflow as tf</p>

<p>alpha = tf.constant([3.0] * 5)</p>

<p>beta = tf.constant(11.0)</p>

<p>gamma = Gamma(alpha=alphaj beta=beta)</p>

<p>shapel = gamma.batch一shape().eval()</p>

<p>shape2 = gamma.get_batch_shape()</p>

<p>然后，可以用lOg_pdf()函数取对应的一些值的log转换后的概率密度函数，我们把6 个值放在numpy.array里，然后得到相应的log概率密度函数值。</p>

<p>x = np.array([2.54 2.5， 4.0, 0.1， 1.0/ 2.0], dtype=np.float32)</p>

<p>log_pdf = gamma.log_pdf(x)</p>

<p>也可以建立多维的Gamma分布，和一维的类似，只需要传入多维的alpha和beta参 数就可以建立多维的Gamma分布。同样，我们可以对多维的x取得相应的log概率密度 函数值。</p>

<p>batch_size = 6</p>

<p>alpha = tf.constant([[2.0, 4.0]] * batch__size)</p>

<p>beta = tf.constant([[3.0, 4.0]] * batch_size)</p>

<p>x = np.array([[2.5j 2.5, 4.0， 0.1, 1.0， 2.0]]) dtype=np.float32).T gamma = Gamma(alpha=alpha_, beta=beta) log_pdf = gamma.log_pdf(x)</p>

<h5 id="11-2-layer-模块">11.2 Layer 模块</h5>

<p>Contrib.layer包含了机器学习算法所需的各种各样的成份和部件，例如卷积层、批标 准化层、机器学习指标、优化函数、初始器、特征列，等等。有了这些基础的建设部件, 我们可以高效地建立复杂的机器学习及机器学习系统。本章将介绍这个模块里一些主要的 音(5件，来帮助理解TensorFlow的各种可能性及灵活性。</p>

<h6 id="11-2-1机器学习层">11.2.1机器学习层</h6>

<p>contrib.layers里含有许多常用的深度学习及机器学习的层，例如卷积层、pooling层、 批标准化等,这些都是各种模型必不可少的部分，也是机器学习研究领域最活跃的一部分。</p>

<p>深度学习和计算机视觉里经常用到的二维的平均池是avg_pool2d0我们用 np.random.uniform建立宽和高都是3的几张假图片，读者可以通过 contrib.layers.avg_pool2d()对图片快速地建立3x3的二维平均池，这里output的形状是[5, 1，1，3],因为我们对每个3x3的区域取计算平均值。</p>

<p>height^ width = 3^ 3</p>

<p>images = np.random.uniform(size=(5_, heightwidth, 3))</p>

<p>output = tf.contrib.layers.avg_pool2d(images[3, 3])</p>

<p>用类似的方法建立卷积层，这里使用同样的图片矩阵，然后用 contrib.layers.convolution2d()建立一个有32个3&gt;&lt;3过滤器的卷积层，也可以改动.stride、 padding, aCtivatiOn_fh等参数建立不同架构的卷积层，使用不同的卷积层激活函数。</p>

<p>output = tf.contrib.layers.convolution2d(imagesj num_outputs=32j kernel<em>size= [3</em>, 3])</p>

<p>值得注意的是，contrib.layers会自动建立op的名字，例如output.op.name的值是 &lsquo;Conv/Relu1,因为我们使用了 Conv层及使用了 ReLU的激活函数，这些layer有自己对应 的op名字，然后会在每个op空间存储对应的变量，可以通过</p>

<p>contrib.framework.get_variables_by_name()得到对应的op空间变量的值。例如，可以用 get_variables_by_name得到我们建立的卷积层的权重，这里权重的形状，也就是 weights_shape 的值，是[3，3，4, 32] o</p>

<p>weights = tf.contrib.framework.get_variables_by_name(&lsquo;weights1)[0]</p>

<p>weights_shape = weights.get_shape().as_list()</p>

<p>接下来我们看看怎么将卷职层layers.convolution2d()和批标准化层layers.batch_norm() 结合使用，我们先建立一些图片的矩阵。</p>

<p>images = tf. random_uniform((5^ heightwidth, 32), seed=l)</p>

<p>接着，使用contrib.framework里面的arg_scope减少代码的重复使用，我们将 layers.convolution2d及一些即将传入的参数放入arg_scope中，这些参数通常是接下来会 被重复使用的，把它们放在arg_SCOpe里就可以避免重复在多个地方传入，这里需要用到 的参数是normalizer_fn和normalizer_params,也就是需要用到的标准化方程及它所需要 的参数，一但在arg_scope里设置了这些，接下来用到convolution2d()时就不用重复传入 normalizer_fii 和 normalizer_params 这两个参数了。</p>

<p>with tf.contrib.framework.arg_scope(</p>

<p>[tf.contrib.layers.convolution2d]j normalizer_fn=tf.contrib.layers.batch_norm? normalizer一params={&lsquo;decay•: 0.9}):</p>

<p>net = tf.contrib.layers.convolution2d(images_, 32s [3, 3])</p>

<p>net = tf.contrib.layers.convolution2d(net32<em>, [3</em>, 3])</p>

<p>可以看到，TensorFlow自动帮我们建立好了默认的一些层的名字。以上的例子里， 我们可以通过 ler^tf.contrib.framework.geLvariablesCConv/BatchNorm1))得到第一个 Conv/BatchN orm 层的长度。</p>

<p>再来看一个完全连接的神经网络层fUlly_COnneCted()的例子。首先，建立一些输入的 矩阵，用fUlly_connected()建立一个输出7个神经单元的神经网络层。 heightj width =3,3</p>

<p>inputs = tf.random一uniform((5_, height * width * 3), seed=l)</p>

<p>with tf.name_scope(&lsquo;fe&rsquo;):</p>

<p>fc = tf. contrib. layers. fully__connected (inputs, 1}</p>

<p>outputs_collections=&lsquo;outputs&rsquo;3 scope=&lsquo;fc*)</p>

<p>output_collected = tf.get_collection(&lsquo;outputs1)[0]</p>

<p>self.assertEquals(output_collected.alias, &lsquo;fe/fc&rsquo;)</p>

<p>值得注意的一些小细节是，我们利用tf.name_scope将截下来的运算放进一个 namejcope里，这样以后就可以更简单地找到我们想要的某个层的值，我们在 fhlly_connected()里传入一个scope，然后就可以通过“fe/fc”，也就是这个层的别号得到这 个层的一些信息。我们通过传入的outputs_collections，可以直接得到这个层的输出。</p>

<p>在contrib.layers里有许多特别方便使用的方法，例如，可以通过repeat()重复使用同 样的参数重复建立某个层，例如y = repeat(x, 3, conv2d, 64, [3, 3], scope=&lsquo;convr)是和 以下代码等同的。</p>

<p>x = conv2d(x, 64, [3_, 3]，scope=&lsquo;convl/convl_l&rsquo;)</p>

<p>x = conv2d(Xj 64) [3， 3]， scope=&lsquo;convl/convl_2&rsquo;)</p>

<p>y = conv2d(Xj 64^ [3j 3], scope=&lsquo;convl/convl_31)</p>

<p>可以使用stack()来使用不同的参数建立多个fhlly_connected()层，我们可以建立一个 三层的完全连接的神经网络，每层的单元数分别为32、64和128。</p>

<p>y = stack(Xj fully_connected， [32， 64, 128]， scope=&lsquo;fc&rsquo;)</p>

<p>以上代码等同于：</p>

<p>x = fully_connected(xJ 32^ scope=&lsquo;fc/fc_l&rsquo;)</p>

<p>x = fully_connected(Xj 64』scope=&lsquo;fc/fc_2*)</p>

<p>y = fully_connected(x, 12% scope=&lsquo;fc/fc_3&rsquo;)</p>

<p>注意，stack会帮你建立一个新的scope,通过在scope里附加一个增量，例如在“fc” 的基础上加上“fc_l”、“fc_2”等。之前提到的repeat()也会使用类似的机制建立新的scope。</p>

<p>我们只简单介绍一些在深度学习中经常使用的层，如果想了解更多、更复杂的层，例 如 conv2d_transpose、conv2d_in_plane、separable_conv2d 等，可以参考官方文档o</p>

<h6 id="11-2-2损失函数">11.2.2损失函数</h6>

<p>Tf.COntrib.lOSSeS模块里包含了各种常用的损失函数，适用于二类分类、多类分类，以</p>

<p>及回归模型等各式各样的机器学习算法。接下来，我们将举例说明它们的使用方法。</p>

<p>我们先以绝对差值举例说明，首先用tf.constant建立一些predictions和targets的数列。 注意，它们必须是同样的shape,然后可以选择性地建立权重，因为在许多实际应用中， 每个预测值的权重也是特别关键的。</p>

<p>predictions = tf• constant([4<em>, 8) 12) 8, 1</em>, 3]) shape=(2j 3))</p>

<p>targets = tf.constant([l, 9, 2, -5, -2, 6], shape=(2, 3))</p>

<p>weight = tf.constant([1.2^ 0.0], shape=[2,])</p>

<p>接着，可以使用losses.absolute_difference()计算这组预测的损失值，从而在之后的建 模中起到引导性的作用。</p>

<p>loss = tf.contrib.losses.absolute_difference(predictionsj targets, weight)</p>

<p>接下来，来看一个计算softmax交叉熵的例子，这种方法多适用于多类分类的机器学 习模型。同样地，我们先建立predictions和labels,与之前不一样的是，它们是多维的， 也就是softmax交叉嫡最擅长处理的。然后，使用losses.softmax_cross_entropy()计算这组 预测中softmax交叉嫡的值。注意，需要像其他TensorFlow的应用一样使用loss.eval()运 行得到它的值。可以从loss.op.name得到TensorFlow自动赋值的op的名字，这个情况下 它是&rsquo;softmax^ross-entropyjoss/value1。其他的损失函数也是使用这样的命名习俗。</p>

<p>predictions = tf .constant ([[10.0)0.0<em>, 0.0], [0.0,10.0</em>,0.0]    [0.0_,0.0,10.0]])</p>

<p>labels = tf.constant([[l_, 0，0]，[0，1，0]，[0，0，1]])</p>

<p>loss = tf.contrib.losses.softmax—cross一entropy(predictions, labels)</p>

<p>loss.eval()</p>

<p>loss.op.name</p>

<p>其他的损失函数的使用方法大同小异，值得注意的是，许多损失函数里有许多的参数 可以使用。例如，可以使用softmax_cross_entropy()里面的label_smoothing将所有的标识 进行平滑，从而使在某些应用中计算出来的softmax交叉嫡更具有实际应用的代表性，使 用方法如下。</p>

<p>logits = tf.constant([[100.0, -100.0, -100.0]])</p>

<p>labels = tf.constant([[1_, 0, 0]])</p>

<p>label一smoothing = 0.1</p>

<p>loss = tf.contrib.losses.softmax_cross_entropy(logitsj labels) label_smoothing=label_smoothing)</p>

<p>许多应用大部分标识的分布都比较稀疏，可以使用sparse_softmax _cross_entropy(), 这样计算起来会更有效率。</p>

<p>logits = tf.constant([[10.0^ 0.0，0.0], 0.0) 10.0_, 0.0]j [0.0，Q.Qj 10.0]])</p>

<p>labels = tf.constant([[0]， [1]， [2]]^ dtype=tf.int64)</p>

<p>loss = tf.contrib.losses.sparse_softmax_cross_entropy(logitslabels)</p>

<h6 id="11-2-3-特征列-feature-column">11.2.3 特征列 Feature Column</h6>

<p>在很多数据科学和机器学习的应用中，大家都习惯以表格的形式存储和处理数据，然 后将数据输入机器学习模型中。处理数据的方式多种多样，例如Python里有大家熟悉的 Pandas包。数据从各种数据源得来，经过各种方式的清理、筛选、合并，以及特征工程， 然后进行模型的建立。在TensorFlow里怎样更好地进行我们的特征工程和建模的工作 呢？</p>

<p>TF.contrib.layers里有许多高阶的特征列(Feature Column ) API,可以让大家的特征 工程更有效率，然后紧密地和TELearn的API结合使用，建立最适合自己数据的模型。</p>

<p>接下来，我们将介绍如何使用这些高阶的特征列API及如何和TF.Leam结合使用。</p>

<p>数据里一般包含连续特征(Continuous Feature )及类别特征(Categorical Feature )□ 像花瓣的长度和宽度这样连续的数值特征称为连续特征,我们可以直接把它们用在模型里。 如果特征代表了类别，例如性别、种族这样的不连续的类别特征，那么往往需要对它们进 行处理，例如将它们数值化，也就是将它们转换为一系列的数值来代表每个不同的类别。 Feature Column API可以很方便地将各种类型的特征转换为想要的格式。</p>

<p>假设读者已经用类似以下的leam.datasets的API来读入数据，例如：</p>

<p>training_set = learn.datasets.base.load_csv(filename=iris_training, target_dtype= np.int)</p>

<p>test_set = learn.datasets.base.load_csv(filename=iris_testing, target_dtype=np.int)</p>

<p>接下来就可以用layers.FeatureColumn的API定义一些特征列，例如，使用 real_valued_column()定义连续的特征(如年龄、收入、开销，以及工作市场)。</p>

<p>from tf.contrib import layers</p>

<p>age = layers.real_valued_column(&ldquo;age&rdquo;)</p>

<p>income = layers.real_valued_column(&ldquo;income&rdquo;)</p>

<p>spending = layers.real_valued_column(&ldquo;spending&rdquo;)</p>

<p>hours_of_work = layers.real_valued_column(&ldquo;hours_of_work&rdquo;)</p>

<p>紧接着，用sparse_column_with_keys()处理像性别这样的类别特征。</p>

<p>gender = layers. sparse_column_with_keys(column_name=,,gender,&lsquo;?</p>

<p>keys:「female&rdquo;male&rsquo;1])</p>

<p>注意，使用sparse_column_with_keys()前，必须要知道这个特征所有可能的值，本例 中，性别分为男性和女性。如果事先不知道所有可能的值，可以使用 sparse_column_with_hash_bucket()将特征转换为特征列。以教育程度这样的特征为例，由 于对数据不是特别熟悉，无法事先知道所有可能的教育程度，我们可以用哈希表建立这样 的特征。</p>

<p>education = layers.sparse_column_with_hash_bucket(neducation&rdquo;?</p>

<p>hash_bucket_size=1000)</p>

<p>&hellip;&hellip;&hellip;&hellip;&hellip; &hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;&hellip;. 1—— - •- - ■ 1&hellip;..: J,,-&lsquo;. ..…„ -X.    , &hellip;i &hellip;.    &hellip;&hellip;&hellip;..</p>

<p>sparse_column_with_keys()及 sparse_column_with_hash_bucket()都能将数据转换为 SparseColumn,然后可以直接在TF.Leam里使用，传入Estimator里。</p>

<p>有时，在数据科学的应用中，一些连续的特征可能需要被离散化，从而形成新的类别 特征，这样能更好地代表特征和目标分类类别之间的关系。例如年龄是连续特征，分类的 类别是职业的类别(如经理、猎头等)，往往这些职业的类别和年龄阶段有关，而不是简 单的数值年龄。因为18岁、19岁、20岁往往没有明显的区别，所以有时会将这样的连续 特征区间化和离散化，例如将I8岁~20岁分为一类。在FeatureColumn API里，.我们可 以很快地进行这样的转换。</p>

<p>age_range = layers.buck?tized_column(age, boundaries=[18J 25,    3S, 40)</p>

<p>45, 50, 55, 60, 65])</p>

<p>在以上的例子里，使用bucketized_column()将之前的年龄SparseColumn进行进一步 的区间化，将年龄段分为18岁~25岁、26岁~30岁、31岁~35岁，等等。</p>

<p>在许多应用里，一个好的模型不仅需要一些单独的特征列，有时两个或多个特征之间 的综合和交互与目标分类类别之间的关系更紧密。有时多个特征之间是相关的，使用特征 的交互往往能建立更有效的模型。例如，对年龄、职业和种族这三个特征，我们可以使用 crossed_column()建立交叉特征列：</p>

<p>combined = layers.crossed_column([age<em>rangerace</em>, occupation] hash_bucket_size=int(le7))</p>

<p>建立好各式各样的特征列之后，我们可以直接将它们传入不同的TF.Learn Estimator。 以下面这个简单的逻辑回归分类模型为例，我们可以使用之前介绍过的fit()、戸出呦等 方法训练和评估模型。</p>

<p>classifier = tf.contrib.learn.LinearClassifier(feature_columns=[</p>

<p>gender, education, occupation^ combined, age_rangej race, income, spending], model_dir=model_dir)</p>

<p>这里我们只是简单地介绍了一些比较常用的函数，在实际应用中有各种各样的需求， 例如有时想取一部分特征的加权求和作为一个新的特征列，可以使用 weighted_sum_from_feature_columns()来很快地实现。读者可以在官方文档里找到更多需 要的函数。</p>

<h6 id="11-2-4-embeddings">11.2.4 Embeddings</h6>

<p>在许多深度模型应用中，包含许多稀疏的、高维的类别特征向量，我们通常先把它们 转换成低维的、稠密的实数值的向量，也通常将它们和连续特征向量联合起来，一起输入 进神经网络模型中进行训练和优化损失函数，这些被统一称为嵌入向量(Embedding Vectors )o大部分文本识别都是先将文本转换成嵌入向量，然后对它们进行分析并用在模 型训练中。</p>

<p>contrib.layers模块里的embedding_column()能迅速将高维稀疏的类别特征向量转换为 读者想要的维数的嵌入向量，以下是一个例子。</p>

<p>embedding_columns =[</p>

<p>tf.contrib.layers.embedding_column(title, dimension=8)i tf.contrib.layers.embedding_column(education, dimension=8) tf.contrib.layers.embedding_column(gender^ dimension=8), tf.contrib.layers.embedding_column(race, dimension=8), tf.contrib.layers.embedding_column(country, dimension=8)]</p>

<p>这里的title、education、gender、race,以及country都是比较稀疏的类别特征向量， 我们通过使用embedding_column(),把它们转换为低维数的稠密向量，从而更好地归纳数 据中的特性，特别是当一组特征中的交互矩阵比较稀疏，级别比较高时，这种方法会使模 型更具有概括性且更有效。</p>

<p>接下来，可以直接将它们传入TF.Leam的Estimator里进行觀的建立、训练，以及 评估。例如，可以将 embedding_columns 传入 DNNLinearCombinedClassifier 里的深度神 经网络特征列里。</p>

<p>est = tf.contrib.learn.DNNLinearCombinedClassifier( model_dir=model_dirj linear_feature_columns=wide_columns? dnn_feature_columns=embedding_columnsj dnn_hidden_units=[100, 50])</p>

<p>embedding_columns()是contrib.layers模块里最简单易用的一个，当涉及实际数据时， 许多稀疏高维的数据里通常有空的特征及无效的ID，这时可以使用 safe_embedding_lookup_sparse()安全地建立嵌入向量。这里先用tf.SparseTensor建立好稀 疏的ID及稀疏的权重。</p>

<p>indices = [[0, 0], [0, 1], [0, 2], [1, 0], [3, 0], [4, 0], [4, 1]]</p>

<p>ids = [0, 1, -1, -1, 2, 0) 1]</p>

<p>weights = [1.0, 2.0^ 1.0, l.Q, 3.0, 0.0, -0.5]</p>

<p>shape = [5, 4]</p>

<p>sparse_ids = tf.SparseTensor(</p>

<p>tf.constant(indices, tf,int64)tf,constant(idSj tf.int64)j    .</p>

<p>tf.constant(shapetf.int64))</p>

<p>sparse_weights = tf.SparseTensor(</p>

<p>tf.constant(indices? tf.int64), tf.constant(weights, tf.float32), tf.constant(shape? tf.int64))</p>

<p>接下来，建立嵌入向量的权重embedding_weights,这取决于词汇量大小、嵌入向量 维数，以及shard数量。然后，使用initializer.runQ和eval()初始化嵌入向量的权重，具体</p>

<p>细节和参数的说明请参考最新的官方文档。</p>

<p>vocab_size=4</p>

<p>embed_dim=4</p>

<p>num_shards=l</p>

<p>embedding_weights = tf.create_partitioned_variables( shape=[vocab_sizej embed_dim] slicing=[num<em>shardsJ 1]</em>,</p>

<p>initializer=tf.truncated_normal_initializer(mean=0.0j</p>

<p>stddev=1.0 / math.sqrt(vocab_size), dtype=tf.float32))</p>

<p>for w in embedding一weights: w.initializer.run()</p>

<p>embedding_weights = [w.eval() for w in embedding_weights]</p>

<p>最后，可以使用safe_embedding_lookup_sparse()将原来的特征向量安全地转换为低维 和稠密的特征向量，这里使用eval()，然后将它们收集到一个tuple里。</p>

<p>embedding_lookup_result = (tf. contrib. layers. safe_embedding_lookup_sparse( embedding_weightssparse_idSj sparse_weights).eval())</p>

<h5 id="11-3性能分析器tf-prof">11.3性能分析器tf prof</h5>

<p>TensorFlow也在Contrib模块里提供了自己的性能分析器tlprof,可以通过它帮助分 析模型的架构及衡量系统的性能。它涵盖了许多实用的功能，例如衡量模型的参数、浮点 运算、叩执行时间、要求的存储大小、探索模型的结构等。本节将简单地介绍一些功能。</p>

<p>首先，通过以下命令安装命令行的工具。</p>

<p>bazel build -c opt tensorflow/contrib/tfprof/&hellip;</p>

<p>可以通过以下命令查询帮助文件。</p>

<p>bazel-bin/tensorflow/contrib/tfprof/tools/tfprof/tfprof help</p>

<p>可以执行互动模式，然后指定graph_path来分析模型的shape和参数。</p>

<p>bazel-bin/tensorflow/contrib/tfprof/tools/tfprof/tfprof </p>

<p>&ndash;graph一path=/graph.pbtxt</p>

<p>类似地，我们用graph_path和checkpoint_path查看checkpoint里Tensor的数据和相 对应的值。</p>

<p>bazel-bin/tensorflow/contrib/tfprof/tools/tfprof/tfprof </p>

<p>&ndash;graph_path=graph.pbtxt </p>

<p>&ndash;checkpoint一path=model.ckpt</p>

<p>与此同时，我们可以多提供一个run_meta_path来查看不同op请求的存储和计时。</p>

<p>bazel-bin/tensorflow/contrib/tfprof/tools/tfprof/tfprof </p>

<p>&ndash;graph_path=graph.pbtxt </p>

<p>&ndash;run_meta_path=run_meta </p>

<p>&ndash;checkpoint_path=model.ckpt</p>

<p>值得注意的是，上面用到了 run_meta_path、graph_path和checkpoint」）ath几个路径， 我们是怎么得到这几种类型的文件的呢？</p>

<p>graphj^ath的文件是GraphDef文本文件，用来在内存里建立模型的代表，例如用 tf.Supervisor写出来的graph.pbtxt就是一个GraphDef文本文件的例子。如果不使用 tf.Supervisor,那么可以使用tf.Graph.as_graph_def（咸者其他类似的API存储模型的定义 到一个GraphDef文件里。</p>

<p>mn_meta_path所需的文件是tensorflow::RunMetadata的结果，这个方程是用来得到模 型中每个op所需的存储和时间消耗的，以下简单的几行代码可以写出一个RunMetadata 文件。</p>

<p>run_options = config_pb2.RunOptions(</p>

<p>trace_level=config_pb2.RunOptions.FULL_TRACE)</p>

<p>run_metadata = config_pb2.RunMetadata()    .</p>

<p>_ = self.一sess.run(…，options=run_optionsj run_metadata=run_metadata) with gfile.Open(os.path.join(output_dirj &ldquo;run^eta&rsquo;^^ &ldquo;w&rdquo;) as f:</p>

<p>f.write(run_metadata.SerializeToStning())</p>

<p>checkpointjpath是模型的checkpoint，它包含了所有checkpoint的变量的op类型、shape 和它们的值。</p>

<p>读者也可以提供其他的路径，例如op_log_path路径，它是tensorflow::tfjprof::OpLog 的结果，包含了额外的叩的信息，由于它包含了 op的组的类别名字，用户可以很简单地 综合op的一些数据，而不会不小心错过其中一部分叩。以下用一个暴露出来的API来很 快地写出了一个OpLog文件。</p>

<p>tf.contrib.tfprof.tfprof_logger.write_op_log(graphj log_dirj op_log=None)</p>

<p>由于tfjjrof是一个CLI命令行的工具，当输入之前的tfjKof命令按下回车键时，会进 入互动模式，再按一下回车键会看到一些类似以下的命令行参数的默认值。</p>

<p>tfprof&gt; -max_depth</p>

<p>-min_bytes</p>

<p>-min_micros</p>

<p>-min_params</p>

<p>-min_float_ops</p>

<p>-device_regexes -order_by</p>

<p>-account_type_regexes -start_name_regexes -trim_name_regexes -show_name_regexes -hide一name_regexes</p>

<p>4</p>

<p>0</p>

<p>0</p>

<p>0</p>

<p>0</p>

<p>name</p>

<p>Variable</p>

<p>木</p>

<p>•氺</p>

<p>VariableInitialized<em>[0-9]+^ save\/.*jAzeros[0-9</em>]*</p>

<p>-account_displayed_op_only false</p>

<p>-select    params</p>

<p>-viz    false -dump_to_file</p>

<p>然后就可以调节里面的参数，例如用show_name<em>regexes查找scope名字正则式为 unit</em> 1 _0. * gamma, max_depth 为 5 的变量，查看这些 tensor 的值：</p>

<p>tfprof&gt; scope -show_name_regexes unit_l_0.*gamma -select tensor_value \ -max_depth 5</p>

<p>读者会得到类似以下符合条件的tensor的值：</p>

<p>unit_l_0/shared_activation/init_bn/gamma ()</p>

<p>[1.80 2.10 2.06 1.91 2.26 1.86 1.81 1.37 1.78 1.85 1.96 1.54 2.04 2.34 2.22</p>

<p>1.99 L</p>

<p>unit_l_0/sub2/bn2/gamma ()</p>

<p>[1.57 1.83 1.30 1.25 1.59 1.14 1.26 0.82 1.19 1.10 1.48 1.01 0.82 1.23 1.21 1.14 ]</p>

<p>tfj)rof提供了两种类型的分析：scope和graph。当读者想查看一些变量和scope的值 的时候，你可以使用scope,也就是我们上面阐述过的例子。当读者想查看op在graph里 所花的内存和时间时，可以使用graph查看。</p>

<p>类似地，可以改动一些命令行参数，例如使用start_name_regexes选择想要查看的op 的名字，这里我们假设需要查看命名为cost的损失op的内存和时间花费的情况：</p>

<p>tfprof&gt; graph -start一name_regexes cost.* -max_depth 100 -min_micros 10000 </p>

<p>-select micros -account_type_regexes .*</p>

<p>init/init_conv/Conv2D (11.75ms/3.10sec)</p>

<p>random_shuffle_queue_DequeueMany (3.09sec/3.09sec)</p>

<p>unit_l_0/sub2/conv2/Conv2D (74.14ms/3.19sec)</p>

<p>unit<strong>l</strong>3/sub2/conv2/Conv2D (60.75ms/3• 34sec)</p>

<p>unit_2_4/sub2/conv2/Conv2D (73.58ms/3.54sec)</p>

<p>unit_3_3/sub2/conv2/Conv2D (10.26ms/3.60sec)</p>

<p>我们就先简单地介绍以上这些模块，这一部分变化很大，更多的功能还需要读者自己 摸索并查看官方文档。</p>

<p>k 296</p>

<p><a href="http://www.aibbt.com">www.aibbt.com</a> 让未来触手可及</p>

    </div>

    
    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">iterateself</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">2018-06-26</span>
  </p>
  
  
</div>

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/02-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/02-tensorflow/tensorflow-%E5%AE%9E%E6%88%98/10-tf.learn-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">10 TF.Learn 从入门到精通</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/01-%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B/01-%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/01-c&#43;&#43;/c&#43;&#43;-primer/20-%E6%9C%AF%E8%AF%AD%E4%B8%8E%E7%B4%A2%E5%BC%95/">
            <span class="next-text nav-default">20 术语与索引</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
