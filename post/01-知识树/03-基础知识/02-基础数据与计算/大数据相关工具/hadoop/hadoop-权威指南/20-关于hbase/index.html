<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>20 关于HBase - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="（作者：Jonathan Gray 和 Michael Stack） 20.1 HBase 基础 HBase是一个在HDFS上开发的面向列的分布式数据库。如果需要实时地随机访 问超大规模数据" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%AE%A1%E7%AE%97/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/hadoop/hadoop-%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/20-%E5%85%B3%E4%BA%8Ehbase/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="20 关于HBase" />
<meta property="og:description" content="（作者：Jonathan Gray 和 Michael Stack） 20.1 HBase 基础 HBase是一个在HDFS上开发的面向列的分布式数据库。如果需要实时地随机访 问超大规模数据" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%AE%A1%E7%AE%97/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/hadoop/hadoop-%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/20-%E5%85%B3%E4%BA%8Ehbase/" /><meta property="article:published_time" content="2018-06-27T07:51:30&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-27T07:51:30&#43;00:00"/>
<meta itemprop="name" content="20 关于HBase">
<meta itemprop="description" content="（作者：Jonathan Gray 和 Michael Stack） 20.1 HBase 基础 HBase是一个在HDFS上开发的面向列的分布式数据库。如果需要实时地随机访 问超大规模数据">


<meta itemprop="datePublished" content="2018-06-27T07:51:30&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-27T07:51:30&#43;00:00" />
<meta itemprop="wordCount" content="20368">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="20 关于HBase"/>
<meta name="twitter:description" content="（作者：Jonathan Gray 和 Michael Stack） 20.1 HBase 基础 HBase是一个在HDFS上开发的面向列的分布式数据库。如果需要实时地随机访 问超大规模数据"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">20 关于HBase</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-27 </span>
        
        <span class="more-meta"> 20368 words </span>
        <span class="more-meta"> 41 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#20-1-hbase-基础">20.1 HBase 基础</a>
<ul>
<li><a href="#背景">背景</a></li>
</ul></li>
<li><a href="#20-2概念">20.2概念</a>
<ul>
<li><a href="#20-2-1数据模型的-旋风之旅">20.2.1数据模型的“旋风之旅”</a></li>
<li><a href="#20-2-2-实现">20.2.2 实现</a></li>
</ul></li>
<li><a href="#20-3安装">20.3安装</a>
<ul>
<li><a href="#测试驱动">测试驱动</a></li>
</ul></li>
<li><a href="#20-4客户端">20.4客户端</a>
<ul>
<li><a href="#20-4-1-java">20.4.1 Java</a></li>
<li><a href="#20-4-2-mapreduce">20.4.2 MapReduce</a></li>
<li><a href="#20-4-3-rest-和-thrift">20.4.3 REST 和 Thrift</a></li>
</ul></li>
<li><a href="#20-5创建在线查询应用">20.5创建在线查询应用</a>
<ul>
<li><a href="#20-5-1模式设计">20.5.1模式设计</a></li>
<li><a href="#宽表-e调">宽表，e调</a></li>
<li><a href="#20-5-2加载数据">20.5.2加载数据</a></li>
<li><a href="#20-5-3在线查询">20.5.3在线查询</a></li>
</ul></li>
<li><a href="#20-6-hbase-和-rdbms-的比较">20.6 HBase 和 RDBMS 的比较</a>
<ul>
<li><a href="#20-6-1成功的服务">20.6.1成功的服务</a></li>
<li><a href="#20-6-2-hbase">20.6.2 HBase</a></li>
</ul></li>
<li><a href="#20-7-praxis">20.7 Praxis</a>
<ul>
<li><a href="#20-7-1-hdfs">20.7.1 HDFS</a></li>
<li><a href="#20-7-2用户界">20.7.2用户界</a></li>
<li><a href="#20-7-3度量">20.7.3度量</a></li>
<li><a href="#20-7-4计数器">20.7.4计数器</a></li>
</ul></li>
<li><a href="#20-8延伸阅读">20.8延伸阅读</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<p>（作者：Jonathan Gray 和 Michael Stack）</p>

<h5 id="20-1-hbase-基础">20.1 HBase 基础</h5>

<p>HBase是一个在HDFS上开发的面向列的分布式数据库。如果需要实时地随机访 问超大规模数据集，就可以使用HBase这一 Hadoop应用。</p>

<p>虽然数据库存储和检索的实现可以选择很多不同的策略，但是绝大多数解决办 法，特别是关系型数据库技术的变种，不是为大规模可伸缩的分布式处理设计 的。很多厂商提供了复制(replication)和分区(partitioning)解决方案，让数据库能够 从单个节点上扩展出去，但是这些附加的技术大都属于“事后”的解决办法，而 且非常难以安装和维护。并且这些解决办法常常要牺牲一些重要的RDBMS特 性。在一个“扩展的” RDBMS上，连接、复杂查询、触发器、视图以及外键约束 这些功能要么运行开销大，要么根本无法用。</p>

<p>HBase从另一个方向来解决可伸缩性的问题。它自底向上地进行构建，能够简单 地通过增加节点来达到线性扩展。HBase并不是关系型数据库，它不支持SQL。® 但在特定的问题空间里，它能够做RDBMS不能做的事：在廉价硬件构成的集群 上管理超大规模的稀疏表。</p>

<p>HBase的一个典型应用是web table, 一个以网页URL为主键的表，其中包含爬取</p>

<p>①不过,可以了解一下17.4.3节中提到的Apache Phoenix项目以及名为Trafodion的基于HBase 的荠务 SQL 奴据库［<a href="https://wiki.trafodion.orgf)o">https://wiki.trafodion.orgf)o</a></p>

<p>的页面和页面的属性(例如语言和MIME类型)。webtable非常大，行数可以达十亿 级(billion)之级。在webtable上连续运行用于批处理分析和解析的MapReduce作 业，能够获取相关的统计信息，增加验证的MIME类型列以及供搜索引擎进行索 引的解析后的文本内容。同时，表格还会被以不同运行速度的“爬取器” (crawler) 随机访问并随机更新其中的行。在用户点击访问网站的缓存页面时，需要实时地 将这些被随机访问的页面提供给他们。</p>

<h6 id="背景">背景</h6>

<p>HBase项目是由Powerset公司的Chad Walters和Jim Kelleman在2006年末发起 的。当时，它起源于在此之前Google刚刚发布的Bigtable。® 2007年2月，Mike Cafarella提供代码，形成了 一个基本可以用的系统，然后Jim Kellerman接手继续 推进该项目。</p>

<p>HBase的第一个发布版本是在2007年10月和Hadoop 0.15.0捆绑在一起发布的。 2010年5月，HBase从Hadoop子项目升级成Apache顶层项目。今天，HBase已 然成为一种广泛应用于各种行业生产中的成熟技术。。</p>

<h5 id="20-2概念">20.2概念</h5>

<p>在本节中，我们只对HBase的核心概念进行快速、简单的介绍。掌握这些概念至 少有助于消化后续内容。</p>

<h6 id="20-2-1数据模型的-旋风之旅">20.2.1数据模型的“旋风之旅”</h6>

<p>应用把数据存放在带标签的表中。表由行和列组成。表格的“单元格(cell)由行 和列的坐标交叉决定，是有版本的。默认情况下，版本号是自动分配的，为 HBase插入单元格时的时间戳。单元格的内容是未解释的字节数组。例如， 图20-1所示为用于存储照片的HBase表。</p>

<p>表中行的键也是字节数组。所以理论上，任何东西都可以通过表示成字符串或将 二进制形式转化为长整型或直接对数据结构进行序列化，来作为键值。表中的行</p>

<p>①引自 Fay Chang 等人的文章 “Bigtable: A Distributed Storage System for Structured Data”</p>

<p>（Bigtable： 一个结构化数据的分布式存储系统），发表于2006年11月，网址为 <a href="http://research.google.com/archive/bigtable.html">http://research.google.com/archive/bigtable.html</a> Q</p>

<p>根据行的键值(也就是表的主键)进行排序。排序根据字节序进行。所有对表的访问 都要通过表的主键。®</p>

<p>Increasing row key</p>

<p>Column</p>

<p>mn family contents contents:image</p>

<p>000001</p>

<p>jpeg</p>

<p>娥灘送 :&lsquo;5縷：’</p>

<p>tiff</p>

<p>000002</p>

<p>000003</p>

<p>51.8厂3.1</p>

<p>I</p>

<p>__。国。    __    __    __ <em>一</em></p>

<p>51.5,-0.1</p>

<p>Versions    Cells</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-249.jpg" alt="img" /></p>

<p>20-1.用于描述存储照片的表的HBase数据模型</p>

<p>行中的列被分成“列族”(column family)。同一个列族的所有成员具有相同的前 缀。因此，像列/w/o./orwo/和info:geo都是列族k/o的成员，而则 属于contents族。列族的前缀必须由“可打印的”(printable)字符组成。而修饰性的 结尾字符，即列族修饰符，可以为任意字节。列族和修饰符之间始终以冒号(:) 分隔。</p>

<p>一个表的列族必须作为表模式定义的一部分预先给出，但是新的列族成员可以随 后按需要加入。例如，只要目标表中已经有了列族/A亦，那么客户端就可在更新时提 供新的列并存储它的值。</p>

<p>物理上，所有的列族成员都一起存放在文件系统中。所以，虽然我们前面把 HBase描述为一个面向列的存储器，但实际上更准确的说法是：它是一个面向列 族的存储器。由于调优和存储都是在列族这个层次上进行的，所以最好使所有列 族成员都有相同的访问模式(access pattern)和大小特征。对于存储照片的表，由</p>

<p>①HBase不支持表中的其他列建立索引(也称为辅助索引)。不过，有几种策略可用于支持辅助索</p>

<p>引提供的査询类型，每种策略在存储空间、处理负载和査询执行时间之间存在不同的利弊权 衡，关干这个问题，请参阅HBase参考指南，讽地为<a href="http://hbase.apache.org/book.html%e3%80%82">http://hbase.apache.org/book.html</a><a href="http://hbase.apache.org/book.html%e3%80%82">。</a></p>

<p>于图像数据比较大(兆字节)，因而跟较小的元数据(千字节)分别存储在不同的列</p>

<p>族中。</p>

<p>简而言之，HBase表和RDBMS中的表类似，只不过它的单元格有版本，行是排 序的，而只要列族预先存在，客户端随时可以把列添加到列族中去。</p>

<p>1.区域</p>

<p>HBase自动把表水平划分成区域(region)。每个区域由表中行的子集构成。每个区 域由它所属干的表、它所包含的第一行及其最后一行(不包括这行)来表示。一开 始，一个表只有一个区域。但是随着区域开始变大，等到它超出设定的大小阈 值，便会在某行的边界上把表分成两个大小基本相同的新分区。在第一次划分之 前，所有加载的数据都故在原始区域所在的那台服务器上。随着表变大，区域的 个数也会增加。区域是在HBase集群上分布数据的最小单位。用这种方式，一个 因为太大而无法放在单台服务器上的表会被放到服务器集群上，其中每个节点都 负责管理表所有区域的一个子集。表的加载也是使用这种方法把数据分布到各个 节点。在线的所有区域按次序排列就构成了表的所有内容。</p>

<p>2.加锁</p>

<p>无论对行进行访问的事务牵涉多少列，对行的更新都是“原子的”(atomic)。这使 得“加锁模型”(locking model)能够保持简单。</p>

<h6 id="20-2-2-实现">20.2.2 实现</h6>

<p>正如HDFS和YARN是由客户端、从属机(slave)和协调主控机(master)—(即HDFS</p>

<p>饱namenode热datanode，以及YARN的资源管理器和节点管理器)-组成，HBase也</p>

<p>采用相同的模型，它用一个master节点协调管理一个或多个regionserver从属机 (参见图20-2)。HBase主控机(master)负责启动(bootstrap)—个全新的安装，把区域 分配给注册的regionserver,恢复regionserver的故障。master的负载很轻。 regionserver负责零个或多个区域的管理以及响应客户端的读/写请求。regionserver 还负责区域的划分并通知HBase master有了新的子区域(daughter region)，这样一来， 主控机就可以把父区域设为离线，并用子区域替换父区域。</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-250.jpg" alt="img" /></p>

<p>Regionserver</p>

<p>Regionserver</p>

<p>Regionserver</p>

<p>••</p>

<p>••</p>

<p>••••</p>

<p>••</p>

<p>♦•减</p>

<p>&ldquo;嚷顏.</p>

<p>图20-2. HBase集群的成员</p>

<p>HBase依赖于ZooKeeper(参见第21章)。默认情况下，它管理一个ZooKeeper实 例，作为集群的“权威机构” (authority),尽管也可以通过配置来使用已有的 ZooKeeper集群。ZooKeeper集合体(ensemble)负责管理诸如hbase:meta目录表</p>

<p>的位置以及当前集群主控机地址等重要信息。如果在区域的分配过程中有服务器 崩溃，就可以通过ZooKeeper•来进行分配的协调。在ZooKeeper上管理分配事务 的状态有助于在恢复时能够从崩溃服务器遗留的状态开始继续分配。在启动一个 客户端到HBase集群的连接时，客户端必须至少拿到到集群所传递的ZooKeeper 集合体的位置。这样，客户端才能访问ZooKeeper的层次结构，从而了解集群的 属性，例如服务器的位置。</p>

<p>类似于在Hadoop中可以通过etc/hadoop/slaves文件查看datanode和节点管理器一 样，regionserver从属机节点列在HBase的conf/regionservers文件中。启动和结束</p>

<p>服务的脚本和Hadoop类似，使用相同的基于SSH的机制来运行远程命令。集群 的站点配置(site-specific configuration)在 HBase 的 conf/hbase-site.xml 和 conf/hbase-</p>

<p>文件中。它们的格式和Hadoop父项目中对应的格式相同(参见第10章)。</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-252.jpg" alt="img" /></p>

<p>对于HBase和Hadoop上相同的服务或类型，HBase实际上直接使用或继承 Hadoop的实现。在无法直接使用或继承时，HBase会尽量遵循Hadoop的模 型。例如，HBase使用Hadoop Configuration系统，所以它们的配置文件格式 相同。对于作为用户的你来说，这意味着你使用HBase时感觉就像使用 Hado叩一样“亲切”。HBase只是在增加特殊功能时才不遵循这一规则。</p>

<p>HBase通过Hadoop文件系统API来持久化存储数据。多数人使用HDFS作为存储 来运行HBase0但是，在默认情况下，除非另行指明，HBase会将存储写入本地 文件系统。如果是体验一下新装HBase,这是没有问题的，但如果稍后要使用 HBase集群，首要任务通常是把HBase的存储配置为指向要使用的HDFS集群。</p>

<p>运行中的HBase</p>

<p>HBase内部保留名为hbase:meta的特殊目录表(catalog table)。它们维护着当前集 群上所有区域的列表、状态和位置。hbase:/neta表中的项使用区域名作为键。区 域名由所属的表名、区域的起始行、区域的创建时间以及对其整体进行的MD5哈 希值(即对表名、起始行、创建的时间戳进行哈希后的结果)组成。</p>

<p>例如，表TestTabLe中起始行为xyz的区域的名称如下:</p>

<p>TestTable,xyz,1279729913622.Ib6el76fb8d8aa88fd4ab6bc80247ece.</p>

<p>在表名、起始行、时间戳中间用逗号分隔。MD5哈希值则使用前后两个句号包 如前所述，行的键是排序的。因此，要查找一个特定行所在的区域只要在目录表 中找到第一个键大干或等于给定行键的项即可。区域变化时，即分裂、禁用/启用 (disable/enable)、删除、为负载均衡重新部署区域或由于regionserver崩溃而重新 部署区域时，目录表会进行相应的更新。这样，集群上所有区域的状态信息就能保持是 最新的。</p>

<p>新连接到ZooKeeper集群上的客户端首先查找hbase:meta的位置。然后客户端通 过查找合适的hbase:meta区域来获取用户空间区域所在节点及其位置。接着， 客户端就可以直接和管理那个区域的regionserver进行交互。</p>

<p>每个行操作可能要访问三次远程节点。为了节省这些代价，客户端会缓存它们遍 历hbase:meta时获取的信息。需要缓存的不仅有位置信息，还有用户空间区域 的开始行和结束行。这样，它们以后不需要访问hbase:meta表也能得知区域存 放的位置。客户端在碰到错误之前会一直使用所缓存的项。当发生错误时，即区 域被移动了，客户端会再去查看hbase:meta获取区域的新位置。如果</p>

<p>hbase:meta区域也被移动了，客户端会重新查找。</p>

<p>到达Regionserver的写操作首先追加到“提交日志” (commit log)中，然后加入内 存中的memstore。如果memstore满，它的内容会被‘‘刷人”(flush)文件系统。</p>

<p>提交日志存放在HDFS中，因此即使一个regionserver崩溃，提交日志仍然可用。 如果发现一个regionserver不能访问(通常因为服务器的znode在ZooKeeper中过 期)，主控机会根据区域对死掉的regionserver的提交日志进行分割。重新分配 后，在打开并使用死掉的regionserver上的区域之前，这些区域会找到属于它们的 从被分割提交日志中得到的文件，其中包含还没有被持久化存储的更新。这些更 新会被“重做”(replay)以使区域恢复到服务器失败前的状态。</p>

<p>在读的时候首先查看区域的memstore。如果在memstore中找到了需要的版本，查 向就结束了。否则，需要按照次序从新到旧检查“刷新文件” (flush file),直到找 到满足查询的版本，或所有刷新文件都处理完为止。</p>

<p>有一个后台进程负责在刷新文件个数到达一个阈值时压缩它们。它把多个文件重 新写人一个文件。这是因为读操作检查的文件越少，它的执行效率越高。在压缩 (compaction)时，进程会清理掉超出模式所设最大值的版本以及删除单元格或标识 单元格为过期。在regionserver上，另外有一个独立的进程监控着刷新文件的大 小，一旦文件大小超出预先设定的最大值，便对区域进行分割。</p>

<h5 id="20-3安装">20.3安装</h5>

<p>从一个 Apache Download Mirror (<a href="http://www.apache.org/dyn/closer.cgi/hbase/">http://www.apache.org/dyn/closer.cgi/hbase/)</a>%e4%b8%ad%e6%a1%83%e9%80%89)<a href="http://www.apache.org/dyn/closer.cgi/hbase/">中桃选</a>%e4%b8%ad%e6%a1%83%e9%80%89) 并下载一个HBase的稳定发布版本，然后在本地文件系统解压。示例如下：</p>

<p>% tar xzf hbase-x.y.z.tar.gz</p>

<p>和用Hadoop 一样，首先需要告诉HBase系统中的Java在哪里。如果设置了 ]AVA_HOME环境变量，把它指向了正确的Java安装，HBase就会使用那个Java安 装。这样便不需要进行其他配置。否则，可以通过编辑HBase的conf/hbase-env.sh，并指明]AVA_HOME变量的值(参见附录A的示例)，从而设置HBase所使 用的Java安装。</p>

<p>为了方便，把HBase的二进制文件目录加入命令行路径中。示例如下：</p>

<p>% export HBASE_HOME=^/sw/hbase-x.y.z % export PATH=$PATH:$HBASE_HOME/bin</p>

<p>要想获取HBase的选项列表，输入以下命令即可：</p>

<p>% hbase</p>

<p>Options:</p>

<p>&ndash;config DIR Configuration direction to use. Default: ./conf &ndash;hosts HOSTS Override the list in * regionservers1 file</p>

<p>Commands:</p>

<p>Some commands take arguments• Pass no args or -h for usage, shell Run the HBase shell hbck Run the hbase <em>fsck</em> tool hlog Write-ahead-log analyzer hfile Store file analyzer zkcli Run the ZooKeeper shell upgrade Upgrade hbase master Run an HBase HMaster node regionserver Run an HBase HRegionServer node zookeeper Run a Zookeeper server rest Run an HBase REST server thrift Run the HBase Thrift server thrift2 Run the HBase Thrift2 server clean Run the HBase clean up script classpath Dump hbase CLASSPATH</p>

<p>mapredcp Dump CLASSPATH entries required by mapreduce</p>

<p>pe Run PerformanceEvaluation</p>

<p>ltt Run LoadTestTool</p>

<p>version Print the version</p>

<p>CLASSNAME Run the class named CLASSNAME</p>

<h6 id="测试驱动">测试驱动</h6>

<p>要启动一个使用本地文件系统々wp目录作为持久化存储的HBase临时实例，键人 以下命令：</p>

<p>% start-hbase.sh</p>

<p>在默认情况下，HBase 会被写入到 /${java.io.tnjpdir}/hbase-${username}中 0${java.io.tmpdir}通常被映射为々即，不过，还是应该通过设置hbase-site.xml 中的hbase.tmp.dir来对HBase进行配置，以便使用更长久的存储位置。在独 立模式下，HBase主控机、regionserver和ZooKeeper实例都是在同一个JVM中运 行的。</p>

<p>要管理HBase实例，键入以下命令启动HBase的shell环境即可：</p>

<p>% hbase shell</p>

<p>HBase Shell; enter •help<RETURN>• for list of supported commands.</p>

<p>Type &ldquo;exit<RETURN>” to leave the HBase Shell</p>

<p>Version 0.98.7-hadoop2, r800c23e2207aa3f9bddb7e9514d8340bcfb89277J Wed Oct 8 15:58:11 PDT 2014</p>

<p>hbase(main):001:0&gt;</p>

<p>这将启动一个加入了一些HBase特有命令的JRuby 1RB解释器。输入help然后 按RETURN键可以查看已分组的shell环境的命令列表。输入help COMMAND_GROUP可以查看某一类命令的帮助，而输入help COMMAND则能获得某 个特定命令的帮助信息和用法示例。命令使用Ruby的格式来指定列表和目录。主 帮助屏幕的最后包含一个快速教程。</p>

<p>现在，让我们创建一个简单的表，添加一些数据，再把表清空 要新建一个表，首先必须为你的表起一个名字，并为其定义模式。一个表的模式 包含表的属性和列族的列表。列族本身也有属性，可以在定义模式时依次定义它 们。例如，列族的属性包括列族是否应该在文件系统中被压缩存储以及一个单元 格要保存多少个版本等。模式可以被修改，需要修改时把表设为“离线” (offline) 即可。在shell环境中使用disable命令可以把表设为离线，使用alter命令可 以进行必要的修改，而enable命令则可以把表重新设为“在线”(online)。</p>

<p>要想新建一个名为test的表，使其只包含一个名为data的列，表和列族属性都 为默认值，则键入以下命令：</p>

<p>hbase(main):001:0&gt; create &lsquo;test1, &lsquo;data&rsquo;</p>

<p>0 row(s) in 0.9810 seconds</p>

<p>如果前面有命令没有成功完成，那么Shell环境会提示错误并显示堆栈跟踪 &lsquo; (stack trace)信息。这时你的安装肯定没有成功。请检查HBase日志目录中的</p>

<p>胃主控机日志，查看哪里出了问题。默认的日志目录是</p>

<p>关于如何在定义模式时添加表和列族属性的示例，可参见help命令的输出。</p>

<p>为了验证新表是否创建成功，运行list命令。这会输出用户空间中的所有表:</p>

<p>hbase(main):002:0&gt; list</p>

<p>TABLE</p>

<p>test</p>

<p>1 row(s) in 0.0260 seconds</p>

<p>要在列族data中三个不同的行和列上插入数据，获取第一行，然后列出表的内</p>

<p>容，输入如下:</p>

<table>
<thead>
<tr>
<th>hbase(main):003:0&gt;</th>
<th>put</th>
<th>■test、</th>
<th>•rowl </th>
<th>&lsquo;data:l、</th>
<th>&lsquo;valuel&rsquo;</th>
</tr>
</thead>

<tbody>
<tr>
<td>hbase(main):004:0&gt;</td>
<td>put</td>
<td>•test、</td>
<td>•row2&rsquo;,</td>
<td>■data^1,</td>
<td>&lsquo;value2’</td>
</tr>

<tr>
<td>hbase(main):005:0&gt;</td>
<td>put</td>
<td>•test、</td>
<td>’ row3、</td>
<td>’data:3、</td>
<td>vvalue3v</td>
</tr>

<tr>
<td>hbase(main):006:0&gt;</td>
<td>get</td>
<td>•test、</td>
<td>&lsquo;rowl&rsquo;</td>
<td></td>
<td></td>
</tr>

<tr>
<td>COLUMN</td>
<td>CELL</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<p>data:1    timestamp=1414927084811, value=valuel</p>

<p>1 row(s) in 0.0240 seconds hbase(main):007:0&gt; scan &lsquo;test&rsquo;</p>

<p>ROW    COLUMN+CELL</p>

<p>rowl</p>

<p>row2</p>

<p>row3</p>

<p>3 row(s) in 0.0240 seconds</p>

<p>column=data:l, timestamp=1414927084811, value=valuel column=data:2Jtimestamp=1414927125174, value=value2 column=data:3j timestamp=1414927131931, value=value3</p>

<p>请注意我们是如何在添加三个新列的时候不修改模式的。</p>

<p>为了移除这个表，首先要把它设为禁用，然后删除:</p>

<p>hbase(main):009:0&gt; 0 row(s) in 5.8420 hbase(main):010:0&gt; 0 row(s) in 5.2560 hbase(main):011:0&gt; TABLE</p>

<p>disable &lsquo;test seconds drop &lsquo;test&rsquo; seconds list</p>

<p>0 row(s) in 0.0200</p>

<p>seconds</p>

<p>通过运行以下命令来关闭HBase实例:</p>

<p>% stop-hbase.sh</p>

<p>要想了解如何设置分布式的HBase，并把它指向正运行的HDFS，请参见HBase 文档中有关 configuration section 的小节，网址为 <a href="http://hbase.apache.org/book">http://hbase.apache.org/book</a> /configuration.html 0</p>

<h5 id="20-4客户端">20.4客户端</h5>

<p>麝</p>

<p>和HBase集群进行交互，有很多种不同的客户端可供选择。</p>

<h6 id="20-4-1-java">20.4.1 Java</h6>

<p>HBase和Hadoop 一样，都是用Java开发的。范例20-1展示了 20.3.1节中在shell 环境下运行的hva实现版本。</p>

<p>范例20-1.基本的表管理与访问 public class ExampleClient {</p>

<p>public static void main(String[] args) throws IOException {</p>

<p>Configuration config = HBaseConfiguration.create();</p>

<p>// Create tabLe</p>

<p>HBaseAdmin admin = new HBaseAdmin(config); try {</p>

<p>TableName tableName = TableName.valueOf(&ldquo;test&rdquo;);</p>

<p>HTableDescriptor htd = new HTableDescriptor(tableName);</p>

<p>HColumnDescriptor hcd = new HColumnDescriptor(Hdata&rdquo;); htd•addFamily(hcd);</p>

<p>admin.createTable(htd);</p>

<p>HTableDescriptor^] tables = admin.listTables(); if (tables.length != 1 &amp;&amp;</p>

<p>Bytes.equals(tableName•getName(), tables[0].getTableName()•getName())) { throw new IOException(&ldquo;Failed create of table11);</p>

<p>I</p>

<p>}</p>

<p>// Run some operations &ndash; three puts, a get, and a scan &ndash; against the tabLe.</p>

<p>HTable table = new HTableCconfig^ tableName); try {</p>

<p>for (int i = 1; i &lt;= 3; i++) {</p>

<p>byte[] row = Bytes.toBytes(&ldquo;row&rdquo; + i);</p>

<p>Put put = new Put(row);</p>

<p>byte[] columnFamily = Bytes.toBytes(&ldquo;data&rdquo;); byte[] qualifier = Bytes.toBytes(String.valueOf(i)); byte[] value = Bytes.toBytes(&ldquo;value.1 + i); put.add(columnFamilyqualifier, value);</p>

<p>table.put(put);</p>

<p>}</p>

<p>Get get = new Get(Bytes.toBytes(n rowl&rdquo;));</p>

<p>Result result = table.get(get);</p>

<p>System.out.printIn(NGet: &ldquo; + result);</p>

<p>Scan scan = new Scan();</p>

<p>ResultScanner scanner = table.getScanner(scan); try {</p>

<p>for (Result scannerResult : scanner) {</p>

<p>System.out.println(HScan: &ldquo; + scannerResult);</p>

<p>}</p>

<p>} finally { scanner.close();</p>

<p>}</p>

<p>// Disable then drop the tabLe admin.disableTable(tableName); admin.deleteTable(tableName);</p>

<p>} finally {</p>

<p>table.close();</p>

<p>}</p>

<p>} finally {</p>

<p>admin.close();</p>

<p>}</p>

<p>} 、</p>

<p>}</p>

<p>关于 HBase 585</p>

<p>这个类只有一个主方法。为了简洁，我们没有给出导入包的信息。大多数HBase 类者P位于 org.apache.hadoop.hbase 和 org.apache.hadoop. hbase.client</p>

<p>包中。</p>

<p>在这个类中，我们首先让HBaseConfiguration类来创建Configuration对 象。这个类会返回一个Configuration，其中已经读入了程序classpath下 site.xml ft hbase-default.xml 文件中的 HBase 配置信息。这个 Configuration 接 下来会被用于创建HBaseAdmin和HTable实例。HBaseAdmin用于管理HBase集 群，添加和丢弃表。HTable则用于访问指定的表。Configuration实例将这些 类指向了执行这些代码的集群。</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-253.jpg" alt="img" /></p>

<p>从HBasel.O开始，新的客户端API更加干净且直观。HBaseAdmin和HTable的</p>

<p>构造函数已被弃用，不建议客户端显式引用这些旧类。取而代之的是，客户端 应该使用新的ConnectionFactory类创建一个Connection对象，然后根据需要 调用getAdmin()或getTable()来检索Admin或Table实例。连接管理以前是在 幕后为用户完成的，而现在则是客户端的职责。在本书的附带网站上可以找到 使用了新的API的本章示例的更新版本。</p>

<p>要创建一个表，我们需要首先创建一个HBaseAdmin的实例，然后让它来创建名 为test且只有一个列族data的表。在我们的示例中，表的模式是默认的。可以 使用HTableDescripttor和HColumnDescriptor中的方法来修改表的模式。接 下来的代码测试了表是否真的被创建了，如果没有，则抛出异常。</p>

<p>要对一个表进行操作，我们需要新建一个HTable的实例，并把我们的 Configuration实例和我们要操作的表的名称传递给它。然后，我们在循环中创 建Put对象，以便将数据插入到表中。Put把单个的单元格值valuen放入名为 rown的行的名为data:n的列上，其中n为1到3。列名通过两个部分指定：歹嗾名 和列族修饰词。上面这段代码使用了 HBase的Bytes实用类来把标识符和值转换为 HBase所需的字节数组。</p>

<p>接着，我们新建一个Get对象来获取和打印刚添加的第一行。然后，再使用Scan 聰来搬酗表，拼丁印攏課。</p>

<p>在程序的最后，我们首先禁用表，接着删除它，把这张表清除掉。我们曾提到过 在丢弃表前必须先禁用它。</p>

<p>扫描器</p>

<p>H Base扫描器(scanner)和传统数据库中的游标(cursor)或Java中的迭代器 (iterator)类似。它和后者的不同之处在于使用后需要关闭。扫描器按次序返回 数据行。用户使用已设置的Scan对象实例作为scan参数，调用 getScanner(),以此来获取HBase中一个表上的担描器。通过Scan实例， 你可以传递扫描开始位置和结束位置的数据行、返回结果中要包含的列以及运 行在服务器端的过滤器。ResultScanner接口是调用getScanner()时返回的 接口，它的定义如下：</p>

<p>public interface ResultScanner extends Closeable厂 Iterable<Result> { public Result next() throws IOException;</p>

<p>.public Result [] next(int nbRows) throws IOException; public void close();</p>

<p>}</p>

<p>可以查看接下来的一个或多个数据行。扫描器会在幕后每次获取100行数据， 把这些结果放在客户端，并只有在当前这批结果都被取光后才再去服务器端获 取下一批结果。以这种方式获取和缓存的行数是由hbase.client. scanner. caching配置选项所决定的，或者也可以通过setCaching()方法设置Scan 实例自己缓存(cache)/預取(prefetch)的行数。</p>

<p>较大的缓存值可以加速扫描器的运行，但也会在客户端使用较多的内存。而 且，还要避免把缓存值设得太高，因为它会导致客户端用于处理一批数据的时 间超出扫描器的超时时间。如果在扫描器超时之前，客户端没有能再次访问服 务器，那么扫描器在服务器端所用的资源会被服务器端的垃圾收集器自动回 收。默认的扫描器超时时间为60秒，它在hbase.client.scanner, timeout .period中设置。如果扫描器超时，客户端会收到一个 UnknownScannerException 异常。</p>

<p>编译这段程序的最简单的方法是使用本书示例代码附带的Maven POM。然后，我 们可以用后面跟着类名的hbase命令来运行程序，如下所示：</p>

<p>% mvn package</p>

<p>% export HBASE_CLASSPATH=hbase-examples.jar</p>

<p>% hbase ExampleClient</p>

<p>Get: keyvalues={rowl/data:l/1414932826551/Put/vlen=6/mvcc=0} Scan: keyvalues={rowl/data:l/1414932826551/Put/vlen=6/mvcc=0} Scan: keyvalues={row2/data:2/1414932826564/Put/vlen=6/mvcc=0} Scan: keyvalues={row3/data:3/1414932826566/Put/vlen=6/mvcc=0}</p>

<p>通过Result的toString()方法可以让每行输出显示一个HBase数据行。字段由</p>

<p>斜杠字符分隔且顺序如下：行名称，列名称，单元格时间戳，单元格类型，值的 字节数组(vlen)长度和一个内部HBase字段(mvcc)。稍后我们将看到如何使用 getValue()方法从Result对象中获取值。</p>

<h6 id="20-4-2-mapreduce">20.4.2 MapReduce</h6>

<p>org.apache.hadoop.hbase.mapreduce包中的类和工具有利于将HBase作为 MapReduce作业的源/输出。Tablel叩utFormat类可以在区域的边界进行分割， 使map能够拿到单个的区域进行处理。TableOutputFormat将把reduce的结果 写人HBase。</p>

<p>范例 20-2 中的 SimpleRowCounter 类(它是 HBase mapreduce 包中 RowCounter 类 的简化版本)使用TablelnputFormat来运行一个map任务，以计算行数。</p>

<p>范例20-2. 一个计算HBase表中行数的MapReduce应用程序</p>

<p>public class SimpleRowCounter extends Configured implements Tool {</p>

<p>static class RowCounterMapper extends TableMapper&lt;ImmutableBytesWritableJ Result〉 {</p>

<p>public static enum Counters { ROWS }</p>

<p>^Override</p>

<p>public void map(ImmutableBytesWritable row, Result value, Context context) { context.getCounter(Counters.ROWS).increment(1);</p>

<p>}</p>

<p>}</p>

<p>@0verride</p>

<p>public int run(String[] args) throws Exception { if (args.length != 1) {</p>

<p>System.err.pnintln(uUsage: SimpleRowCounter <tablename>H); return -1;</p>

<p>}</p>

<p>String tableName = args[0];</p>

<p>Scan scan = new Scan();</p>

<p>scan.setFilter(new FirstKeyOnlyFilter());</p>

<p>Job job = new 3ob(getConf()， getClass().getSimpleName()); job.setJarByClass(getClass());</p>

<p>TableMapReduceUtil.initTableMapper3ob(tableName, scan,</p>

<p>RowCounterMapper.class, ImmutableBytesWritable.class. Result.class, job);</p>

<p>job.setNumReduceTasks(O);</p>

<p>job.setOutputFonmatClass(NullOutputFormat.class); return job.waitForCompletion(true) ? 0 : 1;</p>

<p>}</p>

<p>public static void main(String[] args) throws Exception { int exitCode = ToolRunner.run(HBaseConfiguration.create()</p>

<p>new SimpleRowCounter(), args);</p>

<p>System.exit(exitCode);</p>

<p>嵌套类RowCounterMapper是HBase的TableMapper抽象类的一个子类。后者是</p>

<p>org.apache.hadoop.mapreduce.Mapper 的“特化” (specialization)0 它设定 map 的输入类型由TablelnputFormat来传递。输入的键为ImmutableBytesWritable对象 （行键），值为Result对象（扫描的行结果）。由于这个作业只对行进行计数，而且没有从 map中发送任何输出，因此我们仅为看到的每一行把Counters.ROWS的值递增1。</p>

<p>在run（）方法中，我们创建了一个扫描对象。这个扫描对象通过调用 TableMapReduceUtil.initTableMap]ob（）实用方法来对作业进行配置，它除了设 置输入格式到TablelnputFormat之外，还可以做其他一些事情（例如设置要使用 的map类）。</p>

<p>注意这里是如何设置扫描过滤器（即FirstKeyOnlyFilter实例）的。这个过滤器告 诉服务器，让它在运行服务器端任务时只用每行的第一个单元格来填充mapper中 的Result对象。由于mapper忽略了单元格值，因此这个优化是有意义的。</p>

<p>S也可以通过在HBase的shell环境中输入count ’tablename•命令来査询某个表 的行数。不过，它不是分布式命令，因此适用干MapReduce程序中的大 醜。</p>

<h6 id="20-4-3-rest-和-thrift">20.4.3 REST 和 Thrift</h6>

<p>HBase提供了 REST和Thrift接口。在使用Java以外的编程语言和HBase交互 时，会用到这些接口。在所有情况下，Java服务器上都运行着一个HBase客户端 实例，它负责协调REST和Thrift应用请求和HBase集群间的双向交互。有关服 务的运行以及客户端接口的详细情况，请参阅参考指南，网址为<a href="http://hbase">http://hbase</a>. apache.org/book.html Q</p>

<h5 id="20-5创建在线查询应用">20.5创建在线查询应用</h5>

<p>虽然HDFS和MapReduce是用于对大数据集进行批处理的强大工具，但对于读或 写单独的记录，效率却很低。在这个示例中，我们将看到如何用HBase来填补它 们之间的鸿沟。</p>

<p>前面几章描述的气象数据集包含过去100多年上万个气象站的观测数据。这个数 据集还在继续增长，它的大小几乎是无限的。在这个示例中，我们将构建一个简 单的在线查询（而不是批处理）界面，允许用户按时间顺序导航不同的观测站并浏览 历史气象观测值。我们将为此而构建一个简单的命令行Java应用程序，不过也不 难从中看出应当如何用相同的技术来构建一个具有同样效果的Web应用程序。</p>

<p>为此，让我们假设数据集非常大，观测数据达到上亿条记录，且气温更新数据到 达的速度很快——比如从全球观测站收到超过每秒几百到几千次更新。不仅如 此，我们还假设这个在线应用必须能够及时（most叩-to-date）显示观测数据，即在 收到数据后大约1秒就能进行显示。</p>

<p>对数据集的第一个要求使我们排除了使用RDBMS。HBase是一个可能的选择。对 于查询延时的第二个要求排除了直接使用HDFS。MapReduce作业可以用干建立 索引以支持对观测数据进行随机访问，但HDFS和MapReduce并不擅长在有更新 到达时维护索引。</p>

<h6 id="20-5-1模式设计">20.5.1模式设计</h6>

<p>在我们的示例中，有两个表。</p>

<p>\1. stations 表</p>

<p>这个表包含观测站数据。行的键是stationid。这个表还有一个列族info，它能</p>

<p>作为键/值字典来支持对观测站信息的査找。字典的键就是列名inf0:name、 info:location以及info:description。这个表是静态的，在这里，列族 info的设计类似于RDBMS中表的设计。</p>

<p>\2. observations 表</p>

<p>这个表存放气温观测数据。行的键是stationid和逆序时间戳构成的组合键。这 个表有一个列族data，它包含一列airtemp，其值为观测到的气温值。</p>

<p>对模式的选择取决于我们知道最高效的读取HBase的方式。行和列以字典序升序 保存。虽然有二级索引和正则表达式匹配工具，但它们会损失其他性能。清楚地 理解查询数据最高效的方式对于选择最有效的存储和访问数据的设置非常关键。</p>

<p>在stations表中，显然选择stationid作为键，因为我们总是根据特定站点的 ID来访问观测站的信息。但observations表使用的是一个组合键（把观测的时间 戳加在键之后）。这样，同一个观测站的观测数据就会被分组放到一起，使用逆序 时间戳(Long.MAX_VALUE - epoch)的二进制存储，系统把每个观测站观测数据中 最新的数据存储在最前面。</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-254.jpg" alt="img" /></p>

<p>站点ID是定长的这件事非常重要。在某些情况下我们需要对数字组件填零以 便行键能够正确排序，否则有可能会遇到一些问题，比如说在仅考虑按字节排 序时，10将会排在2之前，而02则排在10之前。</p>

<p>此外，如果键是整数，那么它会采用二进制来表示，而不是以数字的字符串版 本形式存储，因为前者消耗的空间更少。</p>

<p>在shell环境中，可以用以下方法来定义表：</p>

<p>hbase(main):001:0&gt; create &lsquo;stations&rsquo;{NAME =&gt; •info’}</p>

<p>0 row(s) in 0.9600 seconds</p>

<p>hbase(main):002:0&gt; create &lsquo;observations*{NAME =&gt; •data&rsquo;} 0 row(s) in 0.1770 seconds</p>

<p>在两个表中，我们都只对表单元格的最新版本感兴趣，所以把VERSIONS设为1 (默认值是3)。</p>

<h6 id="宽表-e调">宽表，e调</h6>

<p>在HBase中，所有访问都是通过主键的，因此在键的设计上应当尽量照</p>

<p>■</p>

<p>顾到如何查询这些数据。在对HBase这样的面向列(族)的存储 (http:&ldquo;en.wikipedia.org/ wiki/Column-oriented_DBMS)设计模氏时，另一件需 要记住的是它可以以极小的开销管理较宽的稀疏表。</p>

<p>HBase并没有内置对数据库连接的支持。但是宽表使我们并不需要让第一 个表和第二个表或第三个表进行数据库连接。一个宽行有时可以容下一个 主键相关的所有姗</p>

<p>人難筆耀:，4</p>

<p>,■If-r&lt;-.</p>

<h6 id="20-5-2加载数据">20.5.2加载数据</h6>

<p>观测站的数量相对较少，所以我们可以使用任何一种接口来插入这些观测站的静态 数据。在示例代码中包括一个用于执行此操作的Java应用程序，运行方式如下：</p>

<p>% hbase HBaseStationlmporter input/ncdc/metadata/stations-fixed-idth.txt</p>

<p>①引自 Daniel J. Abadi 的文章，标题为 “Column-Stores for Wide and Sparse Data”，发表于 2007 年 1 月，网址为 <a href="http://bit.ly/column-stores%e3%80%82">http://bit.ly/column-stores</a><a href="http://bit.ly/column-stores%e3%80%82">。</a></p>

<p>但是，假设我们要加载数十亿条观测数据。这种数据导入是一个极为复杂的过 程，是一个需要长时间运行的数据库操作。MapReduce和HBase的分布式模型让 我们可以充分利用集群。通过把原始输入数据复制到HDFS，接着运行 MapReduce作业，我们就能读到输人数据并将其写人HBase。</p>

<p>范例20-3展示了一个MapReduce作业，它将观测数据从前几章所用的输入文件导 入 HBaseo</p>

<p>范例20-3•从HDFS向HBase表导入气温数据的MapReduce应用</p>

<p>public class HBaseTemperaturelmporter extends Configured implements Tool {</p>

<p>參</p>

<p>static class HBaseTemperatureMapper<K> extends MapperxLongWritable, Text, K, Put&gt; {</p>

<p>private NcdcRecordParser parser = new NcdcRecordParser();</p>

<p>^Override</p>

<p>public void map(LongWritable key. Text value, Context context) throws</p>

<p>IOException, InterruptedException { parser.parse(value.toString()); if (parser.isValidTemperatureC)) {</p>

<p>byte[] rowKey =</p>

<p>RowKeyConverter.makeObservationRowKey(parser.getStationld (), parser.getObservationDate().getTime());</p>

<p>Put p = new Put(rowKey);</p>

<p>p.add(HBaseTemperatureQueny.DATA^COLUMNFAMILY^ HBaseTemperatureQuery.AIRTEMP—QUALIFIER,</p>

<p>Bytes.toBytes(parser.getAirTemperature()));</p>

<p>context.write(null4 p);</p>

<p>}</p>

<p>}</p>

<p>}</p>

<p>^Override</p>

<p>public int run(String[] args) throws Exception { if (args.length != 1) {</p>

<p>System.err.println(HUsage: HBaseTemperaturelmporter <input>•’)； return -1;</p>

<p>}</p>

<p>Job job = new 3ob(getConf()， getClass().getSimpleName()); job.set3arByClass(getClass());</p>

<p>FileInputFormat.addInputPath(job, new Path(args[0]));</p>

<p>job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, &ldquo;observations’ job.setMapperClass(HBaseTemperatureMapper.class); job.setNumReduceTasks(0);</p>

<p>job.setOutputFormatClass(TableOutputFormat.class); return job.waitForCompletion(true) ? 0 : 1;</p>

<p>}</p>

<p>public static void main(String[] args) throws Exception { int exitCode = ToolRunner.run(HBaseConfiguration.create()</p>

<p>new HBaseTemperatureImporter()， args);</p>

<p>System.exit(exitCode);</p>

<p>HBaseTemperatureimporter 有一个名为 HbaseTemperatureMapper 的嵌套 类，它类似于第6章的MaxTemperatureMaper类。外部类实现了 Tool，并 对调用这个仅含有map的作业进行设置。HBaseTemperatureMapper和 MaxTemperatureMapper的输入相同，所进行的解析方法——都使用第6章所介 绍的NcdcRecordParser来进行——也相同。解析时会检查输入是否为有效的气 温。但是不同于在MaxTemperatureMapper中仅把有效气温加到输出集合中，这 个类创建一个Put对象以便把有效的气温值添加到HBase的observations表的</p>

<p>data:airtemp 列。我们使用了从 HBaseTemperatureQuery 类中导出的 data 和</p>

<p>airtemp静态常量。后面对此会有介绍</p>

<p>我们所用的行的键在RowKeyConverter上的makeObservationRowKey()方法 中，用观测站ID和观测时间创建：</p>

<p>public class RowKeyConverter {</p>

<p>private static final int STATION^ID^LENGTH = 12;</p>

<p>/**</p>

<p>* ^return A roM key whose format is: <station一id>〈reverse—order一timestamp〉 */</p>

<p>public static byte[] makeObservationRowKey(String stationld, long observationTime) {</p>

<p>byte[] row = new byte[STATION_ID_LENGTH + Bytes.SIZEOF_LONG];</p>

<p>Bytes.putBytes(row, Q, Bytes.toBytes(stationld), 0, STATION一ID一LENGTH); long reverseOrderTimestamp = Long.MAX—VALUE - observationTime;</p>

<p>Bytes•putLong(rowJ STATION—ID—LENGTH, reverseOrderTimestamp); return row;</p>

<p>}</p>

<p>}</p>

<p>观测站ID其实是一个定长字符串。在转换时利用了这一点。与前面的例子一样， 我们使用HBase的Bytes类在字节数组和常用的Java类型之间进行转换。 Bytes. SIZEOF_LONG常量用于计算行键字节数组的时间戳部分的大小。 putBytes()和putLong()方法用于填充行键字节数组中的观测站ID和时间戳部 分，使它们处于相应的偏移位置。</p>

<p>该作业在run()方法中被配置为使用HBase的TableOutputFormat。将要写入的 表必须在作业配置中通过设置TableOutputFormat.OUTPUTTABLE属性来指定。</p>

<p>TableOutputFormat的使用为我们提供了方便，</p>

<p>为它负责管理HTable实例的</p>

<p>创建，否则我们需要在mapper的setup()方法中来做这件事(另外还需要在 cleanup()方法中调用 close())o TableOutputFormat 禁用了 HTable 的自动刷</p>

<p>新功能，</p>

<p>此可以缓存对put()的调用以提高效率。这段示例代码包括一个名为</p>

<p>HBaseTemperatureDirectlmporter 的类，以演示如何在 MapReduce 程序中直 接使用HTable。我们可以用下面的命令来运行程序：</p>

<p>% hbase HBaseTemperaturelmporter input/ncdc/all</p>

<p>\1.    加载的分布</p>

<p>要特别当心数据导入所引发的“步调一致”的情况。这时所有的客户端都对同一个 表的区域(在单个节点上)进行操作，然后再对下一个区域进行操作，依次进行。这 时加载操作并没有均匀地分布在所有区域上。这通常是由“排序后输入”(sorted i叩ut)和切分的原理共同造成的。如果在插入数据前，针对行键按数据排列的次序 进行随机处理，可能有助于减少这种情况。在我们的示例中，基于当前 stationid值的分布情况和TextlnputFormat分割数据的方式，上传操作应该 能够保证足够的分布式特性。</p>

<p>如果一个表是新的，一开始它只有一个区域。此时所有的更新都会加载到这个区 域，直到区域分裂为止。即使数据行的键是随机分布的，我们也无法避免这种情 况。这种启动现象意味着上传数据在开始时比较慢，直到有足够多的区域被分布 到各个节点，集群的成员都能够参与到上传为止。不要把这种情况与前面一段所 描述的情况混为一谈，它们是不同的。</p>

<p>这两个问题都可以通过使用批量加载来避免，下面我们将讨论批量加载。</p>

<p>\2.    批量加载</p>

<p>HBase有一个高效的“批量加载” (bulk loading)工具。它从MapReduce把以内部</p>

<p>格式表示的数据直接写入文件系统，从而实现批量加载。顺着这条路，我们加载 HBase实例的速度比用HBase客户端API写入数据的方式至少快一个数量级。</p>

<p>批量加载的处理过程分为两步。第一步使用HFileOutputFormat2通过一个 MapReduce作业将HFiles写入HDFS目录。由于数据行必须按顺序写入，因此 这个作业需要执行行键的完全排序(参见9.2.3节)。HFileOutputFormat2的 configureIncrementalLoad()方法可以完成所有必要的配置。</p>

<p>批量加载的第二步涉及将HFiles从HDFS移入现有的HBase表中。这张表在此</p>

<p>过程中可以是活跃的。在示例代码中包括了一个名为HBaseTemperature Bulklmporter的类用于以批量加载方法来加载气象观测数据。</p>

<h6 id="20-5-3在线查询">20.5.3在线查询</h6>

<p>为了实现在线查询应用，我们将直接使用HBase的Java API。在这里，我们将深 刻体会到选择模式和存储格式的重要性。</p>

<p>1.观测站信息查询</p>

<p>最简单的查询就是获取静态的观测站信息。这是一个单行的查找，通过使用get() 操作来执行。这一类査询在传统数据库中也很简单，但HBase提供了额外的控制 功能和灵活性。我们把info列族作为键/值字典(列名作为键，列值作为值)， HBaseStationQuery中的这段代码如下所示：</p>

<p>static final byte[] static final byte[] static final byte[] static final byte[]</p>

<p>INFO^COLUMNFAMILY = Bytes.toBytes(&ldquo;info&rdquo;);</p>

<p>NAME_QUALIFIER = Bytes.toBytes(&ldquo;name&rdquo;);</p>

<p>LOCATION一QUALIFIER = Bytes.toBytes(Hlocation H; DESCRIPTION一QUALIFIER = Bytes.toBytes(&ldquo;description&rdquo;);</p>

<p>public Map&lt;String, String〉 getStationInfo(HTable table, String stationld) throws IOException {</p>

<p>Get get = new Get(Bytes.toBytes(stationld)); get.addColumn(INFO_COLUMNFAMILY);</p>

<p>Result res = table.get(get); if (res == null) {</p>

<p>return null;</p>

<p>}</p>

<p>Map<String, String〉 resultMap = new HashMap<String, String>(); resultMap.put(” name” , getValue(res, INFO_COLUMNFAMILY, NAME一QUALIFIER)); resultMap.put(Hlocation&rdquo;, getValue(res, INFO^COLUMNFAMILY, LOCATION一QUALIFIER)); resultMap.put(’.description&rdquo;, getValue(res, INFO_COLUMNFAMILY,</p>

<p>DESCRIPTION-QUALIFIER)); return resultMap;</p>

<p>}</p>

<p>private static String getValue(Result res, byte [] cf, byte [] qualifier) { byte [] value = res.getValue(cfqualifier); return value == null? .’： Bytes.toString(value);</p>

<p>}</p>

<p>在这个示例中，getstationlnfo()接收一个HTable实例和一个观测站ID。为 了获取观测站的信息，我们使用HTable.get()来传递一个Get实例。它被设置 为用于获取已定义列族INFO_COLUMNFAMILY中由观测站ID所指明的列的值。</p>

<p>get()的结果在Result中返回。它包含数据行，你可以通过操作需要的列单元格</p>

<p>来取得单元格的值。getstationlnfo()方法把Result转换为更便于使用的由 String类型的键和值构成的Map。</p>

<p>我们已经看出在使用HBase时为什么需要工具函数了。在HBase上，为了处理底 层的交互，我们已经开发出越来越多的抽象。但是，理解它们的工作机理以及各 个存储选项之间的差异，非常重要。</p>

<p>和关系型数据库相比，HBase的优势之一是不需要我们预先设定列。所以在将 来，如果每个观测站在这三个必有的属性以外还有几百个可选的属性，我们便可 以直接插入这些属性而不需要修改模式。当然，你的应用中读和写的代码是需要 修改的。在示例中，我们可以循环遍历Result来获取每个值，而不用显式获取 各个值。</p>

<p>以下是观测站査询的示例：</p>

<p>% hbase HBaseStationQuery 011990-99999 name SIHCCA3AVRI location (unknown) description (unknown)</p>

<p>2.观测数据查询</p>

<p>对observations表的査询需要输入的参数包括站点ID、开始时间以及要返回的 最大行数。由于数据行是按观测站以观测时间逆序存储的，因此査询将返回发生</p>

<p>在开始时间之后的观察值。范例20-4中的getStationObservations()方法使用 HBase扫描器对表行进行遍历。它返回一个NavigableMap&lt;Long, Integer〉，</p>

<p>其中键是时间戳，值是温度。由于这个map按键的升序来排序，因此其中的数据 项是按时间顺序来排列的。</p>

<p>范例20-4.检索HBase表中某范</p>

<p>内气象站观测数据行的程序</p>

<p>public class HBaseTemperatureQuery extends Configured implements Tool { static final byte[] DATA_COLUMNFAMILY = Bytes.toBytes(ndataH); static final byte[] AIRTEMP一QUALIFIER = Bytes.toBytes(,,airtempM);</p>

<p>public NavigableMap&lt;LongJ Integer〉 getStationObservations(HTable table.</p>

<p>String stationld, long maxStamp, int maxCount) throws IOException {</p>

<p>byte[] startRow = RowKeyConverter.makeObservationRowKey(stationldmaxStamp); NavigableMap<Long, Integer〉 resultMap = new TreeMap<Long, Integer>();</p>

<p>Scan scan = new Scan(startRow);</p>

<p>scan.addColumn(DATA_COLUMNFAMILY, AIRTEMP^QUALIFIER);</p>

<p>ResultScanner scanner = table.getScanner(scan); try {</p>

<p>Result res;</p>

<p>int count = 0;</p>

<p>while ((res = scanner.next()) != null &amp;&amp; count++ &lt; maxCount) { byte[] row = res.getRow();</p>

<p>byte[] value = res.getValue(DATA_COLUMNFAMILY, AIRTEMP—QUALIFIER);</p>

<p>Long stamp = Long•MAX-VALUE •</p>

<p>Bytes.toLong(row, row.length - Bytes.SIZEOF—LCNG, Bytes.SIZEOF_LONG); Integer temp = Bytes.tolnt(value); resultMap.put(stamp, temp);</p>

<p>}</p>

<p>} finally { scanner.close();</p>

<p>}</p>

<p>return resultMap;</p>

<p>}</p>

<p>public int run(String[] args) throws IOException { if (args.length != 1) {</p>

<p>System.err.println(HUsage: HBaseTemperatureQuery 〈station—id&gt;&ldquo;)j return -1;</p>

<p>}</p>

<p>HTable table = new HTable(HBaseConfiguration.create(getConf()), &ldquo;observations&rdquo;);</p>

<p>NavigableMap<Long> Integer〉 observations = getStationObservationsCtable^ args[0],</p>

<p>Long.MAX-VALUE， 10).descendingMap(); for (Map.Entry&lt;LongJ Integer〉 observation : observations.entrySet()) {</p>

<p>// Print the date, time, and temperature</p>

<p>System.out.printf(H%l$tF %l$tR\t%2$s\nH, observation.getKey(), observation•getValue());</p>

<p>}</p>

<p>return 0;</p>

<p>} finally {</p>

<p>table.close();</p>

<p>}</p>

<p>}</p>

<p>public static void main(String[] args) throws Exception { int exitCode = ToolRunner.run(HBaseConfiguration.create()} new HBaseTemperatureQuery(), args);</p>

<p>System.exit(exitCode);</p>

<p>}</p>

<p>}</p>

<p>run()方法调用geStationObservations()以请求最近的10个观测值，并通过 调用descendingMap()使返回值仍然回归到降序。观测值被格式化并打印到控制 台(记住，温度的单位是十分之一度)。例如：</p>

<p>% hbase HBaseTemperatureQuery 011990-99999 1902-12-31 20:00 -106 1902-12-31 13:00 -83 1902-12-30 20:00 -78</p>

<p>1902-12-30</p>

<p>3 0 3 6 0 3 0 12 10 2 12</p>

<p>00 -100 00 -128 00 -111 00 -111 00 -117 00 -61</p>

<p>00 -22</p>

<p>1902-12-29</p>

<p>1902-12-29</p>

<p>1902-12-29</p>

<p>1902-12-28</p>

<p>1902-12-28</p>

<p>1902-12-27</p>

<p>按时间逆序存储时间戳的优点是，通常在线应用程序需要的是最新的观测值，而 这样做能更好地满足这一点。如果观测数据直接用实际的时间戳来存放，我们就 只能根据偏移量和限制范围高效地获取最老的观测数据。要获取最新的数据则意 味着要拿到所有的数据，直到最后才能获得结果。与此相比，获取前〃行然后退 出扫描程序(这种场景有时被称为“提早过滤”)的效率显然更高。</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-256.jpg" alt="img" /></p>

<p>HBase 0.98新增了反向扫描的能力，也就是说现在可以按时间顺序存储观测 值，并从给定的起始行开始反向扫描。反向扫描比正向扫描要慢几个百分点。 若要使用反向扫描，请在开始扫描之前调用Scan对象的setReversed(true)</p>

<p>方法。</p>

<h5 id="20-6-hbase-和-rdbms-的比较">20.6 HBase 和 RDBMS 的比较</h5>

<p>HBase和其他面向列的数据库常常被拿来和更流行的传统关系型数据库(或简写为 RDBMS)进行比较。虽然它们在实现和设计上的出发点有着较大的区别，但它们 都力图解决相同的问题。所以，虽然它们有很多不同点，但我们仍然能够对它们 进行客观的比较。</p>

<p>如前所述，HBase是一个分布式的、面向列的数据存储系统。它通过在HDFS上 提供随机读/写来解决Hadoop不能处理的问题。HBase自底层设计开始即聚焦干 各种可伸缩性问题：表可以很“高”(数十亿个数据行h表可以很“宽”(数百万 个列)；水平分区并在上千个普通商用机节点(commodity node)上自动复制。表的 模式是物理存储的直接反映，使系统有可能提供高效的数据结构的序列化、存储 和检索。但是，应用程序的开发者必须承担重任，选择以正确的方式使用这种存 储和检索方式。</p>

<p>严格来说，RDBMS 是一个遵循 “Codd 的 12 条规则” (<a href="http://en.wikipedia.org">http://en.wikipedia.org</a>/wi/d/Codd%27s_l2_rules)的数椐库。标准的RDBMS是模式固定、面向行的数据 库且具有ACID性质和复杂的SQL查询处理引擎。RDBMS强调事务的“强一致 性” (strong consistency)、参照完整性(referential integrity)、数据抽象与物理存储层</p>

<p>相对独立，以及基于SQL语言的复杂查询支持。在RDBMS中，可以非常容易地 建立“二级索引” (secondary index)，执行复杂的内连接和外连接，执行计数、求 和、排序、分组等操作，或对表、行和列中的数据进行分页存放。</p>

<p>对于大多数中小规模的应用，如MySQL和PostgreSQL之类现有开源RDBMS解 决方案所提供的易用性、灵活性、产品成熟度以及强大、完整的功能特性几乎是 无可替代的。但是，如果要在数据规模和并发读/写这两方面中的任何一个(或全部) 上进行大规模向上扩展(scale up),就会很快发现RDBMS的易用性会让你损失不 少性能，而如果要进行分布式处理，更是非常困难。RDBMS的扩展通常要求打破 Codd的规则，如放松ACID的限制，使DBA的管理变得复杂，并同时放弃大多 数关系型数据库引以为荣的易用性。</p>

<h6 id="20-6-1成功的服务">20.6.1成功的服务</h6>

<p>这里将简单介绍一个典型的RDBMS如何进行扩展 到大的生长过程。</p>

<p>下面给出一个成功服务从小</p>

<p>(1)    服务首次提供公开访问。</p>

<p>将服务从本地工作站迁移到拥有良好模式定义的、共享的远程MySQL实 例上。</p>

<p>(2)    服务越来越受欢迎；数据库收到太多的读请求。</p>

<p>用memcached来缓存常用查询结果。这时读不再是严格意义上的 ACID；缓存数据必须在某个时间到期。</p>

<p>(3)    对服务的使用继续增多；数据库收到太多的写请求。</p>

<p>通过购买一个16核、128GB RAM、配备一组15k RPM硬盘驱动器的增 强型服务器来垂直升级MySQL。非常昂贵。</p>

<p>(4)    新的特性增加了查询的复杂度；包含很多连接操作。</p>

<p>对数据进行反规范化以减少连接的次数。(这和DBA培训时所教的不一 样！)</p>

<p>(5)服务被广泛使用；所有的服务都变得非常慢。</p>

<p>停止使用任何服务器端计算(server-side computation)。</p>

<p>(6)有些查询仍然太慢。</p>

<p>定期对最复杂的查询进行“预物化” (prematerialize),并尝试在大多数情</p>

<p>况下停止使用连接。</p>

<p>(7)读性能尚可，但写仍然越来越慢。</p>

<p>放弃使用二级索引和触发器。(没有索引了？)</p>

<p>迄今为止，如何解决以上扩展问题并没有一个清晰的解决办法。无论怎样，都需 要开始橫向进行扩展。可以尝试在大表上进行某种分区或査看一些能提供多主控 机的商业解决方案。</p>

<p>无数应用、行业以及网站都成功实现了 RDBMS的可伸缩性、容错和分布式数据 系统。它们都使用了前面提到的很多策略。但最终，你所拥有的已经不再是一个 真正的RDBMS。由于妥协和复杂性问题，系统放弃了很多易用性特性。任何种类 的从属复本或外部缓存都会对反规范化的数据引入弱一致性(weak consistency)。连 接和二级索引的低效意味着绝大多数查询成为主键查找。而对于多写入机制 (multiwriter)的设置很可能意味着根本没有实际的连接，而分布式事务会成为一个 噩梦。这时，要管理一个单独用于缓存的集群，网络拓扑会变得异常复杂。即使 有一个做了那么多妥协的系统，你仍然会情不自禁地担心主控机崩溃或担心在几 个月后，数据或负载可能会增长到当前的10倍。</p>

<h6 id="20-6-2-hbase">20.6.2 HBase</h6>

<p>让我们考虑HBase，它具有以下特性。</p>

<p>没有真正的索引行是顺序存储的，每行中的列也是，所以不存在索引 膨胀的问题，而且插入性能和表的大小无关。</p>

<p>•自动分区在表增长的时候，表会自动分裂成区域，并分布到可用的节 点上。</p>

<p>•线性扩展和对于新节点的自动处理增加一个节点，把它指向现有集群 并运行regionserver。区域自动重新进行平衡，负载均匀分布。</p>

<p>•普通商用硬件支持集群可以用1000〜5000美金的单个节点搭建，而 不需要使用单个得花5万美金的节点。RDBMS需要支持大量1/0,因此 要求更昂贵的硬件。</p>

<p>•容错大量节点意味着每个节点的重要性并不突出。不用担心单个节点 失效。</p>

<p>批处理MapReduce集成功能使我们可以用全并行的分布式作业根据 C据的位置” (location awareness)来处理它们。</p>

<p>(4 ikZ</p>

<p>如果没日没夜地担心数据库(正常运行时间、扩展性问题、速度)，应该好好考虑从 RDBMS转向使用HBase0应该使用一个针对扩展性问题的解决方案，而不是性能 越来越差却需要大量投入的曾经可用的方案。有了 HBase,软件是免费的，硬件 是廉价的，而分布式处理则是与生俱来的。</p>

<h5 id="20-7-praxis">20.7 Praxis</h5>

<p>在这一小节，我们将讨论在应用中运行HBase集群时用户常常遇到的一些问题。</p>

<h6 id="20-7-1-hdfs">20.7.1 HDFS</h6>

<p>HBase使用HDFS的方式与MapReduce使用HDFS的方式截然不同。在 MapReduce中，首先打开HDFS文件，然后map任务流式处理文件的内容，最后 关闭文件。在HBase中，数据文件在启动时就被打开，并在处理过程中始终保持 打开状态，这是为了节省每次访问操作打开文件所需的代价。所以，HBase更容 易碰到MapReduce客户端不常碰到的问题：    .</p>

<p>1.文件描述符用完</p>

<p>由于我们在连接的集群上保持文件的打开状态，所以用不了太长时间就可能达到 系统和Hadoop设定的限制。例如，我们有一个由三个节点构成的集群，每个节点 上运行一个datanode实例和一个regionserver。如果我们正在运行一个加载任务， 表中有100个区域和10个列族。我们允许每个列族平均有两个“刷入文件”</p>

<p>(flush file)。通过计算，我们知道同时打开了 100x10x2,即2000个文件。此</p>

<p>外，还有各种外部扫描器和Java库文件占用了其他文件描述符。每个打开的文件 在远程datanode上至少占用一个文件描述符。</p>

<p>一个进程默认的文件描述符限制是1024。当我们使用的描述符个数超过文件系统 的Ww/Z值，我们会在日志中看到“Too many open files”(打开了太多文件)的错 误信息。但在这之前，往往就已经能看出HBase的行为不正常。要修正这个问题 需要增加文件描述符的ulimit参数值。有关如何增加集群的ulimit值，请参阅 HBase 参考指南，网址为 <a href="http://hbase.apache.org/book.html%e3%80%82">http://hbase.apache.org/book.html</a><a href="http://hbase.apache.org/book.html%e3%80%82">。</a></p>

<p>\2. datanode上的线程用完</p>

<p>和前面的情况类似，Hadoop 1的datanode上同时运行的线程数不能超过256这一 限制值(dfs.datanode.max.xcievers)，这个限制值会导致HBase异常运行。 Hadoop 2将默认值提高到4,096,因此最近几个版本的HBase(仅在Hadoop 2及更 高版本上运行)出现问题的可能性较小。可以通过在hdfs-site.xml中配置 dfs. datanode. max. transfer .threads(此属性的新名称)来更改设置。</p>

<h6 id="20-7-2用户界">20.7.2用户界</h6>

<p>HBase在主控机上运行了一个Web服务器，它能提供运行中集群的状态视图。在 默认情况下，它监听60010端口。主界面显示了基本的属性(包括软件版本、集群 负载、请求频率、集群表的列表)和加人的regionserver等。在主界面上单击选中 regionserver会把你带到那个regionserver上运行的Web服务器，它列出了这个服 务器上所有区域的列表及其他基本的属性值，比如如使用的资源和请求频率。</p>

<h6 id="20-7-3度量">20.7.3度量</h6>

<p>Hadoop有一个度量(metric)系统。可以用它每过一段时间获取系统重要组件的信 息，并输出到上下文(context),详情参见11.2.2节。启用Hadoop度量系统，并把 它捆绑入Ganglia或导出到JMX,就能得到集群上正在做和刚才做的事情的视 图。HBase也有它自己的度量(比如请求频率、组件计数、资源使用情况等)。相关 信息可以参见HBase conf目录下的hadoop-metrics2-properties文件。</p>

<h6 id="20-7-4计数器">20.7.4计数器</h6>

<p>在 StumbleUpon 公司(<a href="https://www.stufnbleupon.com/">https://www.stufnbleupon.com/)，</a>%ef%bc%8c%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%9c%a8)<a href="https://www.stufnbleupon.com/">第一个在</a>%ef%bc%8c%e7%ac%ac%e4%b8%80%e4%b8%aa%e5%9c%a8) HBase 上部署的产 品特性是为stumbleupon.com前端维护计数器。计数器以前存储在MySQL中， 但计数器的更新太频繁，计数器所导致的写操作太多，所以Web设计者必须对计 数值进行限定。使用HTable的incrementColumnValue()方法以后，计数器每 秒可以实现数千次更新。</p>

<h5 id="20-8延伸阅读">20.8延伸阅读</h5>

<p>本章对HBase做了简单介绍，并没有深入探讨HBase所具有的潜力。有关HBase</p>

<p>更详细的描述，请参阅O’Reilly在2011年出版的H从we: The DefinitiveGuide，讽 址为 <a href="http://hbase.apache.org/book.html%ef%bc%8c%e4%bd%9c%e8%80%85">http://hbase.apache.org/book.html，</a><a href="http://hbase.apache.org/book.html%ef%bc%8c%e4%bd%9c%e8%80%85">作者</a> Lars George，且其新版即将发行，或 者 Manning 出版社 2012 年在 HBase in Action，网址为 <a href="http://www.manning.com/">http://www.manning.com/</a> ditnidukkhurana/，作者 Nick Dimiduk 和 Amandeep Khurana。</p>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%AE%A1%E7%AE%97/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/hadoop/hadoop-%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/19-%E5%85%B3%E4%BA%8Espark/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">19 关于Spark</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%AE%A1%E7%AE%97/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/hadoop/hadoop-%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/22-%E5%8C%BB%E7%96%97%E5%85%AC%E5%8F%B8%E5%A1%9E%E7%BA%B3-cerner-%E7%9A%84%E5%8F%AF%E8%81%9A%E5%90%88%E6%95%B0%E6%8D%AE/">
            <span class="next-text nav-default">22 医疗公司塞纳 (Cerner) 的可聚合数据</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
