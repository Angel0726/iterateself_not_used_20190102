<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>05 Hadoop的IO操作 - 迭代自己</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="iterateself" />
  <meta name="description" content="Hadoop的I/O操作 Hadoop自带一套原子操作用于数据I/O操作。其中有一些技术比Hadoop本身更 常用，如数据完整性保持和压缩，但在" />

  <meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.47.1" />


<link rel="canonical" href="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%AE%A1%E7%AE%97/03-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-hadoop/hadoop-%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/05-hadoop%E7%9A%84io%E6%93%8D%E4%BD%9C/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<link href="/dist/even.min.css?v=3.2.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="05 Hadoop的IO操作" />
<meta property="og:description" content="Hadoop的I/O操作 Hadoop自带一套原子操作用于数据I/O操作。其中有一些技术比Hadoop本身更 常用，如数据完整性保持和压缩，但在" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://iterate.site/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%AE%A1%E7%AE%97/03-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-hadoop/hadoop-%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/05-hadoop%E7%9A%84io%E6%93%8D%E4%BD%9C/" /><meta property="article:published_time" content="2018-06-27T07:51:47&#43;00:00"/>
<meta property="article:modified_time" content="2018-06-27T07:51:47&#43;00:00"/>
<meta itemprop="name" content="05 Hadoop的IO操作">
<meta itemprop="description" content="Hadoop的I/O操作 Hadoop自带一套原子操作用于数据I/O操作。其中有一些技术比Hadoop本身更 常用，如数据完整性保持和压缩，但在">


<meta itemprop="datePublished" content="2018-06-27T07:51:47&#43;00:00" />
<meta itemprop="dateModified" content="2018-06-27T07:51:47&#43;00:00" />
<meta itemprop="wordCount" content="27926">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="05 Hadoop的IO操作"/>
<meta name="twitter:description" content="Hadoop的I/O操作 Hadoop自带一套原子操作用于数据I/O操作。其中有一些技术比Hadoop本身更 常用，如数据完整性保持和压缩，但在"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">迭代自己</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/recent/">
        <li class="mobile-menu-item">最新</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">关于</li>
      </a><a href="/catalog/">
        <li class="mobile-menu-item">完整目录</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">迭代自己</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/recent/">最新</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">关于</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/catalog/">完整目录</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">05 Hadoop的IO操作</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-06-27 </span>
        
        <span class="more-meta"> 27926 words </span>
        <span class="more-meta"> 56 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#hadoop的i-o操作">Hadoop的I/O操作</a>
<ul>
<li>
<ul>
<li><a href="#5-1数据完整性">5.1数据完整性</a>
<ul>
<li><a href="#5-1-1-hdfs的数据完整性">5.1.1 HDFS的数据完整性</a></li>
<li><a href="#5-1-2-localfilesystem">5.1.2 LocalFileSystem</a></li>
<li><a href="#5-1-3-checksumfilesystem">5.1.3 ChecksumFileSystem</a></li>
</ul></li>
<li><a href="#5-2压缩">5.2压缩</a>
<ul>
<li><a href="#5-2-1-codec">5.2.1 codec</a></li>
<li><a href="#5-2-2压缩和输入分片">5.2.2压缩和输入分片</a></li>
<li><a href="#5-2-3在mapreduce中使用压缩">5.2.3在MapReduce中使用压缩</a></li>
</ul></li>
<li><a href="#5-3序列化">5.3序列化</a>
<ul>
<li><a href="#5-3-1-writable-接口">5.3.1 Writable 接口</a></li>
<li><a href="#5-3-2-writable-类">5.3.2 Writable 类</a></li>
<li><a href="#5-3-3实现定制的writable集合">5.3.3实现定制的Writable集合</a></li>
<li><a href="#5-3-4序列化框架">5.3.4序列化框架</a></li>
</ul></li>
<li><a href="#5-4基于文件的数据结构">5.4基于文件的数据结构</a>
<ul>
<li><a href="#5-4-1-关于-sequencefile">5.4.1 关于 SequenceFile</a></li>
<li><a href="#5-4-2-关于-mapfile">5.4.2 关于 MapFile</a></li>
<li><a href="#5-4-3其他文件格式和面向列的格式">5.4.3其他文件格式和面向列的格式</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    

    
    <div class="post-content">
      

<h3 id="hadoop的i-o操作">Hadoop的I/O操作</h3>

<p>Hadoop自带一套原子操作用于数据I/O操作。其中有一些技术比Hadoop本身更 常用，如数据完整性保持和压缩，但在处理多达好几个TB的数据集时，特别值 得关注。其他一些则是Hadoop工具或API,它们所形成的构建模块可用干开发分 布式系统，比如序列化框架和在盘(on-disk)数据结构。</p>

<h5 id="5-1数据完整性">5.1数据完整性</h5>

<p>Hadoop用户肯定都希望系统在存储和处理数据时不会丢失或损坏任何数据。尽管 磁盘或网络上的每个I/O操作不太可能将错误引入自己正在读/写的数据中，但是 如果系统中需要处理的数据量大到Hadoop的处理极限时，数据被损坏的概率还是 很高的。</p>

<p>检测数据是否损坏的常见措施是，在数据第一次引入系统时计算校验和(checksum)并在 数据通过一个不可靠的通道进行传输时再次计算校验和，这样就能发现数据是否 损坏。如果计算所得的新校验和与原来的校验和不匹配，我们就认为数据已损 坏。但该技术并不能修复数据——它只能检测出数据错误。(这正是不使用低端硬件的 原因。具体说来，一定要使用ECC内存。)注意，校验和也是可能损坏的，不只 是数据，但由于校验和比数据小得多，所以损坏的可能性非常小。</p>

<p>常用的错误检测码是CRC-32(32位循环冗余校验)，任何大小的数据输入均计算得 到一个32位的整数校验和。Hadoop ChecksumFileSystem使用CRC-32计算校验 和，HDFS用于校验和计算的则是一个更有效的变体CRC-32C。</p>

<h6 id="5-1-1-hdfs的数据完整性">5.1.1 HDFS的数据完整性</h6>

<p>HDFS会对写入的所有数据计算校验和，并在读取数据时验证校验和。它针对每 个由dfs.bytes-per-checksum指定字节的数据计算校验和。默认情况下为512 个字节，由于CRC-32校验和是4个字节，所以存储校验和的额外开销低于1%。</p>

<p>datanode负责在收到数据后存储该数据及其校验和之前对数据进行验证。它在收 到客户端的数据或复制其他datanode的数据时执行这个操作。正在写数据的客户 端将数据及其校验和发送到由一系列datanode组成的管线(详见第3章)，管线中 最后一个datanode负责验证校验和。如果datanode检测到错误，客户端便会收到 一个IOException异常的一个子类，对于该异常应以应用程序特定的方式来处 理，比如重试这个操作。</p>

<p>客户端从datanode读取数据时，也会验证校验和，将它们与datanode中存储的校 验和进行比较。每个datanode均持久保存有一个用于验证的校验和日志(persistent log of checksum verification),所以它知道每个数据块的最后一次验证时间。客户 端成功验证一个数据块后，会告诉这个dataiKKle，datanode由此更新日志。保存这些 统计信息对于检测损坏的磁盘很有价值。</p>

<p>不只是客户端在读取数据块时会验证校验和，每个datanode也会在一个后台线程 中运行一个DataBlockScanner,从而定期验证存储在这个datanode上的所有数 据块。该项措施是解决物理存储媒体上位损坏的有力措施。11-1.4节将详细描述如何 访问扫描报告。</p>

<p>由于HDFS存储着每个数据块的复本(replica)，因此它可以通过数据复本来修复损 坏的数据块，进而得到一个新的、完好无损的复本。基本思路是，客户端在读取 数据块时，如果检测到错误，首先向namenode报告已损坏的数据块及其正在尝试 读操作的这个datanode,再抛出ChecksumException异常。namenode将这个数</p>

<p>据块复本标记为已损坏，这样它不再将客户端处理清求直接发送到这个节点，或 尝试将这个复本复制到另一个datanode0之后，它安排这个数据块的一个复本复 制到另一个datanode，如此一来，数据块的复本因子(replication factor)又回到期望 水平。此后，已损坏的数据块复本便被删除。</p>

<p>在使用open()方法读取文件之前，将false值传递给FileSystem对象的 setVerifyChecksum()方法，即可以禁用校验和验证。如果在命令解释器中使用 带-get选项的-ignoreCrc命令或者使用等价的-copyToLocal命令，也可以达 到相同的效果。如果有一个已损坏的文件需要检查并决定如何处理，这个特性是 非常有用的。例如，也许你希望在删除该文件之前尝试看看是否能够恢复部分</p>

<p>也L丄口</p>

<p>可以用hadoop的命令fs -checksum来检查一个文件的校验和。这可用于在 HDFS中检查两个文件是否具有相同内容，命令也具有类似的功能。详情可 以参见3.7节。</p>

<h6 id="5-1-2-localfilesystem">5.1.2 LocalFileSystem</h6>

<p>Hadoop的LocalFileSystem执行客户端的校验和验证。这意味着在你写入一个 名为州ename的文件时，文件系统客户端会明确在包含每个文件块校验和的同一 个目录内新建一个隐藏文件。文件块的大小由属性file.bytes-per-checksum控制，默认为512个字节。文件块的大小作为元数据存储在.crc文件 中，所以即使文件块大小的设置已经发生变化，仍然可以正确读回文件。在读取 文件时需要验证校验和，并且如果检测到错误，LocalFileSystem还会抛出一个 ChecksumException 异常。</p>

<p>校验和的计算代价是相当低的（在Java中，它们是用本地代码实现的），一般只是 增加少许额外的读/写文件时间。对大多数应用来说，付出这样的额外开销以保证 数据完整性是可以接受的。此外，我们也可以禁用校验和计算，特别是在底层文 件系统本身就支持校验和的时候。在这种情况下，使用RawLocalFileSystem替 代LocalFileSystem。要想在一个应用中实现全局校验和验证，需要将 fs.file.impl 属性设置为 org.apache.hadoop. fs.RawLocalFileSystem 进</p>

<p>而实现对文件URI的重新映射。还有一个可选方案可以直接新建一个 RawLocalFileSystem实例。如果想针对一些读操作禁用校验和，这个方案非常 有用。示例如下：</p>

<p>Configuration conf =…</p>

<p>FileSystem fs = new RawLocalFileSystem(); fs.initializeCnull^ conf);</p>

<h6 id="5-1-3-checksumfilesystem">5.1.3 ChecksumFileSystem</h6>

<p>LocalFileSystem通过ChecksumFileSystem来完成自己的任务，有了这个</p>

<p>类，向其他文件系统（无校验和系统）加入校验和就非常简单，</p>

<p>ChecksumFileSystem 类继承自 FileSystem 类。一般用法如下:</p>

<p>FileSystem rawFs =…</p>

<p>FileSystem checksummedFs = new ChecksumFileSystem(rawFs);</p>

<p>底层文件系统称为“源”(raw)文件系统，可以使用ChecksumFileSystem实例的 getRawFileSystem()方法获取它。ChecksumFileSystem类还有其他一些与校 验和有关的有用方法，比如getChecksumFile()可以获得任意一个文件的校验和 文件路径。请参考文档了解其他方法。</p>

<p>如果ChecksumFileSystem类在读取文件时检测到错误，会调用自己的 r印ortChecksumFailure()方法。默认实现为空方法，但LocalFileSystem类会将这 个出错的文件及其校验和移到同一存储设备上一个名为badJiles的边际文件夹 (side directory)中。管理员应该定期检查这些坏文件并采取相应的行动。</p>

<h5 id="5-2压缩">5.2压缩</h5>

<p>文件压缩有两大好处：减少存储文件所需要的磁盘空间，并加速数据在网络和磁 盘上的传输。这两大好处在处理大量数据时相当重要，所以我们值得仔细考虑在 Hadoop中文件压缩的用法。</p>

<p>有很多种不同的压缩格式、工具和算法，它们各有千秋。表5-1列出了与Hadoop 结合使用的常见压缩方法。</p>

<p>表5-1.压缩格式总结</p>

<table>
<thead>
<tr>
<th>压缩格式DEFLATE^</th>
<th>工具无</th>
<th>算法DEFLATE</th>
<th>文件扩展名 .deflate</th>
<th>是否可切分否</th>
</tr>
</thead>

<tbody>
<tr>
<td>gzip</td>
<td>gzip</td>
<td>DEFLATE</td>
<td>•gz</td>
<td>否</td>
</tr>

<tr>
<td>bzip2</td>
<td>bzip2</td>
<td>bzip2</td>
<td>.bz2</td>
<td>是</td>
</tr>

<tr>
<td>LZO</td>
<td>lzop</td>
<td>LZO</td>
<td>•lzo</td>
<td>否⑦</td>
</tr>

<tr>
<td>LZ4</td>
<td>无</td>
<td>LZ4</td>
<td>•lz4</td>
<td>否</td>
</tr>

<tr>
<td>Snappy</td>
<td>无</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
</tr>
</tbody>
</table>

<p>①    DEFLATE是一个标准压缩算法，该算法的标准实现是zlib。没有可用于生成DEFLATE文件的常 用命令行工具，因为通常都用gzip格式。注意，gzip文件格式只是在DEFLATE格式上增加了文 件头和一个文件尾。.deflate文件扩展名是Hadoop约定的。</p>

<p>②    但是如果LZO文件已经在预处理过程中被索引了，那么LZO文件是可切分的。详情参见 5.2.2 节。</p>

<p>所有压缩算法都需要权衡空间/时间：压缩和解压缩速度更快，其代价通常是只能 节省少量的空间。表5-1列出的所有压缩工具都提供9个不同的选项来控制压缩 时必须考虑的权衡：选项-1为优化压缩速度，-9为优化压缩空间。例如，下述命令 通过最快的压缩方法创建一个名为片&amp;识的压缩文件：</p>

<p>%gzip -1 file</p>

<p>不同压缩工具有不同的压缩特性。gzip是一个通用的压缩工具，在空间/时间性能 的权衡中，居于其他两个压缩方法之间。bzip2的压缩能力强于gzip,但压缩速度 更慢一点。尽管bzip2的解压速度比压缩速度快，但仍比其他压缩格式要慢一些。 另一方面，LZO、LZ4和Snappy均优化压缩速度，其速度比gzip快一个数量级， 但压缩效率稍逊一筹。Snappy和LZ4的解压缩速度比LZO高出很多。®</p>

<p>表5-1中的“是否可切分”列表不对应的/土缩算法是否支持切分(splitable)，也就 是说，是否可以搜索数据流的任意位置并进一步往下读取数据。可切分压缩格式尤 其适合MapReduce，更多讨论，可以参见5.2.2节。</p>

<h6 id="5-2-1-codec">5.2.1 codec</h6>

<p>Codec是压缩-解压缩算法的一&rsquo;种实现。在Hadoop中，一个对 CompressionCodec 接口的实现代表一个 codec。例如，GzipCodec 包装了 gzip 的压缩和解压缩算法。表5-2列举了 Hadoop实现的codec0</p>

<p>表 5-2. Hadoop 的压缩 codec</p>

<table>
<thead>
<tr>
<th>压缩格式 DEFLATE</th>
<th>HadoopCompressionCodecorg.apache.hadoop.io.compress.DefaultCodec</th>
</tr>
</thead>

<tbody>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>

<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>

<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>

<tr>
<td>LZ4</td>
<td>org.apache.hadoop.io.compress.Lz4Codec</td>
</tr>

<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody>
</table>

<p>LZO代码库拥有GPL许可，因而可能没有包含在Apache的发行版本中，因此， Hadoop 的 codec 需要单独从 GQQg\e(<a href="http://code.google.com/p/hadoop-gpl-compression">http://code.google.com/p/hadoop-gpl-compression</a>)</p>

<p>①对于综合性压缩测试集，可以参考jvm-cotnpressor-benchmark,这里针对JVM兼容类库(包括 一些原始类库)提供了相当不错的介绍。</p>

<p>或 GitHub(<a href="http://github.com/kevimveil/hadoop-lzoyp%e8%bd%bd%ef%bc%8c%e8%af%a5%e4%bb%a3%e7%a0%81%e5%ba%93%e5%8c%85%e5%90%ab%e6%9c%89%e4%bf%ae%e6%ad%a3%e7%9a%84%e8%bd%af%e4%bb%b6">http://github.com/kevimveil/hadoop-lzoyp</a><a href="http://github.com/kevimveil/hadoop-lzoyp%e8%bd%bd%ef%bc%8c%e8%af%a5%e4%bb%a3%e7%a0%81%e5%ba%93%e5%8c%85%e5%90%ab%e6%9c%89%e4%bf%ae%e6%ad%a3%e7%9a%84%e8%bd%af%e4%bb%b6">载，该代码库包含有修正的软件</a> 错误及其他一些工具。LzopCodec与/zop工具兼容，LzopCodec基本上是LZO格 式的但包含额外的文件头，因此这通常就是你想要的。也有针对纯LZO格式的 LzoCodec，并使用作为文件扩展名(类似于DEFLATE，是gzip格式但不 包含文件头)。</p>

<p>1.通过CompressionCodec对数据流进行压缩和解压缩</p>

<p>CompressionCodec包含两个函数，可以轻松用于压缩和解压缩数据。如果要对写 入输出数据流的数据进行压缩，可用createOutputStream (OutputStream out)</p>

<p>方法在底层的数据流中对需要以压缩格式写入在此之前尚未压缩的数据新建一个 CompressionOutputStream对象。相反，对输人数据流中读取的数据进行解压缩的 时候，贝IU周用 createInputStream(InputStream in)获取 CompressionlnputStream, 可以通过该方法从底层数据流读取解压缩后的数据。</p>

<p>CompressionOutputStream 和 CompressionlnputStream,类似于 java.util. zip.DeflaterOutputStream 和 java.util.zip.DeflaterInputStream,只不</p>

<p>过前两者能够重置其底层的压缩或解压缩方法，对于某些将部分数据流(section of data stream)压缩为单独数据块(block)的应用，例如SequenceFile(详情参见5.4.1 节对SequenceFile类的讨论)，这个能力是非常重要的。</p>

<p>范例5-1显示了如何用API来压缩从标准输入中读取的数据并将其写到标准 输出。</p>

<p>范例5-1.该程序压缩从标准输入读取的数据，然后将其写到标准输出 public class StreamCompressor {</p>

<p>public static void main(String[] args) throws Exception {</p>

<p>String codecClassname = args[0];</p>

<p>Class&lt;?&gt; codecClass = Class.forName(codecClassname);</p>

<p>Configuration conf = new Configuration();</p>

<p>CompressionCodec codec = (CompressionCodec)</p>

<p>ReflectionUtils.newlnstance(codecClass, conf);</p>

<p>CompressionOutputStream out = codec.createOutputStream(System.out);</p>

<p>IOUtils.copyBytes(System.inout, 4096, false); out.finish();</p>

<p>}</p>

<p>}</p>

<p>该应用希望将符合CompressionCodec实现的完全合格名称作为第一个命令行参 数。我们使用ReflectionUtils新建一个codec实例，并由此获得在System.out上支持 压缩的一个包裹方法。然后，对IOUtils对象调用copy Bytes ()方法将输入胃复制到输 出，(输出由CompressionOutputStream对象压缩)。最后，我们对 CompressionOutputStream对象调用finish()方法，要求压缩方法完成到压缩</p>

<p>数据流的写操作，但不关闭这个数据流。我们可以用下面这行命令做一个测试，通过 GzipCodec的StreamCompressor对象对字符串“Text”进行压缩，然后使用 从标准输入中对它进行读取并解压缩操作：</p>

<p>% echo &ldquo;Text&rdquo; | hadoop StreamCompressor org•apache•hadoop•io• compress.GzipCodec \ I gunzip</p>

<p>Text</p>

<p>2.通过 CompressionCodecFactory 推断 CompressionCodec</p>

<p>在读取一个压缩文件时，通常可以通过文件扩展名推断需要使用哪个codec。如果 文件以.gz结尾，则可以用GzipCodec来读取，如此等等。前面的表5-1为每一种 压缩格式列举了文件扩展名。</p>

<p>通过使用其getCodec()方法，CompressionCodecFactory提供了一种可以将文 件扩展名映射到一个CompressionCodec的方法，该方法取文件的Path对象作 为参数。范例5-2所示的应用便使用这个特性来对文件进行解压缩</p>

<p>范例5-2.该应用根据文件扩展名选取codec解压缩文件</p>

<p>public class FileDecompressor {</p>

<p>public static void main(String[] args) throws Exception {</p>

<p>String uri = args[0];</p>

<p>Configuration conf = new Configuration。；</p>

<p>FileSystem fs = FileSystem.get(URI.create(uriconf);</p>

<p>Path inputPath = new Path(uri);</p>

<p>CompressionCodecFactory factory = new CompressionCodecFactory(conf); CompressionCodec codec = factory.getCodec(inputPath); if (codec == null) {</p>

<p>System. err. print In (HNo codec found for •• + uri);    *</p>

<p>System.exit(l);</p>

<p>}</p>

<p>String outputUri =</p>

<p>CompressionCodecFactory.removeSuffix(uricodec.getDefaultExtension());</p>

<p>InputStream in = null;</p>

<p>OutputStream out = null; try {</p>

<p>in = codec.createlnputStream(fs.open(inputPath)); out = fs.create(new Path(outputUri));</p>

<p>IOUtils.copyBytes(in， out， conf);</p>

<p>} finally {</p>

<p>IOUtils.closestream（in）;</p>

<p>IOUtils.closeStream（out）;</p>

<p>}</p>

<p>}</p>

<p>}</p>

<p>一旦找到对应的codec ,便去除文件扩展名形成输出文件名，这是通过 CompressionCodecFactory对象的静态方法removeSuffix（）来实现的。按照 这种方法，一个名为力/e.gz的文件可以通过调用该程序解压为名为力/e的文件：</p>

<p>% hadoop FileDecompressor file.gz</p>

<p>CompressionCodecFactory加载表5-2中除LZO之外的所有codec,同样也加载 io.compression.codecs配置属性（参见表5-3）列表中的所有codec。在默认情况 下，该属性列表是空的，你可能只有在你拥有一个希望注册的定制codec（例如外 部管理的LZO codec）时才需要加以修改。每个codec都知道自己默认的文件扩展 名，因此CompressionCodecFactory可通过搜索注册的codec找到匹配指定文 件扩展名的codec（如果有的话）。</p>

<p>表5-3.压缩codec的属性</p>

<p>黑滿腿</p>

<p>用于压缩/解压缩的额外的CompressionCodec 类的列表.</p>

<p>属性名称    类型</p>

<p>io. compression. codecs 逗号分隔的类名</p>

<p>3.原生类库</p>

<p>为了提高性能，最好使用“原生”（native）类库来实现压缩和解压缩。例如，在一 个测试中，使用原生gzip类库可以减少约一半的解压缩时间和约10%的压缩时间（与内置 的Java实现相比）。表5-4说明了每种压缩格式是否有Java实现和原生类库实现。所有的 格式都有原生类库实现，但是并非所有格式都有Java实现（如LZO）。</p>

<p>表5-4.压缩代码库的实现</p>

<table>
<thead>
<tr>
<th>压缩格式DEFLATE</th>
<th>是否有Java实现是</th>
<th>是否、有原生实现是</th>
</tr>
</thead>

<tbody>
<tr>
<td>gzip</td>
<td>是</td>
<td>是</td>
</tr>

<tr>
<td>bzip2</td>
<td>是</td>
<td>否</td>
</tr>

<tr>
<td>LZO</td>
<td>否</td>
<td>是</td>
</tr>

<tr>
<td>LZ4</td>
<td>否</td>
<td>是</td>
</tr>

<tr>
<td>Snappy</td>
<td>否</td>
<td>是</td>
</tr>
</tbody>
</table>

<p>Apache Hadoop二进制压缩包本身包含有为64位Linux构建的原生压缩二进制代</p>

<p>码，称为libhadoop.so0对于其他平台，需要自己根据位于源文件树最顶层的 BUILDING.txt指令编译代码库。</p>

<p>可以通过Java系统的java.library.path属性指定原生代码库。etc/hadoop文 件夹中的/Z67而即脚本可以帮你设置该属性，但如果不用这个脚本，则需要在应用 中手动设置该属性。</p>

<p>默认情况下，Hadoop会根据自身运行的平台搜索原生代码库，如果找到相应的代 码库就会自动加载。这意味着，你无需为了使用原生代码库而修改任何设置。但 是，在某些情况下，例如调试一个压缩相关问题时，可能需要禁用原生代码库。 将属性io.native.lib.available的值设置成false即可，这可确保使用内置 的Java代码库（如果有的话）。</p>

<p>\4. CodecPool</p>

<p>如果使用的是原生代码库并且需要在应用中执行大量压缩和解压缩操作，可以考 虑使用CodecPool,它支持反复使用压缩和解压缩，以分摊创建这些对象的 开销。</p>

<p>范例5-3中的代码显示了 API函数，不过在这个程序中.，它只新建了一个 Compressor,并不需要使用压缩/解压缩池。</p>

<p>范例5-3.使用压缩池对读取自标准输入的数据进行压缩，然后将其写到标准输出</p>

<p>public class PooledStreamCompressor {</p>

<p>public static void main(String[] args) throws Exception { String codecClassname = args[0];</p>

<p>Class&lt;?&gt; codecClass = Class.forName(codecClassname); Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec)</p>

<p>ReflectionUtils.newlnstance(codecClass, conf); Compressor compressor = null; try {</p>

<p>compressor = CodecPool.getCompressor(codec);</p>

<p>CompressionOutputStream out =</p>

<p>codec.createOutputStream(System.out&gt; compressor);</p>

<p>IOUtils.copyBytes(System.inout, 4096, false); out.finish();</p>

<p>} finally {</p>

<p>CodecPool.returnCompressor(compressor);</p>

<p>}</p>

<p>}</p>

<p>}</p>

<p>在codec的重载方法createOutputStream()中，对于指定的CompressionCodec，我们从池中 获取一个Compressor实例。通过使用finally数据块，我们在不同的数据流之 间来回复制数据，即使出现IOException异常，也可以确保compressor可以返回</p>

<p>池中。</p>

<h6 id="5-2-2压缩和输入分片">5.2.2压缩和输入分片</h6>

<p>在考虑如何压缩将由MapReduce处理的数据时，理解这些压缩格式是否支持切分 (splitting)是非常重要的。以一个存储在HDFS文件系统中且压缩前大小为1 GB的 文件为例。如果HDFS的块大小设置为128 MB,那么该文件将被存储在8个块 中，把这个文件作为输入数据的MapReduce作业，将创建8个输入分片，其中每 个分片作为一个单独的map任务的输入被独立处理。</p>

<p>现在想象一下，文件是经过gzip压缩的，且压缩后文件大小为1 GB。与以前一 样，HDFS将这个文件保存为8个数据块。但是，将每个数据块单独作为一个输 入分片是无法实现工作的，因为无法实现从gzip压缩数据流的任意位置读取数 据，所以让map任务独立于其他任务进行数据读取是行不通的。gzip格式使用 DEFLATE算法来存储压缩后的数据，而DEFLATE算法将数据存储在一系列连续 的压缩块中。问题在于每个块的起始位置并没有以任何形式标记，所以读取时无 法从数据流的任意当前位置前进到下一块的起始位置读取下一个数据块，从而实现 与整个数据流的同步。由于上述原因，gzip并不支持文件切分。</p>

<p>三这种情况下，MapReduce会采用正确的做法，它不会尝试切分gzip压缩文件， ］为它知道输入是gzip压缩文件(通过文件扩展名看出)且gzip不支持切分。这是 「行的，但牺牲了数据的本地性：一个map任务处理8个HDFS块，而其中大多 C块并没有存储在执行该map任务的节点上。而且，map任务数越少，作业的粒 ［就较大，因而运行的时间可能会更长。</p>

<p>在前面假设的例子中，如果文件是通过LZO压缩的，我们会面临相同的问题，</p>

<p>为这个压缩格式也不支持数据读取和数据流同步。但是，在预处理LZO文件的时 候使用包含在Hadoop LZO库文件中的索引工具是可能的，你可以在5.2.1所列的 Google和GitHub网站上获得该类库。该工具构建了切分点索引，如果使用恰当 的MapReduce输入格式可有效实现文件的可切分特性。</p>

<p>另一方面，bzip2文件提供不同数据块之间的同步标识(pi的48位近似值)，因而它</p>

<p>支持切分。可以参见表5-1，了解每个压缩格式是否支持切分。</p>

<p>应该使用哪种压缩格式?</p>

<p>Hadoop应用处理的数据集非常大，因此需要借助于压缩。使用哪种压缩格式与 待处理的文件的大小、格式和所使用的工具相关。下面有一些建议，大致是按 照效率从高到低排列的。</p>

<p>•    使用容器文件格式，例如顺序文件（见5.4.1节）、Avro数据文件（参见 12.3 节）、ORCFiles（见 5.4.3 节）或者 Parquet 文件（参见 13.2 节），所 有这些文件格式同时支持压缩和切分。通常最好与一个快速压缩工具联 合使用，例如LZO，LZ4，或者Snappy。</p>

<p>•    使用支持切分的压缩格式，例如bzip2（尽管bzip2非常慢），或者使用 通过索引实现切分的压缩格式，例如LZO。</p>

<p>•    在应用中将文件切分成块，并使用任意一种压缩格式为每个数据块建 立压缩文件（不论它是否支持切分）。这种情况下，需要合理选择数据 块的大小，以确保压缩后数据块的大小近似于HDFS块的大小。</p>

<p>•    存储未经压缩的文件。</p>

<p>对大文件来说，不要使用不支持切分整个文件的压缩格式，因为会失去数据的 本地特性，进而造成MapReduce应用效率低下。</p>

<h6 id="5-2-3在mapreduce中使用压缩">5.2.3在MapReduce中使用压缩</h6>

<p>前面讲到通过CompressionCodecFactory来推断CompressionCodec时指出，如果输 入文件是压缩的，那么在根据文件扩展名推断出相应的codec后，MapReduce会 在读取文件时自动解压缩文件。</p>

<p>要想压缩MapReduce作业的输出，应在作业配置过程中将mapreduce. output.fileoutputformat.compress 属性设为 true， 将 mapre-duce. output.fileoutputformat.compress.codec 属性设置为打算使用的压缩 codec 的 类名。另一种方案是在FileOutputFormat中使用更便捷的方法设置这些属性，如范 例5-4所示。</p>

<p>范例5-4.对查找最高气温作业所产生输出进行压缩</p>

<p>public class MaxTemperatureWithCompression {</p>

<p>public static void main(String[] args) throws IOException { if (args.length != 2) {</p>

<p>System.err.println(HUsage: MaxTemperatureWithCompression 〈input path〉 + &ldquo;〈output path〉&rdquo;)；</p>

<p>System.exit(-l);    -</p>

<p>}</p>

<p>Dob job = new ]ob();</p>

<p>job.setDarByClass(MaxTemperature.class);</p>

<p>FilelnputFormat.addInputPath(job^ new Path(args[0]));</p>

<p>FileOutputFormat.setOutputPath(job, new Path(args[l]));</p>

<p>job.setOutputKeyClass(Text.class);</p>

<p>job.setOutputValueClass(IntWritable.class);</p>

<p>曹</p>

<p>FileOutputFormat，setCompressOutput(job, true);</p>

<p>FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</p>

<p>job.setMapperClass(MaxTemperatureMapper.class); job.setCombinerClass(MaxTemperatureReducer.class); job.setReducerClass(MaxTemperatureReducer.class);</p>

<p>System.exit(job.waitForCompletion(true) ? 0 : 1);</p>

<p>}</p>

<p>}</p>

<p>我们按照如下指令对压缩后的输入运行程序(输出数据不必使用相同的压缩格式进 行压缩，尽管本例中不是这样)：</p>

<p>% hadoop MaxTemperatureWithCompression input/ncdc/sample.txt•gz output</p>

<p>最终输出的每个部分都是经过压缩的。在这里，只有一部分结果：</p>

<p>% gunzip -c output/part-r-00000.gz</p>

<p>1949    111</p>

<p>1950    22</p>

<p>如果为输出生成顺序文件(sequence file)，可以设置mapreduce.out</p>

<p>put.fileoutputformat.compress• type属性来控制限制使用压缩格式。默认</p>

<p>值是RECORD,即针对每条记录进行压缩。如果将其改为BLOCK,将针对一组记录进 行压缩，这是推荐的压缩策略，因为它的压驗率更高(参见5.4.1节)。</p>

<p>SequenceFileOutputFormat 类另外还有一个静态方法 putCompressionType()，可以</p>

<p>用来便捷地设置该属性。</p>

<p>表5-5归纳概述了用于设置MpaReduce作业输出的压缩格式的配置属性。如果你 的MapReduce驱动使用Tool接口（参见6.2.2节），则可以通过命令行将这些属性 传递给程序，这比通过程序代码来修改压缩属性更加简便。</p>

<p>表5-5. MapReduce的压缩属性</p>

<table>
<thead>
<tr>
<th>•BW:傑、」‘    &lsquo;    • : •    ，網&rsquo; ■::■‘巧居女XAt属性名,</th>
<th>;•，興象v稔々4緣纖秀类型</th>
<th>■ ■ . &lsquo; &rsquo; -:默认值</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>mapreduce.output.fileoutputformat.compress</td>
<td>boolean</td>
<td>false</td>
<td>是否压缩输出</td>
</tr>

<tr>
<td>mapreduce.output.fileoutputformat.compress.codec</td>
<td>类名称</td>
<td>org.apache.hadoop.io. compress.DefaultCodec</td>
<td>map输出所用的压缩codec</td>
</tr>

<tr>
<td>mapreduce.output.fileoutputformat.compress.type</td>
<td>String</td>
<td>RECORD</td>
<td>顺序文件输出可以使 用的压缩类型：NONE、RECORD 或者BLOCK</td>
</tr>
</tbody>
</table>

<p>对map任务输出进行压缩</p>

<p>尽管MapReduce应用读/写的是未经压缩的数据，但如果对map阶段的中间输入 进行压缩，也可以获得不少好处。由于map任务的输出需要写到磁盘并通过网络 传输到reducer节点，所以通过使用LZO、LZ4或者Snappy这样的快速压缩方 式，是可以获得性能提升的，因为需要传输的数据减少了。启用map任务输出压 缩和设置压缩格式的配置属性如表5-6所示。</p>

<p>表5-6. map任务输出的压缩属性</p>

<p>属性名称 k类型 默认值 mapreduce.map. boolean false output.compress</p>

<p>描述</p>

<p>是否对map任务输 出进行压缩</p>

<p>map输出所用的压 缩 codec</p>

<p>mapreduce.map.    Class</p>

<p>org.apache.hadoop.io. compress.DefaultCodec</p>

<p>output.compress.codec</p>

<p>下面是在作业中启用map任务输出gzip压缩格式的代码（使用新API）：</p>

<p>Configuration conf = new Configuration();</p>

<p>conf.setBoolean(〕ob.MAPJ3UTPUTJZOMPRESS, true);</p>

<p>conf.setClass(Dob•MAP^OUTPUT^COMPRESS^CODEC, GzipCodec.class, CompressionCodec.class);</p>

<p>Job job = new ZJob(conf);</p>

<p>在旧的API中(参见附录D)，JobConf对象中可以通过更便捷的方法实现该 功能：</p>

<p>conf•setCompressMapOutput(true);</p>

<p>conf.setMapOutputCompressorClass(GzipCodec.class);</p>

<h5 id="5-3序列化">5.3序列化</h5>

<p>序列化(serialization)是指将结构化对象转化为字节流以便在网络上传输或写到磁盘 进行永久存储的过程。反序列化(deserialization)是指将字节流转回结构化对象的逆 过程。</p>

<p>序列化用于分布式数据处理的两大领域：进程间通信和永久存储。</p>

<p>在Hadoop中，系统中多个节点上进程间的通信是通过“远程过程调用” (RPC, remote procedure call)实现的。RPC协议将消息序列化成二进制流后发送到远程节 点，远程节点接着将二进制流反序列化为原始消息。通常情况下，RPC序列化格 式如下。</p>

<p>紧凑</p>

<p>紧凑格式能充分利用网络带宽(数据中心中最稀缺的资源)。</p>

<p>快速</p>

<p>进程间通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序 列化的性能开销，这是最基本的。</p>

<p>可扩展</p>

<p>为了满足新的需求，协议不断变化。所以在控制客户端和服务器的过程 中，需要直接引进相应的协议。例如，需要能够在方法调用的过程中增 添新的参数，并且新的服务器需要能够接受来自老客户端的老格式的消 息(无新增的参数)。</p>

<p>•支持互操作</p>

<p>对于某些系统来说，希望能支持以不同语言写的客户端与服务器交互， 所以需要设计需要一种特定的格式来满足这一需求。</p>

<p>表面看来，序列化框架对选择用于数据持久存储的数据格式应该会有不同的要 求。毕竟，RPC的存活时间不到1秒钟，持久存储的数据却可能在写到磁盘若干 年后才会被读取。但结果是，RPC序列化格式的四大理想属性对持久存储格式而 言也很重要。我们希望存储格式比较紧凑（进而高效使用存储空间）、快速（读/写数 据的额外开销比较小）、可扩展（可以透明地读取老格式的数据）且可以互操作（以可 以使用不同的语言读/写永久存储的数据）。</p>

<p>Hadoop使用的是自己的序列化格式Writable，它绝对紧凑、速度快，但不太容 易用Java以外的语言进行扩展或使用。因为Writable是Hadoop的核心（大多数 MapReduce程序都会为键和值类型使用它），所以在接下来的三个小节中，我们要 进行深入探讨，然后再介绍Hadoop支持的其他序列化框架。Avro（—个克服了 Writable部分不足的序列化系统）将在第12章中讨论。</p>

<h6 id="5-3-1-writable-接口">5.3.1 Writable 接口</h6>

<p>Writable接口定义了两个方法：一个将其状态写入DataOutput二进制流，另一 个从Datalnput二进制流读取状态：</p>

<p>package org.apache.hadoop.io;</p>

<p>import java.io.DataOutput; import java.io.Datalnput; import java.io.IOException;</p>

<p>public interface Writable { void write(DataOutput out) throws IOException; void readFields(Datalnput in) throws IOException;</p>

<p>}</p>

<p>让我们通过一个特殊的Writable类来看看它的具体用途。我们将使用 IntWritable来封装Java int类型。我们可以新建一个对象并使用set()方法来 设置它的值：</p>

<p>IntWritable writable = new IntWritable(); writable.set(163);</p>

<p>也可以通过使用一个整数值作为输入参数的构造函数来新建一个对象:</p>

<p>IntWritable writable = new IntWritable(163);</p>

<p>为 了检查 IntWritable 的序列化形式，我们在 java. io • DataOutput St ream</p>

<p>(java.io.DataOutput</p>

<p>的一个实现）中加入一</p>

<p>个帮助函数来封装</p>

<p>java.io.ByteArrayOutputSteam，以便在序列化流中捕捉字节:</p>

<p>public static byte[] serialize(Writable writable) throws IOException { ByteArrayOutputStream out = new ByteArrayOutputStream(); DataOutputStream dataOut = new DataOutputStream(out);</p>

<p>writable.write(dataOut); dataOut.close(); return out.toByteArray();</p>

<p>}</p>

<p>一个整数占用4个字节（</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-67.jpg" alt="img" /></p>

<p>为我们使用］Unit4进行声明）:</p>

<p>byte[] bytes = serialize(writable); assertThat(bytes.length, is(4));</p>

<p>每个字节是按照大端顺序写入的（按照java.io.DataOutput接口中的声明，最重 要的字节先写入流），并且通过Hadoop的StringUtils,我们可以看到这些字节 的十六进制表示：</p>

<p>assertThat (StringUtils. byteToHexStringCbytes)^ is(,,000000a3,&lsquo;));</p>

<p>让我们试试反序列化。我们再次新建一个辅助方法，从一个字节数组中读取一个</p>

<p>Writable 对象：</p>

<p>public static byte[] deserialize(Writable writable, byte[] bytes) throws IOException {</p>

<p>ByteArraylnputStream in = new ByteArrayInputStream(bytes); DatalnputStream dataln = new DatalnputStream(in); writable.readFields(dataln);</p>

<p>dataln.close(); return bytes;</p>

<p>}</p>

<p>我们构建了一个新的、空值的IntWritable对象，然后调用deserialize（）方法从我</p>

<p>们刚写的输出数据中读取数据。最后，我们看到该值（通过get（）方法获取）是原始 的数值163:</p>

<p>IntWritable newWritable = new IntWritable(); deserialize(newWritable^ bytes); assertThat(newWritable•get(), is(163));</p>

<p>WritableComparable 接□和 comparator</p>

<p>IntWritable 实现原始的 WritableComparable 接口，该接口继承自 Writable 和 java.lang.Comparable 接口：</p>

<p>package org.apache.hadoop.io;</p>

<p>public interface WritableComparable<T> extends Writable, Comparable<T> {</p>

<p>对MapReduce来说，类型比较非常重要，因为中间有个基于键的排序阶段。 Hadoop提供的一个优化接口是继承自]ava Comparator的RawCompanator</p>

<p>接口：</p>

<p>package org.apache.hadoop.io;</p>

<p>import java.util.Comparator;</p>

<p>public interface RawComparator<T> extends Comparator<T> { public int compare(byte[] bl, int si, int 11， byte[] b2, int s2, int 12);</p>

<p>}</p>

<p>该接口允许其实现直接比较数据流中的记录，无须先把数据流反序列化为对象， 这样便避免了新建对象的额外开销。例如，我们根据IntWritable接口实现的 comparator实现原始的compare()方法，该方法可以从每个字节数组bl和b2中 读取给定起始位置(si和S2)以及长度(11和12)的一个整数进而直接进行比较。</p>

<p>WritableComparator 是对继承自 WritableComparable 类的 RawComparator 类的一 个通用实现。它提供两个主要功能。第一，它提供了对原始compare()方法的一</p>

<p>个默认实现，该方法能够反序列化将在流中进行比较的对象，并调用对象的 compare()方法。第二，它充当的是RawComparator实例的工厂(已注册 Writable的实现)。例如，为了获得IntWritable的comparator,我们直接如下 调用：</p>

<p>RawComparator<IntWritable> comparator = WritableComparator.get (IntWritable.class);</p>

<p>这个comparator可以用于比较两个IntWritable对象：</p>

<p>IntWritable wl = new IntWritable(163);</p>

<p>IntWritable w2 = new IntWritable(67);</p>

<p>assertThat(comparator.compareCwl^ w2)greaterThan(O));</p>

<p>或其序列化表示：</p>

<p>byte[] bl = serialize(wl); byte[] b2 = serialize(w2);</p>

<p>assertThat(comparator.compare(bl^ 0, bl.length, b2, 0^ b2.1ength)? greaterThan(O));</p>

<h6 id="5-3-2-writable-类">5.3.2 Writable 类</h6>

<p>Hadoop自带的org.apache.hadoop.io包中有广泛的Writable类可供选择。它们</p>

<p>的层次结构如图5-1所示。</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-68.jpg" alt="img" /></p>

<p>tsr S</p>

<p>Primitives    Others</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-71.jpg" alt="img" /></p>

<p><img src="Hadoop43010757_2cdb48_2d8748-72.jpg" alt="img" /></p>

<p><img src="Hadoop43010757_2cdb48_2d8748-73.jpg" alt="img" /></p>

<p><img src="Hadoop43010757_2cdb48_2d8748-74.jpg" alt="img" /></p>

<p><img src="Hadoop43010757_2cdb48_2d8748-75.jpg" alt="img" /></p>

<p>I L : &lsquo; • +•</p>

<p>Text</p>

<p>，BytesWntabie ：</p>

<p>- .：</p>

<p>MDSHash</p>

<dl>
<dt>醐</dt>
</dl>

<p>:：&rsquo;|gth,n</p>

<p>?續刪腑</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-78.jpg" alt="img" /></p>

<p><img src="Hadoop43010757_2cdb48_2d8748-79.jpg" alt="img" /></p>

<p><img src="Hadoop43010757_2cdb48_2d8748-80.jpg" alt="img" /></p>

<p>5-1. Writable类的层次结构</p>

<p>\1. Java基本类型的Writable封装器</p>

<p>Writable类对所有Java基本类型(参见表5-7)提供封装，char类型除外(可以存 储在IntWritable中)。所有的封装包含get()和set()两个方法用于读取或存 储封装的值。</p>

<p>表5-7. Java基本类型的Writable类</p>

<table>
<thead>
<tr>
<th>Java基本类型</th>
<th>Writable 实现</th>
<th>序列化大小(字节)</th>
</tr>
</thead>

<tbody>
<tr>
<td>boolean</td>
<td>BooleanWritable</td>
<td>1</td>
</tr>

<tr>
<td>byte</td>
<td>ByteWritable</td>
<td>1</td>
</tr>

<tr>
<td>Short</td>
<td>ShortWritable</td>
<td>2</td>
</tr>

<tr>
<td>int</td>
<td>IntWritable</td>
<td>4</td>
</tr>

<tr>
<td></td>
<td>VintWritable</td>
<td>1〜5</td>
</tr>

<tr>
<td>float</td>
<td>FloatWritable</td>
<td>4</td>
</tr>

<tr>
<td>long</td>
<td>LongWritable</td>
<td>8</td>
</tr>

<tr>
<td></td>
<td>VlongWritable</td>
<td>卜9</td>
</tr>

<tr>
<td>double</td>
<td>DoubleWritable</td>
<td>8</td>
</tr>
</tbody>
</table>

<p>对整数进行编码时，有两种选择，即定长格式(IntWritale和LongWritable)和 变长格式(VIntWritable和VLongWritable)。需要编码的数值如果相当小(在-127和127之间，包括-127和127)，变长格式就是只用一个字节进行编码；否 则，使用第一个字节来表示数值的正负和后跟多少个字节。例如，值163需要两 个字节：</p>

<p>byte[] data = serialize(new VIntWritable(163));</p>

<p>assertThat(StringUtils.byteToHexString(data), is(n8fa3&rdquo;));</p>

<p>如何在定长格式和变长格式之间进行选择呢？定长格式编码很适合数值在整个值 域空间中分布非常均匀的情况，例如使用精心设计的哈希函数。然而，大多数数 值变量的分布都不均匀，一般而言变长格式会更节省空间。变长编码的另一个优 点是可以在VIntWritable和VLongWritable转换，因为它们的编码实际上是一 致的。所以选择变长格式之后，便有增长的空间，不必一开始就用8字节的long 表不。</p>

<p>\2. Text类型</p>

<p>Text是针对UTF-8序列的Writable类。一般可以认为它是java.lang.String的</p>

<p>Writable 等价。</p>

<p>Text类使用整型（通过边长编码的方式）来存储字符串编码中所需的字节数，因此该 最大值为2 GB。另外，Text使用标准UTF-8编码，这使得能够更简便地与其他理 解UTF-8编码的工具进行交互操作。</p>

<p>索引由于着重使用标准的UTF-8编码，因此Text类和Java String类之间存</p>

<p>在一定的差别。对Text类的索引是根据编码后字节序列中的位置实现的，并非字 符串中的Unicode字符，也不是Java char的编码单元（如String）。对于ASCII 字符串，这三个索引位置的槪念是一致的。charAt（）方法的用法如下例所示：</p>

<p>Text t = new Text(HhadoopH);</p>

<p>assertThat(t.getLength(), is(6));</p>

<p>assertThat(t.getBytes().lengthy is(6));</p>

<p>assertThat(t.charAt(2)is((int) &lsquo;d•));</p>

<p>assertThat(HOut of bounds&rdquo;, t.charAt(100), is(-l));</p>

<p>注意：harAt()方法返回的是一个表示Unicode编码位置的int类型值，而 String返回一个char类型值。Text还有一个findO方法，该方法类似于 String 的 indexOf ()方法：</p>

<p>Text t = new Text(Hhadoopn);</p>

<p>assertThat (&ldquo;Find a substring&rdquo;, t .find (&ldquo;do&rdquo;), is(2)); assertThatC&rsquo;Finds first •o.&ldquo;, t.find(&ldquo;o&rdquo;), is(3));</p>

<p>assertThat(HFinds 1o* from position 4 or later&rdquo;, t.find(&ldquo;o&rdquo;, 4), is(4)); assertThat(&ldquo;No match&rdquo;, t.find(&ldquo;pig&rdquo;), is(-l));</p>

<p>Unicode —旦开始使用需要多个字节来编码的字符时，Text和String之间的 区别就昭然若揭了。考虑表5-8显示的Unicode字符。① 所有字符（除了表中最后一个字符U+10400），都可以使用单个Java char类型来表 示。U+10400是一个候补字符，并且需要两个Java char来表示，称为“字符代理 对” （surrogate pair）。范例5-5中的测试显示了处理一个字符串（表5-8中的由4个 字符组成的字符串）时String和Text之间的差别。</p>

<p>①本例基干 Norbert Lindenberg 和 Masayoshi Okutsu 发表干 2004 年 5 月的文章 “Supplementary Character in the Java Platform ” (Java 平台中的增补字符)，网址为 <a href="http://bit.ly/">http://bit.ly/</a> java_supp—characters，中文版网址为 <a href="http://m.blog.csdn.net/article/details?id=">http://m.blog.csdn.net/article/details?id=</a>7345527Q</p>

<p>Is</p>

<p>表 5-8. Unicode 字符</p>

<table>
<thead>
<tr>
<th>Unicode. . ■ 编码点</th>
<th>U+0041</th>
<th></th>
<th>U+6771‘%職•事賴勸黎1:&rsquo;::耀鄕&rsquo;•</th>
<th>U+10400</th>
</tr>
</thead>

<tbody>
<tr>
<td>名称</td>
<td>拉丁大写字母A</td>
<td>拉丁小写字母无（统一表示</td>
<td>DESERET</td>
<td></td>
</tr>

<tr>
<td></td>
<td></td>
<td>SHARP S</td>
<td>的汉字）</td>
<td>CAPITAL LETTERLONG I</td>
</tr>

<tr>
<td>UTF-8 编码单元</td>
<td>41</td>
<td>c39f</td>
<td>e69dbl</td>
<td>F0909080</td>
</tr>

<tr>
<td>Java表示</td>
<td>\u0041</td>
<td>\uOODF</td>
<td>\u6771</td>
<td>\uuD801\uDC00</td>
</tr>
</tbody>
</table>

<p>范例5-5.验证String类和Text类的差异性的测试</p>

<p>public class StringTextComparisonTest {</p>

<p>@Test</p>

<p>public void string() throws UnsupportedEncodingException {</p>

<p>String s = &ldquo;\u0041\u00DF\u6771\uD801\uDC00&rdquo;;</p>

<p>assertThat(s.length()， is(5));</p>

<p>assertThat(s.getBytes(&ldquo;UTF-8&rdquo;)•length， is(10));</p>

<p>assertThat(s.indexOf(,,\u0041,1), is(0)); assert That (s. indexOf (n\u00DF&rdquo;), is⑴); assertThat(s.indexOf(&rdquo;\u6771&rdquo;), is(2)); assertThat(s.indexOf(&rdquo;\uD801\uDC00&rdquo;)，is(3));</p>

<p>is(.\u0041.));</p>

<p>is(.\u00DF.));</p>

<p>is(&rsquo;\u6771&rsquo;));</p>

<p>is^\uD80r));</p>

<p>isCXuDCeO1));</p>

<p>assertThat(s.charAt(0), assertThat(s.charAt(l), assertThat(s.charAt(2), assertThat(s.charAt(3) assertThat(s.charAt(4)</p>

<p>assertThat(s.codePointAt(0), is(0x0041)); assertThat(s&gt;codePointAt(1), is(0X00DF)); assertThat(s.codePointAt(2), is(0x6771)); assertThat(s•codePointAt(3), is(0x10400));</p>

<p>}</p>

<p>@Test</p>

<p>public void text() {</p>

<p>Text t = new Text(,,\u0041\u00DF\u6771\uD801\uDC00n);</p>

<p>assertThat(t.getLength()， is(10));</p>

<p>assertThat(t.find(u\u0041H), is(0));</p>

<p>assertThat(t.find(&rdquo;\u00DF&rdquo;) ^ is⑴);</p>

<p>assertThat(t.find(&rdquo;\u6771&rdquo;), is(3));</p>

<p>assertThat(t.find(&rdquo;\uD801\uDC00&rdquo;), is(6));</p>

<p>assertThat(t.charAt(0), is(0x0041)); assertThat(t.charAt(1)^ is(0x00DF)); assertThat(t.charAt(3)is(0x6771));</p>

<p>assertThat(t.charAt(6)&gt; is(0x10400));</p>

<p>这个测试证实String的长度是其所含char编码单元的个数(5,由该字符串的前 三个字符和最后的一个代理对组成)，但Text对象的长度却是其UTF-8编码的字 节数(10=1+2+3+4)。相似的，String类的indexOf()方法返回char编码单元中 的索引位置，Text类的find()方法则返回字节偏移量。</p>

<p>当代理对不能代表整个Unicode字符时，String类中的charAt()方法会根据指 定的索引位置返回char编码单元。根据char编码单元索引位置，需要 codePointAt()方法来获取表示成int类型的单个Unicode字符。事实上，Text 类中的charAt()方法与String中的codePointAt()更加相似(相较名称而言)。 唯一的区别是通过字节的偏移量进行索引。</p>

<p>迭代利用字节偏移量实现的位置索引，对Text类中的Unicode字符进行迭代是</p>

<p>非常复杂的，因为无法通过简单地增加索引值来实现该迭代。同时迭代的语法有 些模糊(参见范例5-6):将Text对象转换为java.nio.ByteBuffer对象，然后 利用缓冲区对Text对象反复调用bytesToCodePoint()静态方法。该方法能够</p>

<p>获取下一代码的位置，并返回相应的int值，最后更新缓冲区中的位置。当 bytesToCodePoint()返回-1时，则检测到字符串的末尾。</p>

<p>范例5-6.遍历Text对象中的字符</p>

<p>public class Textlterator {</p>

<p>public static void main(String[] args) {</p>

<p>Text t = new Text(H\u0041\u00DF\u6771\uD801\uDC00H);</p>

<p>ByteBuffer buf = ByteBuffer.wrap(t.getBytes(), 0, t.getLength()); int cp;</p>

<p>while(buf.hasRemaining() &amp;&amp; (cp = Text.bytesToCodePoint(buf))!=-1){ System.out.println(Integer.toHexString(cp));</p>

<p>}</p>

<p>}</p>

<p>}</p>

<p>Hadoop 的 I/O 操作 117</p>

<p>运行这个程序，打印出字符串中四个字符的编码点(code point)：</p>

<p>% hadoop Textlterator</p>

<p>41</p>

<p>df</p>

<p>6771</p>

<p>10400</p>

<p>可变性与String相比，Text的另一个区别在于它是可变的(与所有Hadoop的</p>

<p>Writable接口实现相似，NullWritable除外，它是单实例对象)。可以通过调用 其中一个set()方法来重用Text实例。例如：</p>

<p>Text t = new Text(&ldquo;hadoop&rdquo;);</p>

<p>t.set(&ldquo;pig&rdquo;);</p>

<p>assertThat(t•getLength()is(3));</p>

<p>assertThat(t•getBytes().length, is(3));</p>

<p>在某些情况下，getBytes()方法返回的字节数组可能比getLength()函数返回 的长度更长：</p>

<p>Text t = new Text(&ldquo;hadoop&rdquo;); t.set(new Text(&ldquo;pig&rsquo;)); assertThat(t.getLength(), is(3));</p>

<p>assertThat(&ldquo;Byte length not shortened&rdquo;, t.getBytes().length, is(6));</p>

<p>以上代码说明了在调用getBytesO之前为什么始终都要调用getLengthO方法，因 为可以由此知道字节数组中多少字符是有效的。</p>

<p>讨String重新排序Text类并不像java.lang.String类那样有丰富的字符串 喿作API。所以，在多数情况下需要将Text对象转换成String对象。这一转换 遺常通过调用toString()方法来实现：</p>

<p>assertThat(new Text(&ldquo;hadoop •).toString(), is(HhadoopH));</p>

<p>\3. BytesWritable</p>

<p>BytesWritable是对二进制数据数组的封装。它的序列化格式为一个指定所含数 据字节数的整数域(4字节)，后跟数据内容本身。例如，长度为2的字节数组包含 数值3和5,序列化形式为一个4字节的整数(00000002)和该数组中的两个字节</p>

<p>(03 和 05):</p>

<p>BytesWritable b = new BytesWritable(new byte[] { 3， 5 }); byte[] bytes = serialize(b);</p>

<p>assertThat(StringUtils.byteToHexString(bytes)is(.’000000020305&rdquo;));</p>

<p>BytesWritable是可变的，其值可以通过set()方法进行修改。和Text相似， BytesWritable类的getBytes()方法返回的字节数组长度(容量)可能无法体现 BytesWritable所存储数据的实际大小。可以通过getLength()方法来确定 BytesWritable的大小。示例如下：</p>

<p>b.setCapacity(ll);</p>

<p>assertThat(b.getLength()is(2)); assertThat(b.getBytes().length, is(ll));</p>

<p>\4. NullWritable</p>

<p>NullWritable是Writable的特殊类型，它的序列化长度为0。它并不从数据流 中读取数据，也不写入数据。它充当占位符；例如，在MapReduce中，如果不需 要使用键或值的序列化地址，就可以将键或值声明为NullWritable,这样可以高 效存储常量空值。如果希望存储一系列数值，与键•值对相对，NullWritable也 可以用作在SequenceFile中的键。它是一个不可变的单实例类型，通过调用 NullWritable. get （）方法可以获取这个实例。</p>

<p>\5. ObjectWritable 和 GenericWritable</p>

<p>ObjectWritable 是对 Java 基本类型（String，enum, Writable, null 或这些 类型组成的数组）的一个通用封装。它在Hadoop RPC中用于对方法的参数和返回 类型进行封装和解封装。</p>

<p>当一个字段中包含多个类型时，ObjectWritable非常有用：例如，如果 SequenceFile中的值包含多个类型，就可以将值类型声明为ObjectWritable,并将每个 类型封装在一个ObjectWritable中。作为一个通用的机制，每次序列化都写封 装类型的名称，这非常浪费空间。如果封装的类型数量比较少并且能够提前知 道，那么可以通过使用静态类型的数组，并使用对序列化后的类型的引用加入位 置索引来提高性能。GenericWritable类采取的就是这种方式，所以你得在继承 的子类中指定支持什么类型。</p>

<p>\6. Writable 集合类</p>

<p>org.apache. hadoop. io 软件包中共有 6 个 Writable 集合类，分别是 ArrayWritable、 ArrayPrimitiveWritable、TwoDArrayWritable &gt; MapWritable、SortedMapWritable 以及 EnumMapWritableo</p>

<p>ArrayWritable和TwoDArrayWritable是对Writable的数组和两维数组（数组 的数组）的实现。ArrayWritable或TwoDArrayWritable中所有元素必须是同一</p>

<p>类的实例（在构造函数中指定），如下所示：</p>

<p>ArrayWritable writable = new ArrayWritable（Text.class）;</p>

<p>如果Writable根据类型来定义，例如SequenceFile的键或值，或更多时候作 为MapReduce的输入，则需要继承ArrayWritable（或相应的TwoDArray</p>

<p>Writable类)并设置静态类型。示例如下:</p>

<p>public class TextArrayWritable extends ArrayWritable { public TextArrayWritable() {</p>

<p>super(Text.class);</p>

<p>}</p>

<p>}</p>

<p>ArrayWritable 和 TwoDArrayWritable 都有 get()、set()和 toArray()方法。 toArray()方法用于新建職龃(或且)的一个“浅拷贝”(shallow copy)。</p>

<p>ArrayPrimitiveWritable是对Java基本数组类型的一个封装。调用set()方法 时，可以识别相应组件类型，因此无需通过继承该类来设置类型。</p>

<p>MapWritable 和 SortedMapWritable 分另U实现了 java. util.Map&lt;Writable, Writable〉和 java .util. SortedMap&lt;WritableComparable, Writable〉。每个键/值字段使用的类</p>

<p>型是相应字段序列化形式的一部分。类型存储为单个字节(充当类型数组的索引)。 在org.apache.hadoop.io包中，数组经常与标准类型结合使用，而定制的</p>

<p>Writable类型也通常结合使用，但对于非标准类型，则需要在包头指明所使用的 数组类型。根据实现，MapWritable类和SortedMapWritable类通过正byte值 来指示定制的类型，所以在MapWritable和SortedMapWritable实例中最多可 以使用127个不同的非标准Wirtable类。下面显示使用了不同键值类型的 MapWritable 实例：</p>

<p>MapWritable src = new MapWritable();</p>

<p>src.put(new IntWritable(l), new Text(&ldquo;catn)); src.put(new VIntWritable(2), new LongWritable(163));</p>

<p>MapWritable dest = new MapWritable();</p>

<p>WritableUtils.clonelnto(dest, src);</p>

<p>assertThat((Text) dest.get(new IntWritable(l)), is(new Text(,,catH))); assertThat((LongWritable) dest.get(new VIntWritable(2))， is(new LongWritable(163)));</p>

<p>显然，可以通过Writable集合类来实现集合和列表。可以使用MapWritable类 型(或针对排序集合，使用SortedMapWritable类型)来枚举集合中的元素，用 NullWritable类型枚举值。对集合的枚举类型可采用EnumSetWritable。对于 单类型的Writable列表，使用ArrayWritable就足够了，但如果需要把不同的 Writable类型存储在单个列表中，可以用GenericWritable将元素封装在一个 ArrayWritable中。另一个可选方案是，可以借鉴MapWritable的思路写一个 通用的 ListWritableo</p>

<h6 id="5-3-3实现定制的writable集合">5.3.3实现定制的Writable集合</h6>

<p>Hadoop有一套非常有用的Writable实现可以满足大部分需求，但在有些情况 下，我们需要根据自己的需求构造一个新的实现。有了定制的Writable类型，就 可以完全控制二进制表示和排序顺序。由于Writable是MapReduce数据路径的 核心，所以调整二进制表示能对性能产生显著效果。虽然Hadoop自带的Writable实</p>

<p>现已经过很好的性能调优，但如果希望将结构调整得更好，更好的做法往往是新 建一个Writable类型（而不是组合打包的类型）。</p>

<p>如果你正考虑写一个定制的Writable,值得尝试另一种序列化框架，例如 CjT Avro,允许你以声明方式定义定制的类型。详情可以参见5.3.4节有关序列化</p>

<p>框架馳容及第12章。</p>

<p>为了演示如何新建一个定制的Writable,我们写一个表示一对字符串的实现，名 为TextPair。范例5-7展示了最基本的实现。</p>

<p>范例5-7.存储一对Text对象的Writable实现</p>

<p>import java.io.*;</p>

<p>import org.apache.hadoop.io.*;</p>

<p>&gt;</p>

<p>public class TextPair implements WritableComparable<TextPair> {</p>

<p>private Text first;</p>

<p>private Text second;</p>

<p>public TextPair() { set(new Text(), new Text());</p>

<p>}</p>

<p>public TextPair(String firsts String second) { set(new Text(first), new Text(second));</p>

<p>}</p>

<p>public TextPair(Text firsts Text second) { set(first， second);</p>

<p>}</p>

<p>public void set(Text firsts Text second) { this.first = first; this.second = second;</p>

<p>}</p>

<p>public Text getFirst() { return first;</p>

<p>public Text getSecond() { return second;</p>

<p>}</p>

<p>^Override</p>

<p>public void write(DataOutput out) throws IOException { first.write(out); second.write(out);</p>

<p>}</p>

<p>^Override</p>

<p>public void readFields(DataInput in) throws IOException { first.readFields(in); second.readFields(in);</p>

<p>}</p>

<p>^Override</p>

<p>public int hashCode() {</p>

<p>return first.hashCode() * 163 + second.hashCode();</p>

<p>}</p>

<p>^Override</p>

<p>public boolean equals(Object o) { if (o instanceof TextPair) {</p>

<p>TextPair tp = (TextPair) o;</p>

<p>return first.equals(tp.first) &amp;&amp; second.equals(tp.second);</p>

<p>}</p>

<p>return false;</p>

<p>}</p>

<p>^Override</p>

<p>public String toString() { return first + &ldquo;\t&rdquo; + second;</p>

<p>}</p>

<p>^Override</p>

<p>public int compareTo(TextPair tp) { int cmp = first.compareTo(tp.first); if (cmp != 0) {</p>

<p>return cmp;</p>

<p>}</p>

<p>return second.compareTo(tp.second);</p>

<p>}</p>

<p>}</p>

<p>这个定制Writable实现的第一部分非常直观：包括两个Text实例变量(first 和second)和相关的构造函数，以及setter和getter(即设置函数和提取函数)。所 有Writable实现都必须有一个默认的构造函数以便MapReduce框架可以对它们 进行实例化，然后再调用readFields()函数查看(填充)各个字段的值。</p>

<p>Writable实例是可变的并且通常可以重用，所以应该尽量避免在write()或 readFields()方法中分配对象。</p>

<p>通过让Text对象自我表示，TextPair类的write()方法依次将每个Text对象 序列化到输出流中。类似的，通过每个Text对象的表示，readFields()方法对 来自输入流的字节进行反序列化。DataOutput和Datalnput接口有一套丰富的 方法可以用于对Java基本类型进行序列化和反序列化，所以，在通常情况下，.你 可以完全控制Writable对象在线上传输/交换(的数据)的格式(数据传输格式)。</p>

<p>就像针对Java语言构造的任何值对象那样，需要重写java.lang.Object中的 hashCode()、equals()和 toString()方法。HashPartitioner (MapReduce 中 的默认分区类)通常用hashCode()方法来选择reduce分区，所以应该确保有一个 比较好的哈希函数来保证每个reduce分区的大小相似。</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-81.jpg" alt="img" /></p>

<p>即便计划结合使用TextOutputFormat和定制的Writable,也得自己动手实 现 toString()方法。TextOutputFormat 对键和值调用 toString()方法，</p>

<p>将键和值转换为相应的输出表示。针对TextPair，我们将原始的Text对象作 为字符串写到输出，各个字符串之间要用制表符来分隔。</p>

<p>TextPair 是 WritableComparable 的一个实现，所以它提供了 compareTo()方</p>

<p>法，该方法可以强制数据排序：先按第一个字符排序，如果第一个字符相同，则 按照第二个字符排序。注意，除了可存储的Text对象数目，TextPair不同于 TextArrayWritable(前一小节中已经提到)，因为TextArrayWritable只继承 Writable,并没有继承WritableComparable。</p>

<p>1.为提高速度实现一个RawComparator</p>

<p>范例5-7中的TextPair代码可以按照其描述的基本方式运行；但我们也可以进一 步优化。按照5.3.1节的说明，当TextPair被用作MapReduce中的键时，需要将 数据流反序列化为对象，然后再调用compareToQ方法进行比较。那么有没有可 能看看它们的序列化表示就可以比较两个TextPair对象呢？</p>

<p>事实证明，我们可以这样做，因为TextPair是两个Text对象连接而成的，而 Text对象的二进制表示是一个长度可变的整数，包含字符串之UTF-8表示的字节 数以及UTF-8字节本身。诀窍在于读取该对象的起始长度，由此得知第一个Text 对象的字节表示有多长；然后将该长度传给Text对象的RawComparator方法， 最后通过计算第一个字符串和第二个字符串恰当的偏移量，这样便可以实现对象</p>

<p>的比较。详细过程参见范例5-8（注意，这段代码已嵌入TextPair类中）。 范例5-8.用于比较TextPair字节表示的RawComparator</p>

<p>public static class Comparator extends WritableComparator {</p>

<p>private static final Text.Comparator TEXT COMPARATOR = new Text.Comparator^);</p>

<p>public Comparator^) { super(TextPair.class);</p>

<p>^Override</p>

<p>public int compare(byte[] bl^ int si, int 11,</p>

<p>byte[] b2) int s2, int 12) {</p>

<p>int firstLl = WritableUtils.decodeVIntSize(bl[sl]) + readVInt(bl, si); int firstL2 = WritableUtils.decodeVIntSize(b2[s2]) + readVInt(b2, s2); int cmp = TEXT COMPARATOR.compare(bl^ si, firstLl, b2, s2, firstL2); if (cmp != 0) {</p>

<p>return cmp;</p>

<p>}</p>

<p>return TEXT COMPARATOR.compare(bl^ si + firstLl, 11 - firstLl, b2, s2 + firstL2, 12 • firstL2);</p>

<p>} catch (IOException e) { throw new IllegalArgumentException(e);</p>

<p>} • •</p>

<p>}</p>

<p>} static {</p>

<p>WritableComparator.define(TextPair.class, new Comparator^));</p>

<p>}</p>

<p>事实上，我们采取的做法是继承WritableComparable类，而非实现RawComparator</p>

<p>接口，因为它提供了一些比较好用的方法和默认实现。这段代码最本质的部分是 计算firstLl和finstL2,这两个参数表示每个字节流中第一个Text字段的长 度。两者分别由变长整数的长度（由WritableUtils的decodeVIntSize（）方法 返回）和编码值（由readVInt（）方法返回）组成。</p>

<p>2.定制的 comparator</p>

<p>从TextPair可以看出，编写原始的comparator需要谨慎，因为必须要处理字节级别的细 节。如果真的需要自己写con平arator，有必要参考org.apache.hadoop.io包中对Writable 接口的实现。WritableUtils提供的方法也比较好用。</p>

<p>如果可能，定制的comparator也应该继承自RawComparator。这些comparator定</p>

<p>义的排列顺序不同于默认comparator定义的自然排列顺序。范例5-9显示了一#对 TextPair 类型的comparator，称为 FirstCompartator，它只考虑 TextPair 对象的第一</p>

<p>个字符串。注意，我们重载了针对该类对象的compare()方法，使两个 compare()方法有相同的语法。</p>

<p>范例5-9.定制的RawComparator用于比较TextPair对象字节表示的第一个字段</p>

<p>public static class FirstComparator extends WritableComparator {</p>

<p>private static final Text.Comparator TEXT COMPARATOR = new Text.Comparator^); public FirstComparator() {</p>

<p>super(TextPair.class);</p>

<p>}</p>

<p>^Override</p>

<p>public int compare(byte[] bl， int si, int 11,</p>

<p>byte[] b2j int s2， int 12) {</p>

<p>try {</p>

<p>int firstLl = WritableUtils.decodeVIntSize(bl[sl]) + readVInt (bl, si); int firstL2 = WritableUtils.decodeVIntSize(b2[s2j) + readVInt (b2, s2); return TEXT COMPARATOR.compare(blsi, firstLl, b2, s2, firstL2);</p>

<p>} catch (IOException e) { throw new IllegalArgumentException(e);</p>

<p>}</p>

<p>}</p>

<p>^Override</p>

<p>public int compare(WritableComparable a, WritableComparable b) { if (a instanceof TextPair &amp;&amp; b instanceof TextPair) {</p>

<p>return ((TextPair) a).first.compareTo(((TextPair) b).first);</p>

<p>}</p>

<p>return super.compare(a, b);</p>

<p>}</p>

<p>}</p>

<p>第9章在介绍MapReduce的连接操作和辅助排序(参见9.3节)的时候，将使用这 个 comparator。</p>

<h6 id="5-3-4序列化框架">5.3.4序列化框架</h6>

<p>尽管大多数MapReduce程序使用的都是Writable类型的键和值，但这并不是 MapReduce API强制要求使用的。事实上，可以使用任何类型，只要能有一种机 制对每个类型进行类型与二进制表示的来回转换就可以。</p>

<p>为了支持这一机制，Hadoop有一个针对可替换序列化框架(serialization framework) 的API。序列化框架用一个Serialization实现(包含在org.apache.hadoop. io.serializer 包中)来表示。例如，WritableSenialization 类是对 Writable 类型的 Serialization 实现0</p>

<p>Serialization对象定义了从类型到Serializer实例（将对象转换为字节流）和 Deserializer实例（将字节流转换为对象）的映射方式。</p>

<p>为了注册Serialization实现，需要将io.serizalizations属性设置为一个由 逗号分隔的类名列表。它的默汄值包括org.apache.hadoop.io.serializer. WritableSerialization 和 Avro 指定（Specific）序列化及 Reflect（自反）序列化类 （详见12.1节），这意味着只有Writable对象和Avro对象才可以在外部序列化和 反序列化。</p>

<p>Hadoop 包含一个名为 DavaSerialization 的类，该类使用 Java Object Serialization。 尽管它方便了我们在MapReduce程序中使用标准的Java类型，如Integer或 String,但不如Writable高效，所以不建议使用（参见以下的补充内容）。</p>

<p>序列化IDL</p>

<p>还有许多其他序列化框架从不同的角度来解决该问题：不通过代码来定义类型， 而是使用“接口定义语言” （IDL, Interface Description Language）以不依赖于具体 语言的方式进行声明。由此，系统能够为其他语言生成类型，这种形式能有效提 高互操作能力。它们一般还会定义版本控制方案（使类型的演化直观易懂）。</p>

<p>两个比较流行的序列化框架 Apache Thrift（<a href="http://thrifi.apache.org/）和Google">http://thrifi.apache.org/）和Google</a> 的 Protocol Buffers （<a href="http://code.google.eom/p/protobuf/%ef%bc%89,%e5%b8%b8%e5%b8%b8%e7%94%a8%e4%bd%9c%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%95%b0%e6%8d%ae%e7%9a%84%e6%b0%b8%e4%b9%85%e5%ad%98">http://code.google.eom/p/protobuf/）,</a><a href="http://code.google.eom/p/protobuf/%ef%bc%89,%e5%b8%b8%e5%b8%b8%e7%94%a8%e4%bd%9c%e4%ba%8c%e8%bf%9b%e5%88%b6%e6%95%b0%e6%8d%ae%e7%9a%84%e6%b0%b8%e4%b9%85%e5%ad%98">常常用作二进制数据的永久存</a> 储格式。MapReduce格式对该类的支持有限，®但在Hadoop内部，部分组件仍使 用上述两个序列化框架来实现KPC和数据交换。</p>

<p>Avro是一个基于IDL的序列化框架，非常适用于Hadoop的大规模数据处理。我 们将在第12章讨论Avro。</p>

<p>为什么不用 Java Object Serialization?</p>

<p>Java有自己的序列化机制，称为“Java Object Serialization”（通常简称为“Java Serialization”），该机制与编程语言紧密相关，所以我们很自然会问为什么不在 Hadoop中使用该机制。针对这个问题，Doug Cutting是这样解释的：“为什么</p>

<p>① Twitter 的大象鸟项目(<a href="http://github.corn/kevinweil/elephant-bird">http://github.corn/kevinweil/elephant-bird)</a>%e5%8c%85%e5%90%ab%e4%b8%80%e5%94%89)<a href="http://github.corn/kevinweil/elephant-bird">包含一唉</a>%e5%8c%85%e5%90%ab%e4%b8%80%e5%94%89) 1:具，用十介: Hadoop 中与 Thrif 和 Protocol Buffers 结合使用。</p>

<p>开始设计Hadoop的时候我不用Java Serialization?因为它看起来太复杂，而我 认为需要有一个至精至简的机制，可以用于精确控制对象的读和写，这个机制</p>

<p>将是Hadoop的核心。使用Java Serialization虽然可以获得一些控制权，但用 起来非常纠结。</p>

<p>不用RMI（Remote Method Invocation远程方法调用）也出于类似的考虑。高效、 高性能的进程间通信是Hadoop的关键。我觉得我们需要晴确控制连接、延迟和缓冲 的处理方式，RMI对此无能为力。”</p>

<p>问题在于Java Serialization不满足先前列出的序列化格式标准：精简、快速、 可扩展、支持互操作。</p>

<h5 id="5-4基于文件的数据结构">5.4基于文件的数据结构</h5>

<p>对于某些应用，我们需要一种特殊的数据结构来存储自己的数据。对于基于 MapReduce的数据处理，将每个二进制数据大对象（blob）单独放在各自的文件中不 能实现可扩展性，所以，Hadoop为此开发了很多更高层次的容器。</p>

<h6 id="5-4-1-关于-sequencefile">5.4.1 关于 SequenceFile</h6>

<p>考虑日志文件，其中每一行文本代表一条日志记录。纯文本不合适记录二进制类 型的数据。在这种情况下，Hadoop的SequenceFile类非常合适，为二进制键-</p>

<p>值对提供了一个持久数据结构。将它作为日志文件的存储格式时，你可以自己选 择键（比如LongWritable类型所表示的时间戳），以及值可以是Writable类型 （用于表示日志记录的数量）。</p>

<p>SequenceFiles也可以作为小文件的容器。HDFS和MapReduce是针对大文件优 化的，所以通过SequenceFile类型将小文件包装起来，可以获得更高效率的存储 和处理。在8.2.1节中，我们讲到将整个文件作为一条记录处理时，提供了一个程序，它 将若干个小文件打包成一个SequenceFile类®。</p>

<p>①无独有偶，Stuart Sierra的博客文章“A Million Little Files”中也包含将tar文件转为 SequenceFile 的代码，参见 <a href="http://stuartsierra.com/2008/04/24/a-million-little-files%e3%80%82">http://stuartsierra.com/2008/04/24/a-million-little-files</a><a href="http://stuartsierra.com/2008/04/24/a-million-little-files%e3%80%82">。</a></p>

<p>\1. SequenceFile 的写操作</p>

<p>通过createWriter()静态方法可以创建SequenceFile对象，并返回 SequenceFile.Writer实例。该静态方法有多个重载版本，但都需要指定待写入 的数据流(FSDataOutputStream 或 FileSystem 对象和 Path 对象)，</p>

<p>Configuration对象，以及键和值的类型。另外，可选参数包括压缩类型以及相 应的codec , Progressable回调函数用于通知写入的进度，以及在</p>

<p>SequenceFile头文件中存储的Metadata实例。</p>

<p>存储在SequenceFile中的键和值并不一定需要是Writable类型。只要能被 Serialization序列化和反序列化，任何类型都可以。</p>

<p>一旦拥有SequenceFile.Writer实例，就可以通过append()方法在文件末尾附 加键-值对。写完后，可以调用close()方法(SequenceFile.Writer实现了 java.io.Closeable 接口)。</p>

<p>范例5-10显示了一小段代码，它使用刚才描述的API将键-值对写入一个 SequenceFile。</p>

<p>范例5-10.写入SequenceFile对象</p>

<p>public class SequenceFileWriteDemo { private static final String[] DATA = { &ldquo;One, two, buckle my shoe&rdquo;</p>

<p>&ldquo;Three， four, shut the door、</p>

<p>•’Five, six, pick up sticks&rdquo;,</p>

<p>•■Seven, eighty lay them straight”， ■•Nine，ten，a big fat henn</p>

<p>public static void main(String[] args) throws IOException {</p>

<p>String uri = args[0];</p>

<p>Configuration conf = new Configuration();</p>

<p>FileSystem fs = FileSystem.get(URI.create(uriconf);</p>

<p>Path path = new Path(uri);</p>

<p>IntWritable key = new IntWritable();</p>

<p>Text value = new Text();</p>

<p>SequenceFile.Writer writer = null;</p>

<p>try {</p>

<p>writer = SequenceFile.createWriter(fs, confpath, key.getClass()， value.getClass());</p>

<p>for (int i = 0; i &lt; 100; i++) { key•set(100 - i); value.set(DATA[i % DATA.length]);</p>

<p>System.out.printf(n[%s]\t%s\t%s\nHwriter.getLength()key, value);</p>

<p>writer.append(key, value);</p>

<p>}</p>

<p>} finally {</p>

<p>IOUtils•closeStream(writer);</p>

<p>}</p>

<p>}</p>

<p>}</p>

<p>顺序文件中存储的键-值对，键是从100到1降序排列的整数，表示为 IntWritable对象，值是Text对象。在将每条记录追加到SequenceFile. Writer实例末尾之前，我们调用getLength()方法来获取文件的当前位置。(在 下一小节中，如果不按顺序读取文件，则使用这一信息作为记录的边界。)我们把 这个位置信息和键-值对输出到控制台。结果如下所示：</p>

<p>% hadoop SequenceFileMriteDemo numbers.seq</p>

<table>
<thead>
<tr>
<th>[128]</th>
<th>100</th>
</tr>
</thead>

<tbody>
<tr>
<td>[173]</td>
<td>99</td>
</tr>

<tr>
<td>[220]</td>
<td>98</td>
</tr>

<tr>
<td>[264]</td>
<td>97</td>
</tr>

<tr>
<td>[314]</td>
<td>96</td>
</tr>

<tr>
<td>[359]</td>
<td>95</td>
</tr>

<tr>
<td>[404]</td>
<td>94</td>
</tr>

<tr>
<td>[451]</td>
<td>93</td>
</tr>

<tr>
<td>[495]</td>
<td>92</td>
</tr>

<tr>
<td>[545]</td>
<td>91</td>
</tr>
</tbody>
</table>

<p>One, two^ buckle my shoe Three， four， shut the door Five, six， pick up sticks Seven, eight, lay them straight Nine， ten, a big fat hen One, two, buckle my shoe Three, four, shut the door Five， six, pick up sticks Seven, eight, lay them straight Nine， ten， a big fat hen</p>

<p>One, two, buckle my shoe Three, four\ shut the door Five, six， pick up sticks Severy eight， lay them straight Nine， ten， a big fat hen</p>

<p>[4557]    5    One, two, buckle my shoe</p>

<p>[4602]    4    Three, four, shut the door</p>

<p>[4649]    3    Five, six, pick up sticks</p>

<p>[4693]    2    Seven, eight, lay them straight</p>

<p>[4743]    1    Nine, ten, a big fat hen</p>

<p>\2. SequenceFile 的读操作</p>

<p>从头到尾读取顺序文件不外乎创建SequenceFile.Reader实例后反复调用 next()方法迭代读取记录。读取的是哪条记录与你使用的序列化框架相关。如果 使用的是Writable类型，那么通过键和值作为参数的next()方法可以将数据流 中的下一条键-值对读入变量中：</p>

<p>public boolean next(Writable key, Writable val)</p>

<p>如果键-值对成功读取，则返回true，如果已读到文件末尾，则返回false。</p>

<p>对于其他非Writable类型的序列化框架（比如Apache Thrift）,则应该使用下面两 个方法：</p>

<p>public Object next(Object key) throws IOException</p>

<p>public Object getCurrentValue(Object val) throws IOException</p>

<p>在这种情况下，需要确保io.serializations属性已经设置了你想使用的序列 化框架，详情参见5.3.4节。</p>

<p>如果next()方法返回的是非null对象，则可以从数据流中读取键、值对，并且 可以通过getCurrentValue()方法读取该值。否则，如果next()返回null 值，则表示已经读到文件末尾。</p>

<p>范例5-11中的程序显示了如何读取包含Writable类型键、值对的顺序文件。注意 如何通过调用getKeyClass()方法和getValueClass()方法进而发现SequenceFile中所 使用的类型，然后通过ReflectionUtils对象生成常见键和值的实例。通过这个技 术，该程序可用于处理有Writable类型键、值对的任意一个顺序文件。</p>

<p>范例 5-11.读取 SequenceFile</p>

<p>public class SequenceFileReadDemo {</p>

<p>public static void main(String[] args) throws IOException {</p>

<p>String uri = args[0];</p>

<p>Configuration conf = new Configuration();</p>

<p>FileSystem fs = FileSystem.get(URI.create(uri)conf);</p>

<p>Path path = new Path(uri);</p>

<p>SequenceFile.Reader reader = null; try {</p>

<p>reader = new SequenceFile.Reader(fs^ path, conf);</p>

<p>Writable key = (Writable)</p>

<p>ReflectionUtils.newlnstance(reader.getKeyClass(), conf);</p>

<p>.Writable value = (Writable)</p>

<p>ReflectionUtils.newlnstance(reader.getValueClass(), conf); long position = reader.getPosition(); while (reader.nextCkey^ value)) {</p>

<p>String syncSeen = reader.syncSeen() ? 11 *M :</p>

<p>System.out.printf(&rdquo;[%s%s]\t%s\t%s\n&rdquo;, position, syncSeen, key, value); position = reader.getPosition(); // beginning of next record</p>

<p>}</p>

<p>} finally {</p>

<p>IOUtils.closestream(reader);</p>

<p>}</p>

<p>}</p>

<p>}</p>

<p>该程序的另一个特性是能够显示顺序文件中同步点的位置信息。所谓同步点，是 指数据读取迷路(lost)后能够再一次与记录边界同步的数据流中的某个位置，例 如，在数据流中由于搜索而跑到任意位置后可采取此动作。同步点是由 SequenceFile.Writer记录的，后者在顺序文件写入过程中插入一个特殊项以便每 隔几个记录便有一个同步标识。这样的特殊项非常小，因而只造成很小的存储开 销，不到1%。同步点始终位于记录的边界处。</p>

<p>运行范例5-11的程序后，会显示星号表示的顺序文件中的同步点。第一同步点位 于2021处(第二个位于4075处，但本例中并没有显示出来)：</p>

<p>% hadoop SequenceFileReadDemo numbers.seq</p>

<p>[451] 93 [495] 92 [545] 91 [590] 90</p>

<p>[1976] 60 [2021*] 59 [2088] 58 [2132] 57 [2182] 56</p>

<p>One, two, buckle my shoe Three, four\ shut the door Five^ six, pick up sticks Seven, eight, lay them straight Nine, ten， a big fat hen One, two, buckle my shoe Three^ four，shut the door</p>

<p>Five， six, pick up sticks Seven, eight, lay them straight Nine， ten， a big fat hen One, two, buckle my shoe</p>

<p>One, two, buckle my shoe Three， four^ shut the door Five, six， pick up sticks Severy eight， lay them straight Nine, ten， a big fat hen</p>

<p>One, two， buckle my shoe Three， four， shut the door Five, six^ pick up sticks Seven, eight, lay them straight Nine， tei% a big fat hen</p>

<p>在顺序文件中搜索给定位置有两种方法。第一种是调用seek()方法，该方法将读 指针指向文件中指定的位置。例如，可以按如下方式搜查记录边界：</p>

<p>reader.seek(359);</p>

<p>assertThat(reader.next(key, value), is(true)); assertThat(((IntWritable) key).get()，is(95));</p>

<p>但如果给定位置不是记录边界，调用next()方法时就会出错：</p>

<p>reader.seek(360);</p>

<p>reader.next(keyvalue); // fails with IOException</p>

<p>第二种方法通过同步点查找记录边界。SequenceFile.Reader对象的sync(long</p>

<p>position)方法可以将读取位置定位到position之后的下一个同步点。如果position 之后没有同步了，那么当前读取位置将指向文件末尾。这样，我们对数据流中的 任意位置调用sync()方法(不一定是一个记录的边界)而且可以重新定位到下一个同 步点并继续向后读取：</p>

<p>reader.sync(360);</p>

<p>assertThat(reader&gt;getPosition()^ is(2021L)); assertThat(reader.next(key4value)is(true)); assertThat(((IntWritable) key).get(), is(59));</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-83.jpg" alt="img" /></p>

<p>SequenceFile.Writer对象有一个sync()方法，该方法可以在数据流的当前 位置插入一个同步点。不要把它和Syncable接口中定义的hsync()方法混为 一谈，后者用于底层设备缓冲区的同步。详情可以参见3.6.3节。</p>

<p>可以将加入同步点的顺序文件作为MapReduce的输入，因为该类顺序文件允许切 分，由此该文件的不同部分可以由独立的map任务单独处理。参见8.2.3节对</p>

<p>SequenceFilelnputFormat 的详细介绍。</p>

<p>1.通过命令行接口显示SequenceFile</p>

<p>hadoop fs命令有一个-text选项可以以文本形式显示顺序文件。该选项可以查 看文件的代码，由此检测出文件的类型并将其转换成相应的文本。该选项可以识 别gzip压缩文件、顺序文件和Avm数据文件；否则，便假设输入为纯文本文件。</p>

<p>对于顺序文件，如果键和值是有具体含义的字符串表示，那么这个命令就非常有 用(通过toString()方法定义)。同样，如果有自己定义的键或值的类，则需要确 保它们在Hadoop类路径目录下。</p>

<p>对前一小节中创建的顺序文件执行这个命令，我们得到如下输出:</p>

<table>
<thead>
<tr>
<th>% hadoop fs -text numbers.seq | head</th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>100</td>
<td>One, two, buckle my shoe</td>
</tr>

<tr>
<td>99</td>
<td>Three， four\ shut the door</td>
</tr>

<tr>
<td>98</td>
<td>Five^ six^ pick up sticks</td>
</tr>

<tr>
<td>97</td>
<td>Seven， eighty lay them straight</td>
</tr>

<tr>
<td>96</td>
<td>Nine， ten^ a big fat hen</td>
</tr>

<tr>
<td>95</td>
<td>One, two, buckle my shoe</td>
</tr>

<tr>
<td>94</td>
<td>Three, four, shut the door</td>
</tr>

<tr>
<td>93</td>
<td>Five, six, pick up sticks</td>
</tr>

<tr>
<td>92</td>
<td>Seven， eight， lay them straight</td>
</tr>

<tr>
<td>91</td>
<td>Nine, ten， a big fat hen</td>
</tr>
</tbody>
</table>

<p>\2. SequenceFile的排序和合并</p>

<p>MapReduce是对多个顺序文件进行排序（或合并）最有效的方法。MapReduce本身 是并行的，并且可由你指定要使用多少个reducer（该数决定着输出分区数）。例 如，通过指定一个reducer,可以得到一个输出文件。我们可以使用Hadoop发行 版自带的例子，通过指定键和值的类型来将输入和输出指定为顺序文件：</p>

<p>% hadoop jar </p>

<p>$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*ejar \ sort -r 1 </p>

<p>-inFormat org.apache.hadoop.mapreduce.lib.input.SequenceFilelnputFormat \ -outFormat org.apache•hadoop•mapreduce.lib•output•SequenceFileOutputFormat \ -outKey org•apache.hadoop.io.IntWritable </p>

<p>-outvalue org•apache•hadoop•io.Text \ numbers.seq sorted</p>

<p>% hadoop fs -text sorted/part-r-00000 | head</p>

<p>1    Nine, ten， a big fat hen</p>

<p>2    Severy eighty lay them straight</p>

<p>3    Five, six^ pick up sticks</p>

<p>4    Three, four, shut the door</p>

<p>5    One, two, buckle my shoe</p>

<p>6    Nine, ten^ a big fat hen</p>

<p>7    Seven, eight, lay them straight</p>

<p>8    Five, six, pick up sticks</p>

<p>9    Three, four\ shut the door</p>

<p>10    One, two, buckle my shoe</p>

<p>更多详情可以参见9.2节。</p>

<p>除了通过MapReduce实现排序/归并，还有一种方法是使用SequenceFile.Sorter类中的 sort（）方法和merge（）方法。它们比MapReduce更早出现，比MapReduce更底</p>

<p>层（例如，为了实现并行，需要手动对数据进行分区），所以对顺序文件进行排序合 并时采用MapReduce是更佳的选择。</p>

<p>\3. SequenceFile 的格式</p>

<p>顺序文件由文件头和随后的一条或多条记录组成（参见图5-2）。顺序文件的前三个 字节为SEQ（顺序文件代码），紧随其后的一个字节表示顺序文件的版本号。文件头 还包括其他字段，例如键和值类的名称、数据压缩细节、用户定义的元数据以及 同步标识。®如前所述，同步标识用于在读取文件时能够从任意位置开始识别记录 边界。每个文件都有一个随机生成的同步标识，其值存储在文件头中。同步标识</p>

<p>①这些字段的格式细节可参见SequenceFile的文档（/z//p://ZH7./y/^we«ceXcfocs）和源码。</p>

<p>位于顺序文件中的记录与记录之间。同步标识的额外存储开销要求小于1%，所以 没有必要在每条记录末尾添加该标识（特别是比较短的记录）。</p>

<p><img src="Hadoop43010757_2cdb48_2d8748-84.jpg" alt="img" /></p>

<p>Record 姻 Syp&lt; Record</p>

<p>No</p>

<p>compression</p>

<p>Record</p>

<p>compression</p>

<p>4</p>

<p>4</p>

<p>Bo</p>

<p>5-2.压缩前和压缩后的顺序文件的内部结构</p>

<p>记录的内部结构取决于是否启用压缩。如果已经启用压缩，则结构取决于是记录 压缩还是数据块压缩。</p>

<p>如果没有启用压缩（默认情况），那么每条记录则由记录长度（字节数）、键长度、键 和值组成。长度字段为4字节长的整数，遵循java.io.DataOutput类中writelnt（） 方法的协定。为写入顺序文件的类定义Serialization类，通过它来实现键和值 的序列化。</p>

<p>记录压缩格式与无压缩情况基本相同，只不过值是用文件头中定义的codec压缩 的。注意，键没有被压缩。</p>

<p>如图5-3所示，块压缩（block compression）是指一次性压缩多条记录，因为它可以</p>

<p>利用记录间的相似性进行压缩，所以相较于单条记录压缩方法，该方法的压缩效 率更高。可以不断向数据块中压缩记录，直到块的字节数不小于io.seqfile. compress.blocksize属性中设置的字节数：默认为1 MB。每一个新块的开始处 都需要插入同步标识。数据块的格式如下：首先是一个指示数据块中字节数的字 段；紧接着是4个压缩字段（键长度、键、值长度和值）。</p>

<p>图5-3.采用块压缩方式之后，顺序文件的内部结构</p>

<h6 id="5-4-2-关于-mapfile">5.4.2 关于 MapFile</h6>

<p>MapFile是已经排过序的SequenceFile,它有索引，所以可以按键查找。索引 自身就是一个SequenceFile,包含了 map中的一小部分键(默认情况下，是每 隔128个键)。由于索引能够加载进内存，因此可以提供对主数据文件的快速查 找。主数据文件则是另一个SequenceFile,包含了所有的map条目，这些条目 都按照键顺序进行了排序。</p>

<p>MapFile提供了一个用于读写的、与SequenceFile非常类似的接口。需要注意 的是，当使用MapFile.Writer进行写操作时，map条目必须顺序添加，否则会 抛出IOException异常。</p>

<p>\1. MapFile的变种</p>

<p>Hadoop在通用的键-值对MapFile接口上提供了一些变种</p>

<p>• SetFile是一个特殊的MapFile，用于存储Writable键的集合。键必</p>

<p>须按照排好的顺序添加。</p>

<p>•    ArrayFile也是一个MapFile变种，该变种中的键是一个整型，用于 表示数组中元素的索引，而值是一个Writable值。</p>

<p>•    BloomMapFile也是一个MapFile变种，该变种提供了 get()方法的一 个高性能实现，对稀疏文件特别有用。该实现使用一个动态的布隆过滤 器来检测某个给定的键是否在map文件中。这个测试非常快，因为是在 内存中完成的，但是该测试结果出现假阳性的概率大于零。仅当测试通 过时(键存在)，常规的get()方法才会被调用。</p>

<h6 id="5-4-3其他文件格式和面向列的格式">5.4.3其他文件格式和面向列的格式</h6>

<p>顺序文件和map文件是Hadoop中最早的、但并不是仅有的二进制文件格式，事 实上，对于新项目而言，有更好的二进制文件格式可供选择。</p>

<p>Avro数据文件（详见12.3节）在某些方面类似顺序文件，是面向大规模数据处理而 设计的（紧凑且可切分）。但是Avro数据文件又是可移植的，它们可以跨越不同的 编程语言使用。Avro数据文件中存储的对象使用模式来描述，而不是像 Writable对象的实现那样使用Java代码（例如顺序文件就是这样的情况，这样的 弊端是过于以Java为中心）。Avro数据文件被Hadoop生态系统的各组件广为支 持，因此它们被默认为是对二进制格式的一种比较好的选择。</p>

<p>顺序文件、map文件和Avm数据文件都是面向行的格式，意味着每一行的值在文 件中是连续存储的。在面向列的格式中，文件中的行（或等价的，Hive中的一张表） 被分割成行的分片，然后每个分片以面向列的形式存储：首先存储每行第1列的 值，然后是每行第2列的值，如此以往。该过程如图5-4所示。</p>

<p>Logical table</p>

<table>
<thead>
<tr>
<th></th>
<th>co&rdquo;</th>
<th>col2</th>
<th>col3</th>
</tr>
</thead>

<tbody>
<tr>
<td>row!</td>
<td>1</td>
<td>2</td>
<td>幽</td>
</tr>

<tr>
<td>row2</td>
<td>4</td>
<td>5</td>
<td></td>
</tr>

<tr>
<td>row3</td>
<td>7</td>
<td>&lsquo;:::8</td>
<td>顯</td>
</tr>

<tr>
<td>row4</td>
<td>10</td>
<td>11</td>
<td>12</td>
</tr>
</tbody>
</table>

<p>Row-oriented layout (SequenceFile)</p>

<table>
<thead>
<tr>
<th>rowl</th>
<th>row2</th>
<th>row3</th>
<th>row4</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>3i.    .*•    &lsquo;1</td>
<td>4</td>
<td>5</td>
<td>|6</td>
<td>7</td>
<td>8</td>
<td>Q</td>
<td></td>
<td>10</td>
<td>iij</td>
<td>12</td>
</tr>
</tbody>
</table>

<p>Column-oriented layout (RCFile)</p>

<p>J row split 1    J row split 2</p>

<p>5-4.面向行的和面向列的存储</p>

<p>面向列的存储布局可以使一个查询跳过那些不必访问的列。让我们考虑一个只需 要处理图5-4中表的第2列的査询。在像顺序文件这样面向行的存储中，即使是 只需要读取第二列，整个数据行（存储在顺序文件的一条记录中）将被加载进内存。 虽说“延迟反序列化” （lazy deserialization）策略通过只反序列化那些被访问的列字 段能节省一些处理开销，但这仍然不能避免从磁盘上读入一个数据行所有字节而 付出的开销。</p>

<p>如果使用面向列的存储，只需要把文件中第2列所对应的那部分（图中高亮部分）读 入内存。一般来说，面向列的存储格式对于那些只访问表中一小部分列的査询比 较有效。相反，面向行的存储格式适合同时处理一行中很多列的情况。</p>

<p>由于必须在内存中缓存行的分片、而不是单独的一行，因此面向列的存储格式需 要更多的内存用于读写。并且，当出现写操作时（通过flush或sync操作），这种缓 存通常不太可能被控制，因此，面向列的格式不适合流的写操作，这是因为，如 果writer处理失败的话，当前的文件无法恢复。另一方面，对于面向行的存储格 式，如顺序文件和Avro数据文件，可以一直读取到writer失败后的最后的同步 点。由于这个原因，Flume（详见第14章）使用了面向行的存储格式。</p>

<p>Hadoop中的第一个面向列的文件格式是Hive的7?CF//e（Record Columnar File）, 它已经被 Hive 的（9/?CF/7e（Optimized Record Columnar File）及    取代（详见第</p>

<p>13章）。Parquet是一个基于Google Dremel的通用的面向列的文件格式，被 Hadoop组件广为支持。Avro也有一个面向列的文件格式，称为Trevni0</p>

    </div>

    
    

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%AE%A1%E7%AE%97/03-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-hadoop/hadoop-%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/04-%E5%85%B3%E4%BA%8Eyarn/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">04 关于YARN</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/01-%E7%9F%A5%E8%AF%86%E6%A0%91/03-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/02-%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E4%B8%8E%E8%AE%A1%E7%AE%97/03-%E5%A4%A7%E6%95%B0%E6%8D%AE/01-hadoop/hadoop-%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97/06-mapreduce%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/">
            <span class="next-text nav-default">06 MapReduce应用开发</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="iteratelyd@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="http://github.com/iterateself" class="iconfont icon-github" title="github"></a>
      <a href="https://www.zhihu.com/people/thebegin/activities" class="iconfont icon-zhihu" title="zhihu"></a>
      <a href="https://www.douban.com/people/68028070/" class="iconfont icon-douban" title="douban"></a>
  <a href="http://iterate.site/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
    <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">iterateself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>


<script type="text/javascript" src="/dist/even.min.js?v=3.2.0"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
