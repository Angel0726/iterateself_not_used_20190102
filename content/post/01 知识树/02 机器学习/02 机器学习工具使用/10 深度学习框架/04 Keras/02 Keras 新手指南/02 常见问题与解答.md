---
title: 02 常见问题与解答
toc: true
date: 2018-10-20
---

# 需要补充的

- 感觉很多问题可以合并到主体里面。
- 还是不错的。

# Keras FAQ：常见问题


## 如何在文章中引用 Keras？

如果Keras对你的研究有帮助的话，请在你的文章中引用 Keras。这里是一个使用 BibTex 的例子

```python
@misc{chollet2015keras,
  author = {Chollet, François and others},
  title = {Keras},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/fchollet/keras}}
}
```

<span style="color:red;">嗯，一直没有很明白，Keras 对你的研究有帮助是指的什么？什么程度的帮助需要引用这个。</span>

***

## 如何使Keras调用GPU？

如果采用 TensorFlow 作为后端，当机器上有可用的GPU时，代码会自动调用GPU进行并行计算。如果使用 Theano 作为后端，可以通过以下方法设置：

方法1：使用 Theano 标记

在执行 python 脚本时使用下面的命令：

```python
THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py
```

方法2：设置 `.theano` 文件

点击[这里](http://deeplearning.net/software/theano/library/config.html)查看指导教程

方法3：在代码的开头处手动设置 `theano.config.device` 和 `theano.config.floatX`

```python
	import theano
	theano.config.device = 'gpu'
	theano.config.floatX = 'float32'
```

<span style="color:red;">嗯，还是第三种方法比较好，这个感觉还是要抽出来，放在这里有点不显眼。</span>


## 如何在多张GPU卡上使用Keras？

我们建议有多张 GPU 卡可用时，使用 TensorFlow 后端。<span style="color:red;">为什么多张 GPU 的时候，使用 Tensorflow 后端？</span>

有两种方法可以在多张 GPU 上运行一个模型：数据并行/设备并行

大多数情况下，你需要的很可能是 “数据并行”


### 数据并行

数据并行将目标模型在多个设备上各复制一份，并使用每个设备上的复制品处理整个数据集的不同部分数据。Keras在 `keras.utils.multi_gpu_model` 中提供有内置函数，该函数可以产生任意模型的数据并行版本，最高支持在8片GPU上并行。

后面我们会介绍。

下面是一个例子：

```python
from keras.utils import multi_gpu_model

# Replicates `model` on 8 GPUs.
# This assumes that your machine has 8 available GPUs.
parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',
                       optimizer='rmsprop')

# This `fit` call will be distributed on 8 GPUs.
# Since the batch size is 256, each GPU will process 32 samples.
parallel_model.fit(x,
                  y,
                  epochs=20,
                  batch_size=256)
```

<span style="color:red;">嗯，这样的并行的，感觉不错。</span>

### 设备并行

设备并行是在不同设备上运行同一个模型的不同部分，当模型含有多个并行结构，例如含有两个分支时，这种方式很适合。<span style="color:red;">什么是含有多个并行结构？什么是两个分支？</span>

这种并行方法可以通过使用 TensorFlow device scopes 实现，下面是一个例子：


```python
# Model where a shared LSTM is used to encode two different sequences in parallel
input_a = keras.Input(shape=(140, 256))
input_b = keras.Input(shape=(140, 256))

shared_lstm = keras.layers.LSTM(64)

# Process the first sequence on one GPU
with tf.device_scope('/gpu:0'):
    encoded_a = shared_lstm(tweet_a)
# Process the next sequence on another GPU
with tf.device_scope('/gpu:1'):
    encoded_b = shared_lstm(tweet_b)

# Concatenate results on CPU
with tf.device_scope('/cpu:0'):
    merged_vector = keras.layers.concatenate([encoded_a, encoded_b],
                                             axis=-1)
```

嗯，上面的程序有几个想要明白的地方：

- <span style="color:red;">什么是 shared LSTM ，是用在什么地方的？</span>
- <span style="color:red;">上面的 tweet_a 和 tweet_b 是写错了吧？</span>
- <span style="color:red;">一直对 concatenate 的时候的 axis 有点疑虑，不知道是要怎么拼接。</span>
- <span style="color:red;">keras 里面的 LSTM 的使用这么方便的吗？</span>
- <span style="color:red;">这个 LSTM 共享的原理是什么？之前好像没有怎么了解过共享模型。要总结下。</span>

## "batch", "epoch"和"sample"都是啥意思？？

下面是一些使用keras时常会遇到的概念，我们来简单解释。

- Sample：样本，数据集中的一条数据。例如图片数据集中的一张图片，语音数据中的一段音频。
- Batch：中文为批，一个 batch 由若干条数据构成。batch 是进行网络优化的基本单位，网络参数的每一轮优化需要使用一个 batch。batch 中的样本是被并行处理的。与单个样本相比，一个 batch 的数据能更好的模拟数据集的分布，batch 越大则对输入数据分布模拟的越好，反应在网络训练上，则体现为能让网络训练的方向“更加正确”。但另一方面，一个batch也只能让网络的参数更新一次，因此网络参数的迭代会较慢。在测试网络的时候，应该在条件的允许的范围内尽量使用更大的 batch，这样计算效率会更高。<span style="color:red;">是的，现在遇到的问题就是 batch 受限于显存比较多，只能用小的 batch。还没有查过有什么好办法解决，要确认下。</span>
- Epoch，epoch 可译为“轮次”。如果说每个 batch 对应网络的一次更新的话，一个 epoch对应的就是网络的一轮更新。每一轮更新中网络更新的次数可以随意，但通常会设置为遍历一遍数据集。因此一个epoch的含义是模型完整的看了一遍数据集。<span style="color:red;">嗯。</span>	设置 epoch 的主要作用是把模型的训练的整个训练过程分为若干个段，这样我们可以更好的观察和调整模型的训练。Keras 中，当指定了验证集时，每个 epoch 执行完后都会运行一次验证集以确定模型的性能。另外，我们可以使用回调函数在每个 epoch 的训练前后执行一些操作，如调整学习率，打印目前模型的一些信息等，详情请参考 Callback 一节。<span style="color:red;">嗯，</span>



## 如何保存Keras模型？

我们不推荐使用 pickle 或 cPickle 来保存 Keras 模型。<span style="color:red;">还可以用这个来保存模型的吗？</span>

你可以使用 `model.save(filepath)` 将Keras模型和权重保存在一个 HDF5 文件中，该文件将包含：

* 模型的结构，以便重构该模型
* 模型的权重
* 训练配置（损失函数，优化器等）
* 优化器的状态，以便于从上次训练中断的地方开始

<span style="color:red;">还可以存储模型的结构和训练配置的吗？包含这四项的 h5 文件会不会很大，全部保存与只保存模型的权重那种比较好？</span>

使用 `keras.models.load_model(filepath)` 来重新实例化你的模型，如果文件中存储了训练配置的话，该函数还会同时完成模型的编译。

例子：

```python
from keras.models import load_model

model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'
del model  # deletes the existing model

# returns a compiled model
# identical to the previous one
model = load_model('my_model.h5')
```

如果你只是希望保存模型的结构，而不包含其权重或配置信息，可以使用：

```python
# save as JSON
json_string = model.to_json()

# save as YAML
yaml_string = model.to_yaml()
```

这项操作将把模型序列化为 json 或 yaml 文件，这些文件对人而言也是友好的，如果需要的话你甚至可以手动打开这些文件并进行编辑。

当然，你也可以从保存好的 json 文件或 yaml 文件中载入模型：

```python
# model reconstruction from JSON:
from keras.models import model_from_json
model = model_from_json(json_string)

# model reconstruction from YAML
model = model_from_yaml(yaml_string)
```

<span style="color:red;">嗯，把模型序列保存在 json 里面一般什么时候会用到这个功能？在 json 里修改而代码不改，感觉会乱吧。。</span>

<span style="color:blue;">而且，如果我之前把权重保存在 h5 文件里，把模型的结构放在 json 里，那么我怎么都加载进来？嗯，下面就介绍了可以通过 `model.load_weights` 把权重从 h5 文件中加载进来。</span>

如果需要保存模型的权重，可通过下面的代码利用 HDF5 进行保存。注意，在使用前需要确保你已安装了 HDF5 和其 Python 库 h5py：

```python
model.save_weights('my_model_weights.h5')
```

如果你需要在代码中初始化一个完全相同的模型，请使用：

```python
model.load_weights('my_model_weights.h5')
```

如果你需要加载权重到不同的网络结构（有些层一样）中，例如 `fine-tune`或 `transfer-learning`，你可以通过层名字来加载模型：

```python
model.load_weights('my_model_weights.h5', by_name=True)
```

例如：
```python
"""
假如原模型为：
    model = Sequential()
    model.add(Dense(2, input_dim=3, name="dense_1"))
    model.add(Dense(3, name="dense_2"))
    ...
    model.save_weights(fname)
"""

# new model
model = Sequential()
model.add(Dense(2, input_dim=3, name="dense_1"))  # will be loaded
model.add(Dense(10, name="new_dense"))  # will not be loaded

# load weights from first model; will only affect the first layer, dense_1.
model.load_weights(fname, by_name=True)

```

<span style="color:red;">这样只加载固定的某些层，然后某些层不加载，是想得到什么效果？为什么会有这种操作？目的是什么？感觉有些 puzzle。一般什么情况下会用到这个？</span>


## 为什么训练误差比测试误差高很多？

一个 Keras 的模型有两个模式：训练模式和测试模式。一些正则机制，如 Dropout，L1/L2 正则项在测试模式下将不被启用。<span style="color:red;">为什么 L1/L2 在测试模式下不启用？</span>

另外，训练误差是训练数据每个 batch 的误差的平均。在训练过程中，每个 epoch 起始时的 batch 的误差要大一些，而后面的 batch 的误差要小一些。另一方面，每个 epoch 结束时计算的测试误差是由模型在 epoch 结束时的状态决定的，这时候的网络将产生较小的误差。

可以通过定义回调函数将每个 epoch 的训练误差和测试误差并作图，如果训练误差曲线和测试误差曲线之间有很大的空隙，说明你的模型可能有过拟合的问题。当然，这个问题与 Keras 无关。<span style="color:red;">嗯，要怎么把训练误差和测试误差合并在图中？非常想知道。</span>


## 如何获取中间层的输出？

一种简单的方法是创建一个新的 `Model`，使得它的输出是你想要的那个输出

```python
from keras.models import Model

model = ...  # create the original model

layer_name = 'my_layer'   # 想要的层的名称
intermediate_layer_model = Model(input=model.input,
                                 output=model.get_layer(layer_name).output)
intermediate_output = intermediate_layer_model.predict(data)
```

<span style="color:red;">嗯，一般，我们什么时候需要输出中间层呢？而且，想知道那个神经网络的各个神经曾的输出的画面是不是这样做出来的？还是说直接能从 IDE 里面看到？</span>


此外，我们也可以建立一个 Keras 的函数来达到这一目的：

```python
from keras import backend as K

# with a Sequential model
get_3rd_layer_output = K.function([model.layers[0].input],
								  [model.layers[3].output])
layer_output = get_3rd_layer_output([X])[0]
```

<span style="color:red;">嗯，这个 `[X]` 是什么？而且，这个 get_3rd_layer_output 在什么时候被调用？想知道一个完整的例子。</span>

当然，我们也可以直接编写 Theano 和 TensorFlow 的函数来完成这件事

注意，如果你的模型在训练和测试两种模式下不完全一致，例如你的模型中含有 Dropout 层，批规范化（BatchNormalization）层等组件，你需要在函数中传递一个 learning_phase 的标记，像这样：

```python
get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],
								  [model.layers[3].output])

# output in test mode = 0
layer_output = get_3rd_layer_output([X, 0])[0]

# output in train mode = 1
layer_output = get_3rd_layer_output([X, 1])[0]
```

<span style="color:red;">有些没明白，这个 0 和 1 是 K.learnging_phase 吗？是学习率吗？还是说 1 只是作为一个标识用的？</span>


## 如何利用Keras处理超过机器内存的数据集？


可以使用 `model.train_on_batch(X,y)` 和 `model.test_on_batch(X,y)`

另外，也可以编写一个每次产生一个batch样本的生成器函数，并调用 `model.fit_generator(data_generator, samples_per_epoch, nb_epoch)` 进行训练。<span style="color:red;">嗯，之前看到的就是这种形式的。</span>

这种方式在 Keras 代码包的 example 文件夹下 CIFAR10 例子里有示范。<span style="color:red;">嗯，这个例子也要总结进来，实际上，对于图像处理来说，很多代码都比较通用，即使不通用也很有借鉴意义。</span>


## 当验证集的loss不再下降时，如何中断训练？

可以定义```EarlyStopping```来提前终止训练


```python
from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss',
                                patience=2)
model.fit(X,
          y,
          validation_split=0.2,
          callbacks=[early_stopping])
```

<span style="color:red;">嗯，回调函数还是要仔细总结下的。</span>

<span style="color:red;">上面这个 patience 是什么？是说两次都不满足条件就终止吗？</span>




## 验证集是如何从训练集中分割出来的？

如果在 `model.fit` 中设置 `validation_spilt` 的值，则可将数据分为训练集和验证集，例如，设置该值为 0.1，则训练集的最后 10% 数据将作为验证集，设置其他数字同理。

注意，原数据在进行验证集分割前并没有被 shuffle，所以这里的验证集严格的就是你输入数据最末的 x%。<span style="color:red;">嗯，那要想加 shuffle 怎么加？</span>

<span style="color:red;">这个地方还是要补充一个完整的例子的。</span>


## 训练数据在训练时会被随机洗乱吗？

是的，如果 `model.fit` 的 `shuffle` 参数为真，训练的数据就会被随机洗乱。

不设置时默认为真。训练数据会在每个 epoch 的训练中都重新洗乱一次。<span style="color:red;">嗯，好的，自己写的时候还是要明确下。</span>

验证集的数据不会被洗乱。<span style="color:red;">嗯，是这样吗？</span>


## 如何在每个epoch后记录训练/测试的loss和正确率？


`model.fit` 在运行结束后返回一个 `History` 对象，其中含有的 `history` 属性包含了训练过程中损失函数的值以及其他度量指标。<span style="color:red;">一个平常的模型，训练完后的 history 里有哪些指标？想详细了解下。</span>


```python
hist = model.fit(X, y, validation_split=0.2)
print(hist.history)
```

<span style="color:red;">想看下完整的例子，想知道 hist.history 里面会有什么，分别是做什么用的。</span>

## 如何使用状态RNN（stateful RNN）？

<span style="color:red;">什么是状态 RNN ？好像第一次听到这个词？与普通的 RNN 有什么区别吗？</span>

一个 RNN 是状态 RNN，意味着训练时每个 batch 的状态都会被重用于初始化下一个 batch 的初始状态。

当使用状态 RNN 时，有如下假设：

* 所有的 batch 都具有相同数目的样本
* 如果 `X1` 和 `X2` 是两个相邻的batch，那么对于任何 `i` ，`X2[i]` 都是 `X1[i]` 的后续序列

要使用状态 RNN，我们需要：

* 显式的指定每个 batch 的大小。可以通过模型的首层参数 `batch_input_shape` 来完成。 `batch_input_shape` 是一个整数 tuple，例如 (32,10,16) 代表一个具有 10 个时间步，每步向量长为 16，每 32 个样本构成一个 batch 的输入数据格式。<span style="color:red;">什么是时间步？什么是每步向量长？</span>
* 在 RNN 层中，设置 `stateful=True`

要重置网络的状态，使用：

* `model.reset_states()` 来重置网络中所有层的状态
* `layer.reset_states()` 来重置指定层的状态

<span style="color:red;">没明白，为什么要重置层的状态？</span>

例子：


```python
X  # this is our input data, of shape (32, 21, 16)
# we will feed it to our model in sequences of length 10

model = Sequential()
model.add(LSTM(32, input_shape=(10, 16), batch_size=32, stateful=True))
model.add(Dense(16, activation='softmax'))

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

# we train the network to predict the 11th timestep given the first 10:
model.train_on_batch(X[:, :10, :], np.reshape(X[:, 10, :], (32, 16)))

# the state of the network has changed. We can feed the follow-up sequences:
model.train_on_batch(X[:, 10:20, :], np.reshape(X[:, 20, :], (32, 16)))

# let's reset the states of the LSTM layer:
model.reset_states()

# another way to do it in this case:
model.layers[0].reset_states()
```



注意，`predict`，`fit`，`train_on_batch`
，`predict_classes` 等方法都会更新模型中状态层的状态。这使得你不但可以进行状态网络的训练，也可以进行状态网络的预测。

上面的程序有几个地方想知道的：

- <span style="color:red;">状态 RNN 就是指 LSTM 吗？还是什么？</span>
- <span style="color:red;">input data 的 shape 为什么是 (32, 21, 16) 这样的，是什么意思？</span>
- <span style="color:red;">对于 LSTM 来说，stateful=True 和 False 区别是什么？</span>
- <span style="color:red;">rmsprop 是什么？之前好像看到过，忘记了。</span>
- <span style="color:red;">为什么上面的 train_on_batch 是分两个阶段的？</span>
- <span style="color:red;">为什么要 reset_states ？</span>


## 如何“冻结”网络的层？

“冻结”一个层指的是该层将不参加网络训练，即该层的权重永不会更新。

一般什么时候会用到呢：

- 在进行 fine-tune 时我们经常会需要这项操作。<span style="color:red;">这个的确是这样，比如训练自己的 YOLO 就是把 Darknet 的固定住，训练后面的三个全连接层。</span>
- 在使用固定的 embedding 层处理文本输入时，也需要这个技术。<span style="color:red;">这个 embedding 层是什么？为什么会用到冻结？</span>

可以通过向层的构造函数传递 `trainable` 参数来指定一个层是不是可训练的，如：

```python
frozen_layer = Dense(32,trainable=False)
```

此外，也可以通过将层对象的 `trainable` 属性设为 `True` 或 `False` 来为已经搭建好的模型设置要冻结的层。
在设置完后，需要运行 `compile` 来使设置生效，例如：

```python
x = Input(shape=(32,))
layer = Dense(32)
layer.trainable = False
y = layer(x)

frozen_model = Model(x, y)
# in the model below, the weights of `layer` will not be updated during training
frozen_model.compile(optimizer='rmsprop', loss='mse')

layer.trainable = True
trainable_model = Model(x, y)
# with this model the weights of the layer will be updated during training
# (which will also affect the above model since it uses the same layer instance)
trainable_model.compile(optimizer='rmsprop', loss='mse')

frozen_model.fit(data, labels)  # this does NOT update the weights of `layer`
trainable_model.fit(data, labels)  # this updates the weights of `layer`
```

<span style="color:red;">上面的 y = layer(x) 是什么意思？说实话，这种 compile 然后 fit 的流程还很带感的。编译出模型，然后训练。嗯，想知道这个编译后会生成一个文件吗？在哪里？里面的内容是什么样的？在把这个东西进行 fit 的时候，内部代码是什么样的？</span>


## 如何从 Sequential 模型中去除一个层？

可以通过调用 `.pop()` 来去除模型的最后一个层，反复调用 n 次即可去除模型后面的 n 个层。

```python
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=784))
model.add(Dense(32, activation='relu'))

print(len(model.layers))  # "2"

model.pop()
print(len(model.layers))  # "1"
```

<span style="color:red;">竟然可以这样，那被 pop 出去的层还能加回来吗？为什么要把一些层 pop 掉？pop 掉之后，输出的数据格式不是变了吗？</span>

## 如何在Keras中使用预训练的模型？

我们提供了下面这些图像分类的模型代码及预训练权重：

- VGG16
- VGG19
- ResNet50
- Inception v3

<span style="color:red;">现在使用 VGG 的还多吗？Inception V3 是用在什么地方的？图像识别定位吗？要好好总结下。</span>

可通过 `keras.applications` 载入这些模型：

```python
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.applications.resnet50 import ResNet50
from keras.applications.inception_v3 import InceptionV3

model = VGG16(weights='imagenet', include_top=True)
```

这些代码的使用示例请参考 `.Application` 模型的文档。


使用这些预训练模型进行特征抽取或 fine-tune 的例子可以参考[此博客](http://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)

<span style="color:red;">嗯，要总结进来，而且，想知道是怎么进行特征抽取的？</span>


VGG模型也是很多Keras例子的基础模型，如：

* [Style-transfer](https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py)
* [Feature visualization](https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py)
* [Deep dream](https://github.com/fchollet/keras/blob/master/examples/deep_dream.py)


<span style="color:red;">嗯，keras 的例子还是都要总结进来的。</span>


## 如何在Keras中使用HDF5输入？

你可以使用 keras.utils 中的 `HDF5Matrix` 类来读取HDF5输入。

可以直接使用 HDF5 数据库，示例：<span style="color:red;">HDF5 数据库与 h5 文件是什么关系？</span>

```python
import h5py
with h5py.File('input/file.hdf5', 'r') as f:
    X_data = f['X_data']
    model.predict(X_data)
```

<span style="color:red;">HDF5 到底是什么文件？类似一个数据库吗？好像没怎么接触过。</span>

## Keras的配置文件存储在哪里？

所有的 Keras 数据默认存储在：

```bash
$HOME/.keras/
```

对windows用户而言，`$HOME` 应替换为 `%USERPROFILE%`

当 Keras 无法在上面的位置创建文件夹时（例如由于权限原因），备用的地址是 `/tmp/.keras/`

Keras 配置文件为 JSON 格式的文件，保存在 `$HOME/.keras/keras.json`。默认的配置文件长这样：

```
{
    "image_data_format": "channels_last",
    "epsilon": 1e-07,
    "floatx": "float32",
    "backend": "tensorflow"
}
```


该文件包含下列字段：


- 默认的图像数据格式 `channels_last` 或 `channels_first` <span style="color:red;">嗯。</span>
- 用于防止除零错误的 `epsilon` <span style="color:red;">这个是用来干什么的？</span>
- 默认的浮点数类型 <span style="color:red;">这个改了会怎么样？</span>
- 默认的后端

类似的，缓存的数据集文件，即由 `get_file()` 下载的文件，默认保存在 `$HOME/.keras/datasets/`


<span style="color:red;">这个.keras 下面的文件要不要移动到项目里？不然别人用的时候还需要重新配置吧？</span>

## 在使用Keras开发过程中，我如何获得可复现的结果?

<span style="color:red;">这个不清楚，看看是在什么时候使用的，最好有完整的例子。</span>

在开发模型中，有时取得可复现的结果是很有用的。例如，这可以帮助我们定位模型性能的改变是由模型本身引起的还是由于数据上的变化引起的。

下面的代码展示了如何获得可复现的结果，该代码基于 Python3 的 tensorflow 后端：

```python
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/fchollet/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,
                              inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(),
                  config=session_conf)
K.set_session(sess)

# Rest of code follows ...
```



<span style="color:red;">对于上面这个完全不清楚。random.seed 也不是很清楚。都要补充下。</span>






# 相关资料

- [Keras中文文档](https://keras-cn.readthedocs.io/en/latest/)
