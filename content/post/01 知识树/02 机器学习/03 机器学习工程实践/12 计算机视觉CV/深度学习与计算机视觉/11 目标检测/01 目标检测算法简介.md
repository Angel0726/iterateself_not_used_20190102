---
title: 01 目标检测算法简介
toc: true
date: 2018-08-29
---
##### 第11章目标检测



第11章针对R-CNN系和YOLO/SSD系这两类算法，简要介绍了基于深度学习的目标 检测算法的发展史，并给出了基于MXNet的SSD检测算法实例，以及分析了结果的可视化。



本章简要介绍基于卷积神经网络的目标检测(Object Detection)算法，并运行一个基 于SSD (Single Shot Detection)算法的目标检测例子。

11.1目标检测算法简介

本节介绍常见的目标检测算法背后的基本思想，并简要回顾基于深度学习的目标检测 算法发展历史。本章介绍的所有主要方法的文献下载链接可以在本书的github仓库中找到。

滑窗(Sliding Window)法的思路极其简单，首先需要一个已经训练好的分类器，然 后把图像按照一定间隔和不同的大小分成一个个窗口，在这些窗口上执行分类器，如果得 到较高的分类分数，就认为是检测到了物体。把每个窗口都用分类器执行一遍之后，再对 得到的分数做一些后处理如非极大值抑制(Non-Maximum Suppression, NMS)等，最后 就得到了物体类别和对应区域，其方法示意图如图11-1所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-286.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-287.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-288.jpg)



图11-1滑窗法检测物体示意图

判断是否鸭子的分 类器给每个窗口打 分，并筛选出分数 最高的窗口



滑窗法非常简单，但是效率极其低下，尤其是还要考虑物体长宽比。如果是执行比较

耗时的分类器，用滑窗法就不太现实。常见的都是一些小型分类网络和滑窗法结合的应用， 如Dalle Molle人工智能研究所（IDSIA）的高级研究员Dan Claudiu CireSan就做过用卷积 神经网络结合滑窗法，检测胸腔切片图像中的有丝分裂用于辅助癌症诊断，论文《Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks》于 2013 年发表在医 学影像处理领域最顶级的刊物MICCAI上。

11.1.2 PASCAL VOC、mAP 和 IOU 简介

在继续介绍目标检测算法演进历史之前，有必要先介绍一下最常用的目标检测算法数 据集，以及评估一个目标检测算法的最常见指标。

PASCAL VOC，全称是 Pattern Analysis Statistical Modelling and Computational Learning, Visual Object Classes,是一套用于评估图像分类、检测、分割和人体姿势动作等 的数据集，当然被用到最多的，还是物体检测。PASCAL VOC包含4大类共20个细分类 别，分别是人、动物（鸟、猫、牛、狗、马、羊）、交通工具（飞机、自行车、船、大巴、 轿车、摩托车、火车）、室内（瓶子、椅子、餐桌、盆栽、沙发、电视/显示器）。

直观上讲，评价一个检测算法的时候，主要看两个标准，即是否正确预测了框内的物 体类别；预测的框和人工标注框的重合程度。这两个标准的量化指标分别是mAP （mean Average Precision）和 IOU （Intersection Over Union） o

mAP中文翻译过来叫做平均精度均值，其中的概念在第10章已经讲解过。mAP 是把每个类别的//P都单独“拎”出来，然后计算所有类别的平均值，代表着对检测到 的目标平均精度的一个综合评价。

/6X7用来衡量预测的物体框和真实框的重合程度，计算方法如图11-2所示。

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-291.jpg)

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-292.jpg)

IOU=-

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-293.jpg)

图11-2计算（交并比）的示意图

图11-2中实线是人工标注的框，虚线是模型预测的框。重合度的计算方法如图11-2最 右边的灰色区域所示，是用两个框重合的面积，除以两个框并集所占的面积，所以叫做交 并比。评价一个算法的时候，一种常见的方法是先设定一个/Of；的阈值，只要算法找到的 框的/O[/大于这个阈值，就是一个有效的检测，把结果拿来计算作为最终的评价指 标。在PASCAL VOC中，这个阈值设定为0.5。需要提示的是，物体检测的评价方法仍在 不断演化中，如虽然简单易懂，不过很多时候未必合理，视觉上重合度差不多的两 个框，实际应用中很可能因为分辨率不同得到差异很大的值。

11.1.3 Selective Search 和 R-CNN 简介

滑窗法相当于对一张图像上的子区域进行类似穷举式的搜索，一般情况下这种低效率 搜索到的区域里大部分都是没有任何物体的。所以一个很自然的想法就是只对图像中最有 可能包含物体的区域进行搜索，进而提升物体检测的效率。在这种思路的方法中，最为熟 知的是 Selective Search。

Selective Search的思路是，可能存在物体的区域都应该是有某种相似性的或连续的区 域。针对这两个特点，Selective Search采用的是超像素合并的思路，首先用分割算法在图 像上产生很多的小区域，这些区域就是最基础的子区域，或者可以看作是超像素。然后根 据这些区域之间的相似性进行区域合并，成为大一点的区域。衡量相似性的标准可以是颜 色、纹理和大小等。不断迭加这种小区域合并为大区域的过程，最后整张图会合并成为一 个区域。这个过程中，给每个区域做一个外切的矩形，就得到了许许多多的可能是物体的 区域方框。其实无论是滑窗还是Selective Search,这种找出可能包含物体的区域的方法， 统称为Region Proposal。算法执行过程的示意图如图11-3所示。



图 11-3 Selective Search 进行 Region Proposal 示意图

可以看到Selective Search和滑窗法相比第一个优点就是高效，因为不再是漫无目的的 穷举式搜索。另外，在Selective Search中，一开始的区域是小区域，合并过程中不断产生 大区域，所以天然能够包含各种大小不同的疑似物体框。另外计算相似度采用了多样的指 标，提升了找到物体的可靠性。至于算法本身当然也不能太慢，否则和滑窗法相比的优势 就体现不出来了。算法的具体细节这里就不展开了，有兴趣的读者可以参考作者J.R.R. Uijlings 的论文《Selective Search for Object Recognition》o

有了 Selective Search高效地寻找到可能包含物体的方框（实际中常进行一定像素的外 扩包含一定背景），那么接下来只要接个CNN提取特征，然后做个分类不就相当于检测 吗？这正是Ross B. Girshick的基于深度学习做物体检测的开山之作R-CNN （Region-based Convolutional Neural Networks）,当然直接用Selective Search选出的框未必精确，所以还 加入了一些改进，如和物体标注框的位置的回归来修正Selective Search提出的原始物体框。 R-CNN就像Alexnet—样，让物体检测的指标跃上了新台阶（PASCAL VOC，mAP: 40.1%

—53.3%)。R-CNN的更多细节可以参考发表在2014年CVPR上的论文《Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation》o

11.1.4 SPP、ROI Pooling 和 Fast R-CNN 简介

R-CNN虽然比起滑窗法已经快了很多，但可用性还是很差，因为一个稍微“靠谱”的 识别任务需要用Selective Search提出上千个框(R-CNN中是2000个)。这上千个图像区 域都需要单独过一遍卷积神经网络进行前向计算，速度自然快不了。

\1. SPP和ROI Pooling简介

在第4章中已经讲过了卷积的同变性质(equivariance)。第10章的最后通过每类别 对应物体的激活响应图的例子，定性理解了物体通过卷积网络之后，会在语义层的响应图 上的对应位置有特别的响应。所以一个思路就是，对整张图片执行一次卷积神经网络的前 向计算，到了最后一层的激活响应图的时候，通过某种方式把目标物体所在区域部分的响 应图拿出来作为特征给分类器。这样做对画面内所有可能物体的检测就可以共享一次卷积 神经网络的前向计算，大大节省了时间。第一个在物体检测中实现这个思路的就是当时还 在MSRA的Kaiming He提出的SPP,全称为空间金字塔池化(Spatial Pyramid Pooling)， 示意图如图11-4所示。



如图11-4所示，假设输入图片中框住小马和摄影师的两个框是Selective Search选出 来的框，那么经过了(没有全连接层的)卷积神经网络，到了最后一层输出的A2个通道的 响应图时，原图像上的两个框也会对应两个区域。这样的区域称为感兴趣区域(Region Of Interest, ROI)。一般常用的分类器，无论是SVM还是称浅层神经网络，都需要输入固 定的维度。所以如果可以有一种方式把ROI的信息都转化成固定维度的向量，那么就能把 每个ROI都给分类器去计算获得结果，并且在进入分类器之前，只需要运行一次卷积神经 网络的前向计算，所有的ROI都共享同样的响应图。

SPP就是这样一种方法，对于每一个ROI,如图11-4中所示，SPP分层将指定区域划 分为不同数目，图中分为3个层次，最底层划分为4x4=16个子区域，中间层是2x2=4个 子区域，最顶层则直接整个区域进行池化，对于每个通道，每个ROI变成了一个21维的 向量，因为有个个通道，所以每个ROI都生成了一个21«维的向量。因为越是底层划分 的数目越多，SPP是通过像金字塔一样的结构来获得感兴趣区域不同尺度的信息，所以叫 空间金字塔池化。借助SPP,不仅实现了对ROI的分类，而且对于整张图像只需要进行一 次卷积神经网络的前向计算，大幅降低了算法执行的时间。另外需要提的是，这里只讲了 将SPP用于检测的思路，其实SPP把任意大小的向量转化为固定大小的向量的方法还有另 -个意义，就是让输入神经网络的图像大小不再固定，在执行分类任务的时候，这种做法 的优点是不需要再对图像进行裁剪或者缩放。SPP的论文《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》发表在 2014 的 ECCV 上，有兴趣的读者可

以自行搜索原文参考。

在SPP中，包含信息最多的其实就是最底层，所以另一个思路是直接把ROI划分为固 定的大小，并不分层。如把所有RO1区域池化到7x7的大小，再送入分类器，这就是ROI Pooling,如图11-4的右下部分所示。

\2. FastR-CNN简介

SPP用于物体检测相比R-CNN获得了速度上的巨大提升，但仍然继承了一些R-CNN 的缺点。最明显的是分阶段训练，不仅麻烦，而且物体框回归训练过程和卷积神经网络的 训练过程是割裂的，整个参数优化的过程不是一体的，限制了达到更高精度的可能性。

针对SPP的缺点，R-CNN的作者RossB.Girshick再度发力，在SPP检测的基础上提 出了两个主要的改进/变化：第一点是RO1提取特征后，把物体框的回归和分类这两个任务 的loss融合一起训练，相当于端到端的多任务训练（end-to-end with a multi-task loss）。这 让整个训练过程不再是多个步骤分别进行，训练效率也更高；第二点是把SPP换成了 R01 Pooling,这就是Fast R-CNN。在计算预测的框和标注框的loss时，Ross B. Girshick并没 有像在R-CNN和SPP中那样采用常见的Z2方法，而是采用了一种叫做Sw⑽Zb的loss 计算方法：

£ioc（/M?v）= Z smooth^ -v,.）    （公式 11-1）

ie[x,y,w,h}

###### 其中，

smoothL\ (x) = *

0.5x2, |x| < 1 x|-0.5,其他

(公式11-2)

其实就是把12和£1拼一块了，其中小的偏差利用£2计算，大的偏差利用L1计算。 对偏差很大的值没有那么敏感，好处是提高了 loss计算的稳定性。

在这种框架下，因为卷积神经网络计算对每张图像只执行了一次，所以重复计算大都 在ROI Pooling之后，于是Ross B. Girshick又提出用SVD分解然后忽略次要成分，把全连 接层的计算量减小，达到精度损失极其微小的情况下，获得较大幅度的计算速度提高，这

也是算是论文中的一个小的优点。Fast R-CNN的更多细节可以参考2015年ICCV的论文 《Fast R-CNN》。

11.1.5 RPN 和 Faster R-CNN 简介

Fast R-CNN主要改进的是卷积神经网络开始往后面的计算，这部分的计算速度大幅提 升，Selective Search反倒成为了限制计算效率的瓶颈。所以是不是可以考虑用神经网络的 办法取代 Selective Search 呢？这次 Ross B. Girshick 联合 Kaiming He 一起提出了 Faster R-CNN o

在第10章末尾激活响应图的例子中了解到，响应图中是可以包含粗略的位置信息的， 所以Region Proposal的这一步也完全可以放到最后一层响应图上来做。所以在Faster R-CNN 中，Region Proposal Networks (RPN)就被提出来替代 Selective Search。这样做的 一个重要意义是算法的所有步骤都被包含到一个完整的框架中了，实现了端到端的训练。

RPN首先对基础网络的最后一层卷积响应图，按照执行一次卷积，输出指定通道 数(原文中为256, github代码中为512)的响应图。这步相当于用滑窗法对响应图进行特 征提取，在论文中n的大小是3,也就是3x3的窗口大小。然后对得到的响应图的每个像 素分别进入两个全连接层：一个计算该像素对应位置是否有物体的分数，输出是或否的分 数，所以有两个输出；另一个计算物体框的二维坐标和大小，所以有4 +输出。其中对于 每一个APQ;卷积输出的响应图像素，都尝试用中心在该像素位置，不同大小和不同长宽比 的窗口作为anchor box,回归物体框坐标和大小的网络是在anchor box基础上做offset。所 以假设有&个anchor box，则计算是否有物体分数的输出实际有2A个，计算物体框坐标和 大小的输出实际有从个。因为是对每个像素计算，所以其实RPN就是在前面章节中讲过 的NIN，使用1x1卷积实现。在论文中采用的是3种尺寸和3种长宽比，每个像素对应9 个anchor box,于是每个像素对应的物体分数有2x9=18个，对应的物体框的输出有4x9=36 个。一个RPN的示意图如图11-5所示。



图 11-5 Region Proposal Networks 的例子

图11-5中最左边的响应图是基础的卷积神经网络得到的最后一层卷积响应。经过256

通道的3x3卷积得到了每个位置对应的256维特征，然后以一个1x1的卷积层用于得到每 个位置对应的*个anchor box是否物体的得分，另一个1><1的卷积层用于得到每个位置对 应众个anchor box的位置和大小。

基于RPN的物体分数和物体框得到可能的物体框之后，训练时经过NMS和分数从大 到小排序筛选出有效的物体框，从中随机选取作为一个batch。然后通过R01 Pooling进行 分类的同时，会进一步对物体框的位置及大小进行回归，ROI Pooling之后的这两个任务对 应两个loss,再和RPN的两个loss放一起就实现了端到端的训练。Faster R-CNN无论是训 练/测试的速度，还是物体检测精度都超过了 Fast R-CNN,达到了这一系算法的巅峰。这 个方法的论文正式版本发表在2015的NIPS上《Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks》，有兴趣了解更多的读者可以查找并阅读原文。

11.1.6 YOLO 和 SSD 简介

从R-CNN到Faster R-CNN,这一系列的方法始终都是基于Region Proposal的。不用 Region Proposal的方法其实也有很多的研究，其中最有代表性的是YOLO和SSD。

YOLO全称You Only Look Once,取这个名字的意思是源于人眼看东西时，只需要一 瞥就能感知出认识的物体，YOLO也希望能达到这种简单高效的检测。执行速度快，是 YOLO提出时最大的特点，达到非常高效的检测，其背后是YOLO原理和实现上的简单。 YOLO的基本思想是，把一幅图片划分为一个SxS的格子，以每个格子所在位置和对应内 容为基础，来预测：

1）    物体框，包含物体框中心相对格子中心的坐标（x,y）和物体框的宽w和高/?，每个格 子预测B个物体框;<

2）    每个物体框是否有物体的置信度，其中当前这个格子中如果包含物体，则置信度的 分数为当前预测的物体框和标注物体框的IOU,否则置信度分数为0;

3）    每个格子预测一共C个类别的概率分数，并且这个分数和物体框是不相关的，只 是基于这个格子。

综上所述，每个格子需要输出的信息维度是Bx（4+1）+C=Bx5+C。在YOLO的论文中， B为2, C是PASCAL VOC的类别数20,所以每个格子预测的信息维度是2x5+20=30。格 子数S为7,所以最后得到的关于物体预测的信息是一个7<7x3O的张量。实现这个过程的 原理示意图如图11-6所示。

一幅图片首先缩放为一个正方形的图片，论文中采用的是448x448,然后送进一个卷 积神经网络，到最后一层卷积响应图的时候，接两层全连接，输出（并Reshape）是7x7x30, 对应前面提到的7x7x30的张量。最后从这7x7x30张量中提取出来的物体框和类别的预测 信息经过NMS，就得到了最终的物体检测结果。和基于Region Proposal方法的不同之处 在于，YOLO就是一个单纯的卷积神经网络，把物体检测转化成了个单纯的回归问题，端 到端的味道比R-CNN系列更加纯正。没有了 Region Proposal和对每个ROI的单独计算， 再加上利用GPU对计算响应图的并行处理，执行效率得到了极大提升。

YOLO在速度上获得了很大的提升，但是精度上比RCNN系还是逊色一些。其中一个 原因是基于格子回归物体框的方式也在一定程度上限制了物体框位置和大小的灵活性。另 外7x7的格子并不能将画面划分为足够精细的区域，如在一个格子对应的区域内如果同时

第11章目标检测

出现多个小物体就比较麻烦了。YOLO的论文最终发表在2016的CVPR上《You Only Look Once: Unified, Real-Time Object Detection》。



图11-6 YOLO原理示意图

SSD 全称是 Single Shot multibox Detector,算是同时借鉴了 YOLO 和 Faster R-CNN 思 想的方法。可以在达到实时的检测速度水平下，仍然保持很高的检测精#。和YOLO相近 的地方是，SSD也会在卷积神经网络的最后阶段，得到SxS的响应图'。然后是和Faster R-CNN相近的地方，SSD会基于每个格子的位置借鉴anchor box的思想生成默认的物体框。

本节一开始也提到过，相对于Faster R-CNN, SSD并没有Region Proposal对ROI分 类的两步框架，所以叫做Single Shot,其实从这个角度来说YOLO也是一种Single Shot 的检测方法。相比YOLO, SSD的主要改进是从一个分辨率较大的响应图幵始，逐渐得到 分辨率更低的响应图，每个分辨率下的响应图都作为产生物体类别分数和物体框的格子， 这样就得到了不同大小感受野对应的局部图像信息。如图11-7所示为SSD的一个示意图。



图11-7 SSD原理示意

第2篇实例精讲

图11-7中的例子是一幅输入图像经过基础网络后，得到了 8x8的响应图，然后这组响 应图的每个像素位置会产生类似anchor box那样A个默认物体框，其中每个框的大小和 位置的修正量对应4个数值，每个框内物体所属类别对应C个数值，所以用一个通道数为 (C+4X的卷积得到预测的框和结果。8x8的响应图进一步卷积可以得到4x4的响应图，这 个响应图中的每个像素对应更大的感受野，如图11-7右下的两个图所示。对于这个4x4 的响应图也可以用同样办法得到(C+4从通道的卷积响应作为预测结果，注意不同的响应图 上，A的取值可以不同。在550中*的取值策略也是不同大小和不同长宽比，最常见的配 置是如图11-7左下角所示，取4个默认物体框，或者让长宽比更夸张一些多取两个，即一 共6个。

在SSD论文中，基于VGG-16的基础模型在300x300输入分辨率下，得到的conv5是 38x38的响应图，每个像素上取*=4,经过进一步降低采样分别得到19x19、10><10和5><5 的响应图，对这3个响应图取h6,最后继续降低采样得到3x3和1x1的响应图，取^4, 则每个类别一共得到 38x38x4+(19x19+10x10+5x5)x6+(3x3+1x1)x4=8732 个默认物体框。 而YOLO默认配置448x448的分辨率，最后7x7的格子上每个格子默认预测2个物体框， 每个类别一共是7x7x2=98个物体框。SSD-300比起YOLO输入的分辨率更低，但是感受 野的精细程度更高，而且默认物体框的数量高出了快两个量级，结果就是执行速度和精度 的双双提升。

训练SSD的思路就和其他流行方法一样，两种loss，一种用来分类，一种用来定位。 不过SSD是一个细节非常多的方法，就像SSD的作者在2016的ECCV上讲的那样“The Devil is in the Details”，对SSD做了非常详尽的实验，从训练样本的选取，到数据增加、 默认框的长宽比策略、输入图像分辨率，甚至是卷积核的类型等都做了不同尝试，这些细 节这里就不——讲解了，有兴趣的读者可以参考发表在2016年ECCV的原文《SSD: Single Shot MultiBox Detector》来了解这篇实验完备且细节丰富的文章。

11.2基于PASCAL VOC数据集训练SSD模型

本节• •起来了解MXNet下的训练/测试SSD的官方例子，并试试SSD的效果。

11.2.1 MXNet 白勺 SSD 实I见

MXNet的SSD的实现其实就是把SSD作者Wei Liu基于Caffe的官方实现(网址为 <https://github.com/weiliu89/caffe/tree/ssd> )。在 MXNet 上的重新实现，原作者是 Missouri-Columbia大学的博士生Zhi (Joshua) Zhang。具体实现是在mxnet根目录中的 example\ssd目录下，如图11-8所示。

其中config下是训练相关的一些设置，如随机镜像和检测及每个epoch的随机设置等; data是默认存放数据或者数据软链接的地方；dataset是数据迭代的实现，默认是支持 PASCAL VOC的数据格式。另外，训练的物体类别定义也是在dataset/pascal_voc.py中实 现，所以如果需要训练自己的数据，就要在这个文件中做相应修改；detect中包含执行目 标检测的代码和接口； evaludate是封装目标检测代码并评估模型指标的脚本；model文件

第11章目标检测

夹默认用来存储模型结构和参数值；operator是个比较关键的文件夹，里面是对MXNet默 认不包含的底层操作的实现，如Faster R-CNN中的Region Proposal, SSD中的默认物体框 和标注框的匹配，正负样本的选取，还有NMS等操作的实现代码都是在operator中;symbol 里是默认网络结构的定义；tools里定义了一些其他的常用基本操作如随机裁剪和随机补边 的实现等；train文件夹则是定义了训练网络的基本操作，由根目录下的train.py调用。根 目录下的其他文件后面再介绍。

config

dataset

detect

evaluate

Update deter

days age

days^go

![img](file:///E:/00.Ebook/__Recent__html__/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)/00_深度学习与计算机视觉%20%20算法原理、框架应用与代码实现_14279998(1)_files/00_f1a666600ea1973ac6c9%20%2097d59f060146b694280ee3019eb0_14279998(1)-299.jpg)

symbol

tools

mn

g) ReAOMLmd g 一⑽ 一 w

demapy deploy.py 窗 evaluate-py

gj traiapy

nx example ssd vandrvce vector 价ruatization (#3840) add ssd as example (#3S?4>

to nix.fn)Q API (^4442)

tc mxjmg API («4442)



2 mohttu ago .1 months ago

25 days ago

Pylhonl    Support few exampi.

kckl asd as cx/inipt<&    、

Lonvert iterator io mxumg API {#4442}

图11 -8 MXNet中SSD的目录结构

11.2.2下载PASCAL VOC数据集

PASCAL VOC数据集已经简要介绍过了，下载地址如下。

□    [http://host.robots.ox.ac.Uk/pascal/V](http://host.robots.ox.ac.uk/pascal/V) OC/voc2012/V OCtrainvall 1 -May-2012.tar；

□    <http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar>;

□    [http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar%e3%80%82)[。](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar%e3%80%82)

其中前两个分别是2012年和2007年的训练验证集下载地址，第3个是2007年版本的 测试集下载地址，通常的用法是前两个数据作为训练验证数据，最后一个用来测试指标。 下载完毕后,分别对三个文件执行tar-xvf，就可以得到一个VOCdevkit的文件夹。VOCdevkit 包含两个文件夹VOC2007和VOC2012,这两个文件夹下的文件结构基本一致，-共是5 个子文件夹，分别是 Annotations、ImageSets、JPEGImages、SegmentationClass 和 SegmentationObject。

对于目标检测任务，主要关心的是前3个文件夹：Annotations下是所有的标注信息， 格式为XML,每个XML里的＜object〉就包含了标注物体的名称＜name〉和物体框的坐标 ＜bndbox〉; JPEGImages文件夹下是和Annotations文件夹下的XML文件同名的jpg图片文 件，与同名的XML的标注信息相对应；ImageSets/Main下就包含了训练集、验证集和测 试集的列表。

如果想要自己制作数据，如用本书第6章的小工具标注的数据来训练模型。最简单的 方式是按照这里说的规则，把所有标注信息放到Annotations里，图片做成JPG格式放到

第2篇实例精讲

JPEGImages中，然后训练/验证/测试集的列表放到ImageSets/Main下，再修改 dataset/pascal_voc.py下的相应代码并另存，其他步骤与训练PASCAL VOC区别不大了。

执行本书github代码仓库中的prepare_voc_data.sh可以自动完成下载到解压的步骤。

11.2.3训练SSD模型

第7章已经讲过如何安装和配置MXNet,不过默认下载并编译好的MXNet无法直接 训练SSD,原因就是SSD中有些操作是默认不被MXNet支持的。这些操作定义在operator 文件夹下，所以我们需要做的是，找到根目录下的make文件夹，打开里面的config.mk， 找到EXTRA_OPERATORS =这一行，修改为ssd中operator的路径：

EXTRA_OPERATORS = example/ssd/operator

如果使用的MXNet已经加入了自定义的operator,则用下面语句：

EXTRA_OPERATORS += example/ssd/operator

然后到MXNet的根目录下，输入make命令，就得到了支持SSD的MXNet版本。当 然用Python调用前别忘了再到mxnet\python目录下执行以下语句来更新python接口。

» python setup.py install

训练SSD并不是从头训起，而是从一个ImageNet数据集预训练好的简化版VGG-16 模型开始，按照官网的说明，该模型可以从下面链接下载 [https://dl.dropboxusercontent.eom/u/39265872/vggl6_reduced.zip](https://dl.dropboxusercontent.eom/u/39265872/vggl6_reduced.zip%ef%bc%8c%e5%9b%bd%e5%86%85%e7%9a%84%e8%af%bb%e8%80%85%e4%b9%9f%e5%8f%af%e4%bb%a5%e9%80%89%e6%8b%a9%e5%88%b0)[，国内的读者也可以选择到](https://dl.dropboxusercontent.eom/u/39265872/vggl6_reduced.zip%ef%bc%8c%e5%9b%bd%e5%86%85%e7%9a%84%e8%af%bb%e8%80%85%e4%b9%9f%e5%8f%af%e4%bb%a5%e9%80%89%e6%8b%a9%e5%88%b0) 本书的github代码仓下找到：

<https://github.com/frombeijingwithlove/dlcv_for_beginners/tree/master/chapl> 1 链接下载。

下载后解压得到定义网络结构的文件vgg 16_reduced-symbol.json和对应的参数文件 vggl6_reduced-0001 .paramso把这两个文件放到example\ssd\model下，基础模型就准备好 了。接下来把11.2.2节中准备好的VOCdevkit文件复制到example\ssd\data下，或者通过 In-s在example I ssd I data下建立一个链接。现在万事俱备，直接到ssd目录下执行train.py 就可以开始训练了，考虑到硬件配置的不同及训练效果等，可以通过修改train.py的参数 执行训练，如笔者修改了 batch-size和epoch:

» python train.py --batch-size=24 --epoch=20

训练过程中会得到实时的训练精度和loss的输出，每个epoch结束会在验证集上测试 模型，输出例子如下：

| 工NFO::Epoch[0]    BatchTrain-Acc=O.453572          | [20] | Speed: | 18.31 | samples/sec |
| --------------------------------------------------- | ---- | ------ | ----- | ----------- |
| 工NFO::Epoch[0]    BatchTrain-Obj ectAcc=0.02 924 9 | [20] | Speed: | 18.31 | samples/sec |
| 工NFO::Epoch[0]    BatchTrain-SmoothLl=92.689718    | [20] | Speed: | 18.31 | samples/sec |
| 工NFO::Epoch[0]    Batch                            | [40] | Speed: | 18.34 | samples/sec |

Train-Acc=O.622241

| ...中间部分省略... |       |       |        |       |             |
| ------------------ | ----- | ----- | ------ | ----- | ----------- |
| 工NFO::Epoch[0]    | Batch | [680] | Speed: | 18.39 | samples/sec |

Train-SmoothLl=78.859422

INFO::Epoch [0] Train-Acc=O.750000 INFO::Epoch [0] Train-ObjectAcc=0.000000

INFO::Epoch [0] Train-SmoothLl=87.912789

工NFO::Epoch[0] Time cost=902.594

INFO::Saved    checkpoint    to

"/path/to/mxnet/example/ssd/model/ssd_300-0001.params"

INFO::Epoch[0] Validation-Acc=0.750000

INFO::Epoch[0] Validation-ObjectAcc=0.000000

INFO::Epoch[0] Validation-SmoothLl=102.787167

11.2.4测试和评估模型效果

因为训练比较耗时，笔者象征性地训练了一个epoch,此外官网也很贴心地提供了训 练好的模型（从原作者提供的模型转换而来），地址为[https://dl.dropboxusercontent](https://dl.dropboxusercontent/). com/u/39265872/ssd_300_vggl 6_reduced_voc0712_jrainval.zip 0

如果无法下载，本书的github代码第11章的目录下有国内下载的链接。下载后解压 得到 ssd_300-0000.params 和 ssd_300-symbol.json 两个文件，放到 example/ssd/model 下。然 后执行：

» python evaluate.py

就会默认执行vggl6_reduced, epoch为0的模型在VOC2007测试集上的评估，输出每个 类别的和最后的心IP。    /

images,

INFO:root:Start evaluation with 4952 Writing aeroplane VOC results file

...中间部分省略...

Writing tvmonitor VOC results file

VOC 07 metric? Y reading annotations ...中间部分省略... reading annotations

for 1/4952

for 4901/4952

0.7215

0.6798

AP for aeroplane =

...中间部分省略... AP for tvmonitor = Mean AP =    0.7158

如果要评估别的模型可以在输入参数中指定，如下面命令评估训练完3个epoch得到 的模型，并且batch-size指定为24。

» python evaluate.py ——batch-size 24    ——epoch 3

上段代码评估的模型文件实际上是用来训练和验证的模型，实际部署的时候，和训练 相关的如loss等都是不需要的，对默认的Vggl6训练出的模型，可以用下面脚本生成用于 部署的模型和参数文件。

\>> python deploy --num-class 20

执行完该脚本，在model文件夹下生成的两个depk）y_前缀的文件就是用来部署的模型 结构和对应参数值，其中.params文件其实就复制了一遍而已。

11.2.5物体检测结果可视化

现在万事倶备，可以使用训练好（或是下载好）的模型执行检测任务了。MXNet的

SSD自带demo,首先到data/demo下运行：

» python download_demo_images.py

这个脚本会下载几张用于演示的图片，其中包括根目录下demo.py的默认演示图片 dog.jpg。回到ssd目录下，执行：

» python demo.py 得到图11-9所示的结果。

o

100

200

300

400

500



0    100    200    300    400    500    600    700

图H-9默认的demo图片dog.jpg的检测结果

f

如果要指定图片就用-image选项，下面用前面也用过的照片来试试: >> python demo.py ——images beihong_village.jpg 得到结果如图11-10所示。



图 11-10 beihong_village.jpg 的检测结果

下面来简单看一下demo.py到底做了什么，demo.py中主要就是一个交互的总還辑， 通过 get_detector()获取 了一个定义在 detect 文件夹下的 Detector 对象。在 detect/detector.py 中，定义了 Detector的类。首先Detector的初始化阶段，和之前讲的MXNet做分类的代码

没有什么不同，就是把一个symbol读进来，并进行bind用于后续计算。成员函数中的detect() 用于执行前向计算，用mod.predict()执行前向计算之后，得到的结果detections是一个三维 的张量。其中第一个维度和batch相关，因为demo.py中默认batch-size为1，所以所有结 果都在deteCtiOnS[0]里，这个结果如看做是一个二维矩阵的话，每行就是一个检测到的物体 框。第一列是物体框的类别下标，如果为-1说明是背景。第2列是类别的分数，第3〜6 列分别是xmin、ymin、xmax和ymax相对于画面宽高的百分比，所以物体框在画面中左 上角的坐标就是(xminx宽度，yminx高度)，右下角坐标是(xmaxx宽度，ymaxx高度)。 im_detect()函数是调用detect对读取的数据进行前向计算。detect_and_visualize()函数会调 用im_detect()函数返回的结果，然后在visualize_detection()函数中进行可视化，可视化的规 则是取类别得分大于给定阈值的框进行可视化，detector.py中，相关部分代码和注释如下

(代码中的省略号表示省略了部分代码)：

class Detector(object):

def _init_(self, symbol, model_prefix, epoch, data_shape,

mean_pixelsr \

def detect(self, det_iter, show_timer=False):

detections = self.mod.predict(det_iter).asnumpy()

\#    • •    i

result =    []

\#    detection中包含了所有的经过处理之后的物体框的结果 #第一个维度是预测的物体类别的下标

\#第二个维度是预测类别的分数

\#第3〜6个维度分别是xmin、ymin、xmax和ymax相对于画面宽高的百分比 #也就是说物体框的坐标的左上角是(xmin*width, ymin*height)

\#    右下角是(xmax*width, ymax*height)

\#这6个维度包含了目标检测结果的全部信息

\#    res把第一个维度为-1的结果(背景)全部剔除，保留类别分数大于0的部分 for i in range(detections.shape[0]):

det = detections [i,    :,    :]

res = det[np.where(det[:,    0]    >=    0) [0]]

result.append(res)

return result

def im_detect(self,    root_dir=None, extension=None,

show— timer=False):

def visualize_detection(self, img, dets, classes=[], thresh=0.6):

import matplotlib.pyplot as pit

import random

pit.imshow(img)

height = img.shape[0]

width = img.shape[1]

colors = diet()

for i in range(dets.shape[0]):

\#类别分数大于设定的阈值，则认为是检测到了物体，并在图

片上进行物体框的可视化

if score > thresh:

if cls_id not in colors:

| random.random(), random.random()) | colors[cls_id]                   | =    | (random.random(), |
| --------------------------------- | -------------------------------- | ---- | ----------------- |
| xmin                              | =int(dets[i.                     | 2]   | ★ width)          |
| ymin                              | =int(dets[i,                     | 3]   | ★ height)         |
| xmax                              | =int(dets[i,                     | 4]   | ★ width)          |
| ymax                              | =int(dets[i,                     | 5]   | ★ height)         |
| rect                              | = pit.Rectangle((xmin,    ymin), |      |                   |

xmax - xmin,

ymax - ymin, fill=False,

edgecolor=colors[cls_id],

linewidth=3.5)

pit.show()

def detect_and_visualize(self, im一list, root_dir=None, extension=

None,

classes=[], thresh=0.6,

show_timer=False):

11.2.6制作自己的标注数据

本书第6章最后曾实现了一个简单的标注小工具，考虑到灵活性，该小工具是用自定 义的格式保存了标注信息。但其实基本上现在的物体检测算法大都是默认支持PASCAL VOC的格式，本节提供一个进行转换的小脚本，可以把小工具标注好的数据转换为 PASCAL VOC支持的XML格式，方便各种主流框架下训练自己标注的数据。

标注信息其实就是个XML：对于目标检测而言，需要的最小信息是〈filename〉、＜size〉 和〈object〉。＜filename〉就是标注对应的图片文件的名字；＜size〉中包含〈width〉、〈height〉 和＜（1印*〉分别是图片像素的宽、高和通道数；＜object〉就是物体和对应标注框，其中＜name〉 是物体的名称，＜1）11（1130\〉中＜*0^11〉、＜ymin：^[l＜xmax〉、＜ymax〉分别是左上角和右下角的 像素位置坐标。根据这些规则，将第6章的小工具生成信息转换为XML标注信息的脚本 如下：

import os

import sys

import xml.etree.ElementTree as ET

\#import xml.dom.minidom as minidom

import cv2

from bbox_labeling import SimpleBBoxLabeling

input_dir = sys.argv[1].rstrip(os.sep)

\#获取所有bbox标注文件列表

bbox_filenames = [x for x in os.listdir(input_dir) if x.endswith ('.bbox')]

for bbox_filename in bbox_filenames:

bbox_f ilepath = os.sep.join([input_dir, bbox_filename]) jpg_filepath = bbox_filepath[:-5]

if not os.path.exists(jpg一filepath):

print('Something is wrong with {}!*.format(bbox_

filepath))

break

\# 生成根节点annotation

root = ET.Element(* annotation1)

\#文件名节点

filename = ET.SubElement(rootz * filename *)

jpg_filename = jpg_filepath.split(os.sep) [-1] filename.text = jpg_filename

\#读取基本图像信息’

img = cv2.imread(jpg_filepath)

h, w, c = img.shape

\#将图像信息写入节点

size = ET.SubElement(root, * size *)

width = ET.SubElement(size, * width')

width.text = str(w)

height = ET.SubElement(size, * height *)

height.text = str(h)

depth = ET.SubElement(size, * depth *)

depth.text = str(c)

\#读取标注信息，需要调用第6章的小工具中的函数

bboxes = SimpleBBoxLabeling.load_bbox(bbox_filepath) for obj_name, coord in bboxes:

\#~遍历每个标注框并写入信息到XML

obj = ET.SubElement(root, * object *) name = ET.SubElement(obj ,    * name *)

name. text = obj name

bndbox xmin = xmax = ymin = ymax = (left,

’ bndbox *)

\*    xmin *)

'xmax *)

\*    ymin *)

\*    ymax，)

=coord

=ET.SubElement(obj,

ET.SubElement(bndbox, ET.SubElement(bndbox, ET.SubElement(bndbox, ET.SubElement(bndbox, top) , (right, bottom)

xmin.text = xmax.text = ymin.text = ymax.text =

str(left) str(right) str(top) str (bottom)

生成XML标注文件

xml_filepath = jpg_filepath[:jpg_filepath.rfind(*.*)]    +

with open (xml_f ilepath, * w') as f:

anno_xmlstr = ET.tostring(root)

\# Python 自带 ElementTree 输出的 xml 是~■挖 #如果需要格式化好的输出，将下面注释和import中的注释取消 #anno_xml = minidom.parseString(anno_xmlstr)

\#anno_xmlstr = anno_xml.toprettyxml()

f.write(anno_xmlstr)

保存为detection_anno_bbox2voc.py，用Python执行时第一个参数是包含图片和标注信 息的文件夹，就可以根据第6章定义的后缀为bbox的标注信息，生成相应的XML标注信 息并保存在同一文件夹下。
