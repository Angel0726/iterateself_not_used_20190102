---
title: 01 介绍
toc: true
date: 2018-09-02
---
# attention-mechanism


另一类不需要对文字预先分割的方法就是 attention-mechanism，attention 可以分为 hard attention和 soft attention。其中 hard attention 能够直接给出 hard location，通常是 bounding box 的位置 （https://arxiv.org/pdf/1412.7755.pdf), 想法直观，缺点是不能直接暴力 bp 。soft attention通常是rnn/lstm/gru encoder-decoder model (https://arxiv.org/abs/1603.03101), 可以暴力 bp。还有一种比较特别的 gradient-based attention( http://www.ics.uci.edu/~yyang8/research/feedback/feedback-iccv2015.pdf ) 也挺有意思。
